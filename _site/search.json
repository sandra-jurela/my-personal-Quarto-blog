[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Let’s be friends",
    "section": "",
    "text": "Hi and welcome to my personal blog. My name is Sandra Jurela and here I will be sharing my journey into Data Science. Recently I have developed a real passion for Data Science and this blog truly is a manifestation of that.\nI hold a Master’s degree in Civil Engineering (Hydrology and Water Resources Science). I started my career as a construction designer, and then for 10 years worked as a hydrologist at the Croatian Meteorological and Hydrological Service. I currently work for my family’s business, and freelance as a data analyst.\nI live in Zagreb, Croatia, with my two boys (life partner and our sweet son).\nIf you’d like to connect with me, please do. I hope you enjoy your visit!\n\n\nSince the bootstrap icon for Tableau is not yet available, here is the link to my Tableau Public repository."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sandra Jurela",
    "section": "",
    "text": "Bank Customer Retention Analysis in Python\n\n\n\n\n\n\n\npython\n\n\naltair\n\n\nEDA\n\n\n\n\nWhat marketing campaigns could help reduce customer churn?\n\n\n\n\n\n\nJun 10, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nShiny App - Mass Shootings in the USA\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nEDA\n\n\ndata cleaning\n\n\n\n\nEDA with Shiny app on mass shootings between August 20th, 1982 and May 15th, 2023.\n\n\n\n\n\n\nMar 1, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Segmentation With K-Means and UMAP\n\n\n\n\n\n\n\nR\n\n\nk-means\n\n\nUMAP\n\n\n\n\nA report for the non-technical marketing team.\n\n\n\n\n\n\nDec 15, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCryptocurrency SQL Case Study\n\n\n\n\n\n\n\nSQL\n\n\nPostgreSQL\n\n\nEDA\n\n\n\n\nData with Danny - SQL masterclass - O’Reilly. With R solutions for Part 4: Window functions.\n\n\n\n\n\n\nNov 25, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nScraping 200 Best Movies of 2010s from Rotten Tomatoes\n\n\n\n\n\n\n\nweb scraping\n\n\ndata wrangling\n\n\nrvest\n\n\nR\n\n\nTableau\n\n\n\n\nScraping data on movies from Rotten Tomatoes and finally creating a dashboard in Tableau\n\n\n\n\n\n\nSep 1, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nTitanic Survival Exercises\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ndata visualization\n\n\n\n\nAssessment of visualization skills acquired in the HarvardX’s Data Science: Visualization course.\n\n\n\n\n\n\nJun 9, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Analysis – Advanced Plots With {ggplot2}\n\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\nSQL\n\n\nPostgreSQL\n\n\nR\n\n\n\n\nQuerying database in R code chunk and making some useful plots with ggplot2.\n\n\n\n\n\n\nMay 20, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nAnswering Business Questions Using SQL\n\n\n\n\n\n\n\nSQL\n\n\nSQLite\n\n\ndata visualization\n\n\nR\n\n\n\n\nMy SQL project for the “Intermediate SQL for Data Analysis” course at Dataquest.\n\n\n\n\n\n\nFeb 15, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html",
    "href": "posts/answering-business-questions-using-sql/chinook.html",
    "title": "Answering Business Questions Using SQL",
    "section": "",
    "text": "The aim of this project is to explore a modified version of the Chinook database using SQL and answer some business questions. The Chinook database represents a fictional digital media shop, based on real data from an iTunes Library and manually generated data. The database is provided as a SQLite database file called chinook.db.\nHere’s a schema diagram for the Chinook database:"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "href": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "title": "Answering Business Questions Using SQL",
    "section": "Connecting to the Database and Data Overview",
    "text": "Connecting to the Database and Data Overview\n\nlibrary(DBI)\n\ndb <- dbConnect(RSQLite::SQLite(), dbname = \"data/chinook.db\")\n\nListing all tables in the Chinook database.\n\nSELECT\n  name,\n  type\nFROM sqlite_master\nWHERE type IN (\"table\", \"view\")\n\n\n11 records\n\n\nname\ntype\n\n\n\n\nalbum\ntable\n\n\nartist\ntable\n\n\ncustomer\ntable\n\n\nemployee\ntable\n\n\ngenre\ntable\n\n\ninvoice\ntable\n\n\ninvoice_line\ntable\n\n\nmedia_type\ntable\n\n\nplaylist\ntable\n\n\nplaylist_track\ntable\n\n\ntrack\ntable\n\n\n\n\n\nThe database consists of 11 tables containing information about artists, albums, media tracks, playlists, invoices, customers, and shop employees. Let’s start by getting familiar with our data from the main tables:\nemployee table\n\nSELECT *\nFROM employee\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ncustomer table\n\nSELECT *\nFROM customer\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice table\n\nSELECT *\nFROM invoice\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice_line table\n\nSELECT *\nFROM invoice_line\nLIMIT 30\n\n\n\n\n\n  \n\n\n\ntrack table\n\nSELECT *\nFROM track\nLIMIT 20"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "href": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "title": "Answering Business Questions Using SQL",
    "section": "1. Selecting Albums to Purchase",
    "text": "1. Selecting Albums to Purchase\nThe Chinook record store has just signed a deal with a new record label, and you’ve been tasked with selecting the first three albums that will be added to the store, from a list of four. All four albums are by artists that don’t have any tracks in the store right now - we have the artist names, and the genre of music they produce:\n\n\n\nArtist Name\nGenre\n\n\n\n\nRegal\nHip-Hop\n\n\nRed Tone\nPunk\n\n\nMeteor and the Girls\nPop\n\n\nSlim Jim Bites\nBlues\n\n\n\nThe record label specializes in artists from the USA, and they have given Chinook some money to advertise the new albums in the USA, so we’re interested in finding out which genres sell the best in the USA.\nYou’ll need to write a query to find out which genres sell the most tracks in the USA, write up a summary of your findings, and make a recommendation for the three artists whose albums we should purchase for the store.\nInstructions\n\nWrite a query that returns each genre, with the number of tracks sold in the USA:\n\nin absolute numbers\nin percentages.\n\nWrite a paragraph that interprets the data and makes a recommendation for the three artists whose albums we should purchase for the store, based on sales of tracks from their genres.\n\n\nSELECT \n  g.name AS genre,\n  SUM(il.quantity) AS tracks_sold,\n  ROUND(CAST(SUM(il.quantity) AS FLOAT)/\n  (\n    SELECT SUM(il.quantity) \n    FROM invoice i\n    INNER JOIN invoice_line il\n    ON i.invoice_id = il.invoice_id\n    WHERE i.billing_country = 'USA'\n  ) \n  , 4) AS percentage_sold\nFROM invoice i\nINNER JOIN invoice_line il\nON i.invoice_id = il.invoice_id\nINNER JOIN track t \nON il.track_id = t.track_id\nINNER JOIN genre g\nON t.genre_id = g.genre_id\nWHERE i.billing_country = 'USA'\nGROUP BY genre\nORDER BY tracks_sold DESC\n\n\n\n\n\n\ngenre\ntracks_sold\npercentage_sold\n\n\n\n\nRock\n561\n0.5338\n\n\nAlternative & Punk\n130\n0.1237\n\n\nMetal\n124\n0.1180\n\n\nR&B/Soul\n53\n0.0504\n\n\nBlues\n36\n0.0343\n\n\nAlternative\n35\n0.0333\n\n\nPop\n22\n0.0209\n\n\nLatin\n22\n0.0209\n\n\nHip Hop/Rap\n20\n0.0190\n\n\nJazz\n14\n0.0133\n\n\nEasy Listening\n13\n0.0124\n\n\nReggae\n6\n0.0057\n\n\nElectronica/Dance\n5\n0.0048\n\n\nClassical\n4\n0.0038\n\n\nHeavy Metal\n3\n0.0029\n\n\nSoundtrack\n2\n0.0019\n\n\nTV Shows\n1\n0.0010\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_classic())\n\ngenre_of_interest <- c(\"Alternative & Punk\", \"Hip Hop/Rap\", \"Pop\", \"Blues\")\n\ngenre_perc %>% \n  mutate(of_interest = ifelse(genre %in% genre_of_interest, \"yes\", \"no\"),\n         perc_text = scales::percent(percentage_sold, accuracy = 0.1)) %>% \n  ggplot(aes(x=tracks_sold, y=reorder(genre, tracks_sold, sum), fill=of_interest)) + \n  geom_bar(stat = 'identity', width = 0.7) +\n  geom_text(aes(label = perc_text), hjust = -0.2) +\n  labs(title = \"Sold Tracks by Genre, USA\", x = \"Tracks Sold\", y = \"Genre\", \n       fill = \"Genre of Interest\") + \n  scale_fill_manual(values = c(\"gray74\", \"orange\")) +\n  scale_x_continuous(limits = c(0, 600)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe most popular genres in the USA are Rock, Alternative & Punk, and Metal, followed with a big gap by all the others. Since our choice is limited by Hip-Hop, Punk, Pop, and Blues genres, and since we have to choose 3 out of 4 albums, we should purchase the new albums by the following artists:\n\nRed Tone (Punk)\nSlim Jim Bites (Blues)\nMeteor and the Girls (Pop)"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "title": "Answering Business Questions Using SQL",
    "section": "2. Analyzing Employee Sales Performance",
    "text": "2. Analyzing Employee Sales Performance\nEach customer for the Chinook store gets assigned to a sales support agent within the company when they first make a purchase. You have been asked to analyze the purchases of customers belonging to each employee to see if any sales support agent is performing either better or worse than the others.\nYou might like to consider whether any extra columns from the employee table explain any variance you see, or whether the variance might instead be indicative of employee performance.\nInstructions\n\nWrite a query that finds the total dollar amount of sales assigned to each sales support agent within the company. Add any extra attributes for that employee that you find are relevant to the analysis.\nWrite a short statement describing your results, and providing a possible interpretation.\n\n\nSELECT \n  e.first_name || ' ' || e.last_name AS sales_support_agent,\n  e.hire_date,\n  COUNT(DISTINCT c.customer_id) AS customers,\n  SUM(i.total) AS total_sales\nFROM employee e\nINNER JOIN customer c\nON e.employee_id = c.support_rep_id\nINNER JOIN invoice i\nON c.customer_id = i.customer_id\nGROUP BY sales_support_agent\n\n\n3 records\n\n\nsales_support_agent\nhire_date\ncustomers\ntotal_sales\n\n\n\n\nJane Peacock\n2017-04-01 00:00:00\n21\n1731.51\n\n\nMargaret Park\n2017-05-03 00:00:00\n20\n1584.00\n\n\nSteve Johnson\n2017-10-17 00:00:00\n18\n1393.92\n\n\n\n\n\nWhile there is a 20% difference in sales between Jane (the top employee) and Steve (the bottom employee), the difference roughly corresponds with the differences in their hiring dates."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "title": "Answering Business Questions Using SQL",
    "section": "3. Analyzing Sales by Country",
    "text": "3. Analyzing Sales by Country\nYour next task is to analyze the sales data for customers from each different country. You have been given guidance to use the country value from the customers table, and ignore the country from the billing address in the invoice table.\nInstructions\n\nWrite a query that collates data on purchases from different countries.\nWhere a country has only one customer, collect them into an “Other” group.\nThe results should be sorted by the total sales from highest to lowest, with the “Other” group at the very bottom.\nFor each country, include:\n\ntotal number of customers\ntotal value of sales\naverage value of sales per customer\naverage order value\n\n\n\nWITH t1 AS (\n  SELECT\n    CASE\n      WHEN COUNT(DISTINCT c.customer_id) = 1 THEN 'Other'\n      ELSE c.country\n      END AS country,\n    COUNT(DISTINCT c.customer_id) AS customers,\n    SUM(i.total) AS total_sales,\n    SUM(i.total)/COUNT(DISTINCT c.customer_id) AS avg_sales_per_cust,\n    AVG(i.total) AS avg_order\n  FROM customer c\n  INNER JOIN invoice i\n  ON c.customer_id = i.customer_id\n  GROUP BY country\n)\n\nSELECT \n  country,\n  SUM(customers) AS customers,\n  SUM(total_sales) AS total_sales,\n  AVG(avg_sales_per_cust) AS avg_sales_per_cust,\n  AVG(avg_order) AS avg_order\nFROM \n  (\n    SELECT\n      t1.*,\n      CASE \n        WHEN country = 'Other' THEN 1\n        ELSE 0\n        END AS sorted\n    FROM t1\n  )\nGROUP BY country\nORDER BY sorted, total_sales DESC\n\n\n\n\n\n\ncountry\ncustomers\ntotal_sales\navg_sales_per_cust\navg_order\n\n\n\n\nUSA\n13\n1040.49\n80.03769\n7.942672\n\n\nCanada\n8\n535.59\n66.94875\n7.047237\n\n\nBrazil\n5\n427.68\n85.53600\n7.011147\n\n\nFrance\n5\n389.07\n77.81400\n7.781400\n\n\nGermany\n4\n334.62\n83.65500\n8.161463\n\n\nCzech Republic\n2\n273.24\n136.62000\n9.108000\n\n\nUnited Kingdom\n3\n245.52\n81.84000\n8.768571\n\n\nPortugal\n2\n185.13\n92.56500\n6.383793\n\n\nIndia\n2\n183.15\n91.57500\n8.721429\n\n\nOther\n15\n1094.94\n72.99600\n7.445071\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, customers, total_sales) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(customers = customers/sum(customers),\n         total_sales = total_sales/sum(total_sales)) %>%\n  pivot_longer(-country, names_to = \"variable\", values_to = \"value\") %>% \n  ggplot(aes(x=value, y=country, fill=variable)) +\n  geom_bar(stat = \"identity\", width = 0.65, position = position_dodge(0.8)) +\n  labs(title=\"Share of Customers and Sales by Country\", x=\"share\", fill=\"\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"gray77\", \"seagreen3\")) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, avg_order, avg_sales_per_cust) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(avg_order = (avg_order - mean(avg_order)) / mean(avg_order),\n         avg_sales_per_cust = (avg_sales_per_cust - mean(avg_sales_per_cust)) / \n                                 mean(avg_sales_per_cust)) %>%\n  rename(`Average Order` = avg_order,\n         `Average Sales per Customer` = avg_sales_per_cust) %>% \n  pivot_longer(-country, names_to = \"variable\", values_to = \"pct_diff_from_mean\") %>% \n  ggplot(aes(x=pct_diff_from_mean, y=country, fill=variable)) +\n  geom_col(width = 0.65, position = position_dodge(0.8)) +\n  facet_wrap(~ variable, scales = \"free_x\") + \n  labs(title=\"Average Order & Average Sales per Customer\", \n       subtitle = \"(Percent Difference from Mean)\", x=\"pct diff from mean\", fill=\"\") +\n  scale_fill_manual(values = c(\"steelblue\", \"lightskyblue2\")) +\n  scale_x_continuous(labels = scales::percent) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe USA has the largest customer base and, consequently, the highest total sales.\nBased on the data, there may be opportunity in the following countries:\n\nCzech Republic\nUnited Kingdom\nIndia\n\nIt’s worth keeping in mind that the amount of data from each of these countries is relatively low. Because of this, we should be cautious spending too much money on new marketing campaigns, as the sample size is not large enough to give us high confidence. A better approach would be to run small campaigns in these countries, collecting and analyzing the new customers to make sure that these trends hold with new customers."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "href": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "title": "Answering Business Questions Using SQL",
    "section": "4. Albums vs Individual Tracks",
    "text": "4. Albums vs Individual Tracks\nThe Chinook store is setup in a way that allows customer to make purchases in one of the two ways:\n\npurchase a whole album\npurchase a collection of one or more individual tracks.\n\nThe store does not let customers purchase a whole album, and then add individual tracks to that same purchase (unless they do that by choosing each track manually). When customers purchase albums they are charged the same price as if they had purchased each of those tracks separately.\nManagement are currently considering changing their purchasing strategy to save money. The strategy they are considering is to purchase only the most popular tracks from each album from record companies, instead of purchasing every track from an album.\nWe have been asked to find out what percentage of purchases are individual tracks vs whole albums, so that management can use this data to understand the effect this decision might have on overall revenue.\nInstructions\n\nWrite a query that categorizes each invoice as either an album purchase or not, and calculates the following summary statistics:\n\nNumber of invoices\nPercentage of invoices\n\nWrite one to two sentences explaining your findings, and making a prospective recommendation on whether the Chinook store should continue to buy full albums from record companies\n\n\nWITH cat_purchase AS (\n  SELECT\n    il.invoice_id,\n    CASE\n      WHEN\n      COUNT(DISTINCT t.album_id) = 1\n      AND \n      COUNT(DISTINCT t.track_id) = c.count_album_tracks\n      THEN 'album'\n      ELSE 'individual track(s)'\n      END AS purchase_type,\n      c.count_album_tracks\n    FROM track t\n    JOIN invoice_line il\n    ON il.track_id = t.track_id\n    JOIN (SELECT COUNT(*) AS count_album_tracks, album_id\n          FROM track\n          GROUP BY album_id) c\n    ON c.album_id = t.album_id\n    GROUP BY invoice_id\n)\n\nSELECT\n  purchase_type,\n  COUNT(*) AS number_of_invoices,\n  ROUND(CAST(COUNT(*) AS float) / CAST(\n    (SELECT COUNT(*)\n    FROM invoice) AS FLOAT), 2) AS percentage_of_invoices\nFROM cat_purchase\nGROUP BY purchase_type\n\n\n2 records\n\n\npurchase_type\nnumber_of_invoices\npercentage_of_invoices\n\n\n\n\nalbum\n114\n0.19\n\n\nindividual track(s)\n500\n0.81\n\n\n\n\n\nAlbum purchases account for 19% of all purchases. Based on this data, I would recommend against purchasing only the most popular tracks from each album from record companies, since there is a potential of losing a significant portion of revenue."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "href": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "title": "Answering Business Questions Using SQL",
    "section": "5. Which artist is used in the most playlists?",
    "text": "5. Which artist is used in the most playlists?\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY number_of_playlists DESC, unique_tracks DESC\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nEugene Ormandy\nClassical\n7\n3\n\n\nBerliner Philharmoniker & Herbert Von Karajan\nClassical\n6\n3\n\n\nThe King’s Singers\nClassical\n6\n2\n\n\nEnglish Concert & Trevor Pinnock\nClassical\n6\n2\n\n\nAcademy of St. Martin in the Fields & Sir Neville Marriner\nClassical\n6\n2\n\n\nMichael Tilson Thomas & San Francisco Symphony\nClassical\n5\n2\n\n\nYo-Yo Ma\nClassical\n5\n1\n\n\nWilhelm Kempff\nClassical\n5\n1\n\n\nTon Koopman\nClassical\n5\n1\n\n\nSir Georg Solti, Sumi Jo & Wiener Philharmoniker\nOpera\n5\n1\n\n\n\n\n\nEugene Ormandy takes the first place with only 3 unique tracks in 7 different playlists. His music belongs to the classical genre, which we have previously seen is one of the least popular genres in the USA.\nIf we order this table by number of unique tracks, we get a completely different list.\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY unique_tracks DESC, number_of_playlists DESC\n\n\nDisplaying records 1 - 10\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nLed Zeppelin\nRock\n3\n114\n\n\nMetallica\nMetal\n4\n112\n\n\nU2\nRock\n3\n112\n\n\nIron Maiden\nMetal\n4\n95\n\n\nDeep Purple\nRock\n3\n92\n\n\nIron Maiden\nRock\n3\n81\n\n\nPearl Jam\nRock\n4\n54\n\n\nVan Halen\nRock\n3\n52\n\n\nOs Paralamas Do Sucesso\nLatin\n3\n49\n\n\nLost\nTV Shows\n2\n48"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "href": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "title": "Answering Business Questions Using SQL",
    "section": "6. How many tracks have been purchased vs not purchased?",
    "text": "6. How many tracks have been purchased vs not purchased?\n\nWITH all_and_purchased_tracks AS (\n  SELECT \n    t.track_id AS all_tracks,\n    il.track_id AS purch_tracks\n  FROM track t\n  LEFT JOIN invoice_line il\n  ON t.track_id = il.track_id\n)\n  \nSELECT\n  COUNT(DISTINCT all_tracks) AS total_tracks,\n  COUNT(DISTINCT purch_tracks) AS pirchased,\n  COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) AS not_purchased,\n  ROUND(CAST(COUNT(DISTINCT purch_tracks) AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_purchased,\n  ROUND(CAST(COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) \n    AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_not_purchased\nFROM all_and_purchased_tracks\n\n\n1 records\n\n\n\n\n\n\n\n\n\ntotal_tracks\npirchased\nnot_purchased\nperc_purchased\nperc_not_purchased\n\n\n\n\n3503\n1806\n1697\n0.516\n0.484\n\n\n\n\n\n\n\nCode\npie(c(51.6, 48.4), labels = c(\"purchased\", \"not purchased\"), \n    main = \"Purchased vs not purchased tracks\")\n\n\n\n\n\nAlmost half of all the unique tracks available in the Chinook store were never bought, probably being of unpopular genre or unpopular artists."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "href": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "title": "Answering Business Questions Using SQL",
    "section": "7. Do protected vs non-protected media types have an effect on popularity?",
    "text": "7. Do protected vs non-protected media types have an effect on popularity?\nLet’s take a look at the media_type table.\n\nSELECT *\nFROM media_type\n\n\n5 records\n\n\nmedia_type_id\nname\n\n\n\n\n1\nMPEG audio file\n\n\n2\nProtected AAC audio file\n\n\n3\nProtected MPEG-4 video file\n\n\n4\nPurchased AAC audio file\n\n\n5\nAAC audio file\n\n\n\n\n\nThere are 2 out of 5 media types that are protected.\n\nWITH t AS (\n  SELECT \n    t.track_id,\n    CASE\n      WHEN mt.name LIKE \"%protected%\" THEN \"yes\" ELSE \"no\"\n      END AS protected\n  FROM track t\n  JOIN media_type mt ON mt.media_type_id = t.media_type_id\n)\n\nSELECT \n  protected,\n  COUNT(DISTINCT t.track_id) AS unique_tracks,\n  ROUND(CAST(COUNT(DISTINCT t.track_id) AS FLOAT) / (SELECT COUNT(*) FROM track), 2)\n    AS 'unique_tracks_%',\n  COUNT(DISTINCT il.track_id) AS sold_unique_tracks,\n  COUNT(il.track_id) AS sold_tracks,\n  ROUND(CAST(COUNT(il.track_id) AS FLOAT) / (SELECT COUNT(*) FROM invoice_line), 2)\n    AS 'sold_tracks_%'\nFROM t\nLEFT JOIN invoice_line il ON t.track_id = il.track_id\nGROUP BY protected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprotected\nunique_tracks\nunique_tracks_%\nsold_unique_tracks\nsold_tracks\nsold_tracks_%\n\n\n\n\nno\n3052\n0.87\n1652\n4315\n0.91\n\n\nyes\n451\n0.13\n154\n442\n0.09\n\n\n\n\n\n\n\nCode\nby_media_type %>% \n  select(protected, unique_tracks, sold_tracks) %>% \n  rename(unique = unique_tracks, \n         sold = sold_tracks) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  mutate(tracks = as.factor(tracks) %>% fct_rev()) %>% \n  group_by(tracks) %>% \n  mutate(pct = count/sum(count) %>% round(2)) %>% \n  ungroup() %>% \n  ggplot(aes(x=tracks, y=count, fill=protected)) +\n  geom_col(width = 0.5, position = \"stack\", color = \"white\") +\n  geom_text(aes(label = count), position = position_stack(vjust = .5),\n            color = \"white\", fontface = \"bold\")+\n  scale_fill_manual(values = c(\"#fbc02d\", \"#03a9f4\")) +\n  theme(legend.position = \"top\")\nby_media_type %>% \n  select(protected, unique_tracks, sold_unique_tracks) %>% \n  rename(unique = unique_tracks, \n         unique_sold = sold_unique_tracks) %>% \n  mutate(unique_unsold = unique - unique_sold) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  filter(tracks != \"unique\") %>% \n  group_by(protected) %>% \n  mutate(percentage = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n  ggplot(aes(x=protected, y=count, fill=tracks)) +\n  geom_col(width = 0.5, position = \"fill\", color = \"white\") +\n  scale_fill_manual(values = c(\"seagreen3\", \"tomato\")) +\n  geom_text(aes(label = percentage), position = position_fill(vjust = .5), \n            color = \"white\", fontface = \"bold\") +\n  labs(y=\"proportion\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can make the following observations:\n\nOnly 13% of all the unique tracks available in the Chinook store are of protected media types.\nAmong all the tracks that were sold, those of protected media types amounts only to 9%.\nFrom all the unique tracks of protected media types, only 34,1% were sold, while from those of non-protected ones 54,1%.\n\nWe can conclude that the tracks of protected media types are much less popular than those of non-protected."
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "title": "Cryptocurrency SQL Case Study",
    "section": "",
    "text": "On November 18, 2022 I attended the SQL masterclass “SQL and PostgreSQL for Data Analytics”, presented live by Danny Ma on O’Reilly platform.\nThis Github repository contains all the necessary data, sql scripts, and other materials.\nI’m posting some parts of it here for my own reference, but I strongly recommend that you register for that event that takes place every once in a while. It’s free and worth it. Danny Ma is an excellent teacher and his explanations are outstanding.\n\n\nFor the purposes of this project, I created the local trading Postgres database on my machine and ran the sql script to create and populate the tables.\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database (typically via the DBI::dbConnect() function).\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"trading\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> trading@localhost:5432"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "title": "Cryptocurrency SQL Case Study",
    "section": "Introduction",
    "text": "Introduction\nIn this fictitious case study - Danny’s data mentors from the Data With Danny team have been busy trading cryptocurrency markets since 2017.\nThe main purpose for this case study is to analyze the performance of the DWD mentors over time. We will accomplish this by writing SQL queries to utilize all available datasets to answer a series of realistic business questions.\n\nAvailable Datasets\nAll of our data for this case study exists within the trading schema in the PostgreSQL database.\nThere are 3 data tables available in this schema:\n\nmembers\nprices\ntransactions\n\n\n\nEntity Relationship Diagram\nI drew the ERD here.\n\n\n\nData Dictionary and Overview\nThe trading.members table consists of information about the mentors from the Data With Danny team.\n\ntrading.members table\n\n\nColumn name\nDescription\n\n\n\n\nmember_id\nunique id for each mentor\n\n\nfirst_name\nfirst name for each mentor\n\n\nregion\nregion where each mentor is from\n\n\n\n\nSELECT * FROM trading.members\n\n\n14 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\nc9f0f8\nAbe\nUnited States\n\n\n45c48c\nBen\nAustralia\n\n\nd3d944\nEnoch\nAfrica\n\n\n6512bd\nVikram\nIndia\n\n\nc20ad4\nLeah\nAsia\n\n\nc51ce4\nPavan\nAustralia\n\n\naab323\nSonia\nAustralia\n\n\n\n\n\nThe trading.prices table consists of daily price and volume information from January 2017 through to August 2021 for the 2 most popular cryptocurrency tickers: Bitcoin and Ethereum.\n\ntrading.prices table\n\n\nColumn name\nDescription\n\n\n\n\nticker\none of either BTC or ETH\n\n\nmarket_date\nthe date for each record\n\n\nprice\nclosing price at end of day\n\n\nopen\nthe opening price\n\n\nhigh\nthe highest price for that day\n\n\nlow\nthe lowest price for that day\n\n\nvolume\nthe total volume traded\n\n\nchange\n% change in daily price\n\n\n\nThe first 5 rows from this dataset.\n\nSELECT * FROM trading.prices LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nopen\nhigh\nlow\nvolume\nchange\n\n\n\n\nETH\n2021-08-29\n3177.84\n3243.96\n3282.21\n3162.79\n582.04K\n-2.04%\n\n\nETH\n2021-08-28\n3243.90\n3273.78\n3284.58\n3212.24\n466.21K\n-0.91%\n\n\nETH\n2021-08-27\n3273.58\n3093.78\n3279.93\n3063.37\n839.54K\n5.82%\n\n\nETH\n2021-08-26\n3093.54\n3228.03\n3249.62\n3057.48\n118.44K\n-4.17%\n\n\nETH\n2021-08-25\n3228.15\n3172.12\n3247.43\n3080.70\n923.13K\n1.73%\n\n\n\n\n\nThe trading.transactions table consists of buy and sell transactions data for each trade made by the DWD mentors.\n\ntrading.transactions table\n\n\nColumn name\nDescription\n\n\n\n\ntxn_id\nunique ID for each transaction\n\n\nmember_id\nmember identifier for each trade\n\n\nticker\nthe ticker for each trade\n\n\ntxn_date\nthe date for each transaction\n\n\ntxn_type\neither BUY or SELL\n\n\nquantity\nthe total quantity for each trade\n\n\npercentage_fee\n% of total amount charged as fees\n\n\ntxn_time\nthe timestamp for each trade\n\n\n\nThe first 5 frows from this transactions table.\n\nSELECT * FROM trading.transactions LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\ntxn_id\nmember_id\nticker\ntxn_date\ntxn_type\nquantity\npercentage_fee\ntxn_time\n\n\n\n\n1\nc81e72\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n2\neccbc8\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n3\na87ff6\nBTC\n2017-01-01\nBUY\n50\n0.0\n2017-01-01\n\n\n4\ne4da3b\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n5\n167909\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 1️⃣: Basic Data Analysis Techniques",
    "text": "PART 1️⃣: Basic Data Analysis Techniques\n\nQuestion 1.1\nShow only the top 5 rows from the trading.members table.\n\nSELECT * \nFROM trading.members\nLIMIT 5\n\n\n5 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n\n\n\n\n\nQuestion 1.2\nSort all the rows in the trading.members table by first_name in alphabetical order and show the top 3 rows with all columns.\n\nSELECT * \nFROM trading.members\nORDER BY first_name\nLIMIT 3\n\n\n3 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc9f0f8\nAbe\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n\n\n\n\n\nQuestion 1.3\nCount the number of records from the trading.members table which have United States as the region value.\n\nSELECT \n  COUNT(*) AS record_count\nFROM trading.members\nWHERE region = 'United States'\n\n\n1 records\n\n\nrecord_count\n\n\n\n\n7\n\n\n\n\n\n\n\nQuestion 1.4\nSelect only the first_name and region columns for mentors who are not from Australia.\n\nSELECT \n  first_name, \n  region\nFROM trading.members\nWHERE region != 'Australia'\n\n\nDisplaying records 1 - 10\n\n\nfirst_name\nregion\n\n\n\n\nVipul\nUnited States\n\n\nCharlie\nUnited States\n\n\nNandita\nUnited States\n\n\nRowan\nUnited States\n\n\nAyush\nUnited States\n\n\nAlex\nUnited States\n\n\nAbe\nUnited States\n\n\nEnoch\nAfrica\n\n\nVikram\nIndia\n\n\nLeah\nAsia\n\n\n\n\n\n\n\nQuestion 1.5\nReturn only the unique region values from the trading.members table and sort the output by reverse alphabetical order.\n\nSELECT DISTINCT region\nFROM trading.members\nORDER BY region DESC\n\n\n5 records\n\n\nregion\n\n\n\n\nUnited States\n\n\nIndia\n\n\nAustralia\n\n\nAsia\n\n\nAfrica"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 2️⃣: Aggregate Functions for Data Analysis",
    "text": "PART 2️⃣: Aggregate Functions for Data Analysis\n\nQuestion 2.1\nHow many records are there per ticker value in the trading.prices table?\n\nSELECT\n  ticker,\n  COUNT(*) AS record_count\nFROM trading.prices\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nrecord_count\n\n\n\n\nBTC\n1702\n\n\nETH\n1702\n\n\n\n\n\n\n\nQuestion 2.2\nWhat is the maximum, minimum values for the price column for both Bitcoin and Ethereum in 2020?\n\nSELECT \n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nmin_price\nmax_price\n\n\n\n\nBTC\n4826.0\n28949.4\n\n\nETH\n107.9\n751.8\n\n\n\n\n\n\n\nQuestion 2.3\nWhat is the annual minimum, maximum and average price for each ticker?\n\nInclude a calendar_year column with the year from 2017 through to 2021\nCalculate a spread column which calculates the difference between the min and max prices\nRound the average price output to 2 decimal places\nSort the output in chronological order with Bitcoin records before Ethereum within each year\n\n\nSELECT\n  EXTRACT(YEAR FROM market_date) AS calendar_year,\n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price,\n  ROUND(AVG(price)::NUMERIC, 2) AS avg_price,\n  MAX(price) - MIN(price) AS spread\nFROM trading.prices\nGROUP BY calendar_year, ticker\nORDER BY calendar_year, ticker\n\n\nDisplaying records 1 - 10\n\n\ncalendar_year\nticker\nmin_price\nmax_price\navg_price\nspread\n\n\n\n\n2017\nBTC\n785.40\n19345.50\n3981.07\n18560.10\n\n\n2017\nETH\n8.20\n799.98\n220.34\n791.78\n\n\n2018\nBTC\n3228.70\n17172.30\n7552.16\n13943.60\n\n\n2018\nETH\n83.81\n1380.00\n481.33\n1296.19\n\n\n2019\nBTC\n3397.70\n13063.80\n7371.82\n9666.10\n\n\n2019\nETH\n104.55\n338.54\n180.99\n233.99\n\n\n2020\nBTC\n4826.00\n28949.40\n11111.63\n24123.40\n\n\n2020\nETH\n107.90\n751.80\n307.30\n643.90\n\n\n2021\nBTC\n29359.90\n63540.90\n44353.55\n34181.00\n\n\n2021\nETH\n729.12\n4167.78\n2199.12\n3438.66\n\n\n\n\n\n\n\nQuestion 2.4\nWhat is the monthly average of the price column for each ticker from January 2020 and after?\n\nCreate a month_start column with the first day of each month\nSort the output by ticker in alphabetical order and months in chronological order\nRound the average_price column to 2 decimal places\n\n\nSELECT\n  ticker,\n  DATE_TRUNC('MON', market_date)::DATE AS month_start,\n  ROUND(AVG(price)::NUMERIC, 2) AS average_price\nFROM trading.prices\nWHERE market_date >= '2020-01-01'\nGROUP BY ticker, month_start\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\naverage_price\n\n\n\n\nBTC\n2020-01-01\n8378.80\n\n\nBTC\n2020-02-01\n9636.56\n\n\nBTC\n2020-03-01\n6863.11\n\n\nBTC\n2020-04-01\n7211.03\n\n\nBTC\n2020-05-01\n9253.55\n\n\nBTC\n2020-06-01\n9481.85\n\n\nBTC\n2020-07-01\n9592.10\n\n\nBTC\n2020-08-01\n11638.41\n\n\nBTC\n2020-09-01\n10643.33\n\n\nBTC\n2020-10-01\n11888.36"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 3️⃣: Case When Statements",
    "text": "PART 3️⃣: Case When Statements\n\nQuestion 3.1\nConvert the volume column in the trading.prices table with an adjusted integer value to take into the unit values\n\nReturn only the market_date, price, volume and adjusted_volume columns for the first 10 days of August 2021 for Ethereum only\n\n\nSELECT\n  market_date,\n  price,\n  volume,    \n  CASE\n    WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000 \n    WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000000 \n    WHEN volume = '-' THEN 0\n    END AS adjusted_volume\nFROM trading.prices\nWHERE ticker = 'ETH'\n  AND market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY market_date\n\n\nDisplaying records 1 - 10\n\n\nmarket_date\nprice\nvolume\nadjusted_volume\n\n\n\n\n2021-08-01\n2556.23\n1.20M\n1200000\n\n\n2021-08-02\n2608.04\n970.67K\n970670\n\n\n2021-08-03\n2506.65\n158.45K\n158450\n\n\n2021-08-04\n2725.29\n1.23M\n1230000\n\n\n2021-08-05\n2827.21\n1.65M\n1650000\n\n\n2021-08-06\n2889.43\n1.06M\n1060000\n\n\n2021-08-07\n3158.00\n64.84K\n64840\n\n\n2021-08-08\n3012.07\n1.25M\n1250000\n\n\n2021-08-09\n3162.93\n1.44M\n1440000\n\n\n2021-08-10\n3140.71\n1.12M\n1120000\n\n\n\n\n\n\n\nQuestion 3.2\nHow many “breakout” days were there in 2020 where the price column is greater than the open column for each ticker? In the same query also calculate the number of “non breakout” days where the price column was lower than or equal to the open column.\n\nSELECT\n  ticker,\n  SUM(CASE WHEN price > open THEN 1 ELSE 0 END) AS breakout_days,\n  SUM(CASE WHEN price <= open THEN 1 ELSE 0 END) AS non_breakout_days\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nbreakout_days\nnon_breakout_days\n\n\n\n\nBTC\n207\n159\n\n\nETH\n200\n166\n\n\n\n\n\n\n\nQuestion 3.3\nWhat was the final quantity Bitcoin and Ethereum held by all Data With Danny mentors based off the trading.transactions table?\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'SELL' THEN -quantity ELSE quantity END) AS final_btc_holding\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nfinal_btc_holding\n\n\n\n\nBTC\n42848.67\n\n\nETH\n32801.04\n\n\n\n\n\nDivided by quantity bought and quantity sold.\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'BUY' THEN quantity ELSE 0 END) AS qty_bought,\n  SUM(CASE WHEN txn_type = 'SELL' THEN quantity ELSE 0 END) AS qty_sold\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nqty_bought\nqty_sold\n\n\n\n\nBTC\n53250.15\n10401.485\n\n\nETH\n42599.20\n9798.154"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 4️⃣: Window Functions",
    "text": "PART 4️⃣: Window Functions\n\nQuestion 4.1\nWhat are the market_date, price and volume and price_rank values for the days with the top 5 highest price values for each tickers in the trading.prices table?\n\nThe price_rank column is the ranking for price values for each ticker with rank = 1 for the highest value.\nReturn the output for Bitcoin, followed by Ethereum in price rank order.\n\n\nWITH cte_rank AS (\n  SELECT\n    ticker,\n    market_date,\n    price, \n    volume,\n    RANK() OVER (PARTITION BY ticker ORDER BY price DESC) AS price_rank\n  FROM trading.prices\n)\n\nSELECT *\nFROM cte_rank\nWHERE price_rank <= 5\nORDER BY ticker, price_rank\n\n\nDisplaying records 1 - 10\n\n\nticker\nmarket_date\nprice\nvolume\nprice_rank\n\n\n\n\nBTC\n2021-04-13\n63540.90\n126.56K\n1\n\n\nBTC\n2021-04-15\n63216.00\n76.97K\n2\n\n\nBTC\n2021-04-14\n62980.40\n130.43K\n3\n\n\nBTC\n2021-04-16\n61379.70\n136.85K\n4\n\n\nBTC\n2021-03-13\n61195.30\n134.64K\n5\n\n\nETH\n2021-05-11\n4167.78\n1.27M\n1\n\n\nETH\n2021-05-14\n4075.38\n2.06M\n2\n\n\nETH\n2021-05-10\n3947.90\n2.70M\n3\n\n\nETH\n2021-05-09\n3922.23\n1.94M\n4\n\n\nETH\n2021-05-08\n3905.55\n1.34M\n5\n\n\n\n\n\n\nQuestion 4.1 -  solution\n\nmembers <- read_csv(\"data/members_tbl.csv\")\nprices <- read_csv(\"data/prices_tbl.csv\")\ntransactions <- read_csv(\"data/transactions_tbl.csv\")\n\nprices %>% head()\n\n# A tibble: 6 × 8\n  ticker market_date price  open  high   low volume  change\n  <chr>  <date>      <dbl> <dbl> <dbl> <dbl> <chr>   <chr> \n1 ETH    2021-08-29  3178. 3244. 3282. 3163. 582.04K -2.04%\n2 ETH    2021-08-28  3244. 3274. 3285. 3212. 466.21K -0.91%\n3 ETH    2021-08-27  3274. 3094. 3280. 3063. 839.54K 5.82% \n4 ETH    2021-08-26  3094. 3228. 3250. 3057. 118.44K -4.17%\n5 ETH    2021-08-25  3228. 3172. 3247. 3081. 923.13K 1.73% \n6 ETH    2021-08-24  3173. 3323. 3358. 3151. 988.82K -4.41%\n\n\n\n# R solution\nprices %>% \n  select(ticker, market_date, price, volume) %>% \n  group_by(ticker) %>% \n  arrange(ticker, desc(price)) %>% \n  mutate(price_rank = row_number()) %>% \n  filter(price_rank <=5)\n\n# A tibble: 10 × 5\n# Groups:   ticker [2]\n   ticker market_date  price volume  price_rank\n   <chr>  <date>       <dbl> <chr>        <int>\n 1 BTC    2021-04-13  63541. 126.56K          1\n 2 BTC    2021-04-15  63216  76.97K           2\n 3 BTC    2021-04-14  62980. 130.43K          3\n 4 BTC    2021-04-16  61380. 136.85K          4\n 5 BTC    2021-03-13  61195. 134.64K          5\n 6 ETH    2021-05-11   4168. 1.27M            1\n 7 ETH    2021-05-14   4075. 2.06M            2\n 8 ETH    2021-05-10   3948. 2.70M            3\n 9 ETH    2021-05-09   3922. 1.94M            4\n10 ETH    2021-05-08   3906. 1.34M            5\n\n\n\n\n\nQuestion 4.2\nCalculate a 7 day rolling average for the price and volume columns in the trading.prices table for each ticker.\n\nReturn only the first 10 days of August 2021\n\n\n-- Step 1 - Adjusted prices CTE\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\n-- Step 2 - Moving Averages CTE\ncte_moving_averages AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    AVG(price) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_price,\n    volume,\n    AVG(volume) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_volume\n  FROM cte_adjusted_prices\n)\n\n-- final output\nSELECT * FROM cte_moving_averages\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date;\n\n\n20 records\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nmoving_avg_price\nvolume\nmoving_avg_volume\n\n\n\n\nBTC\n2021-08-01\n39878.30\n40052.657\n80330\n103645.71\n\n\nBTC\n2021-08-02\n39168.40\n40322.914\n74810\n88957.14\n\n\nBTC\n2021-08-03\n38130.30\n40134.100\n260\n74674.29\n\n\nBTC\n2021-08-04\n39736.90\n40096.057\n79220\n64717.14\n\n\nBTC\n2021-08-05\n40867.20\n40219.743\n130600\n72617.14\n\n\nBTC\n2021-08-06\n42795.40\n40304.314\n111930\n74542.86\n\n\nBTC\n2021-08-07\n44614.20\n40741.529\n112840\n84284.29\n\n\nBTC\n2021-08-08\n43792.80\n41300.743\n105250\n87844.29\n\n\nBTC\n2021-08-09\n46284.30\n42317.300\n117080\n93882.86\n\n\nBTC\n2021-08-10\n45593.80\n43383.514\n80550\n105352.86\n\n\nETH\n2021-08-01\n2556.23\n2394.166\n1200000\n1069824.29\n\n\nETH\n2021-08-02\n2608.04\n2448.239\n970670\n938491.43\n\n\nETH\n2021-08-03\n2506.65\n2477.729\n158450\n782555.71\n\n\nETH\n2021-08-04\n2725.29\n2538.611\n1230000\n819850.00\n\n\nETH\n2021-08-05\n2827.21\n2602.366\n1650000\n963742.86\n\n\nETH\n2021-08-06\n2889.43\n2663.577\n1060000\n968028.57\n\n\nETH\n2021-08-07\n3158.00\n2752.979\n64840\n904851.43\n\n\nETH\n2021-08-08\n3012.07\n2818.099\n1250000\n911994.29\n\n\nETH\n2021-08-09\n3162.93\n2897.369\n1440000\n979041.43\n\n\nETH\n2021-08-10\n3140.71\n2987.949\n1120000\n1116405.71\n\n\n\n\n\n\nQuestion 4.2 -  solution\n\nprices %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  group_by(ticker) %>% \n  arrange(ticker, market_date) %>% \n  mutate(moving_avg_price = zoo::rollmean(price, k = 7, align = \"right\", fill = NA),\n         moving_avg_volume = zoo::rollmean(volume, k = 7, align = \"right\", fill = NA)) %>% \n  select(ticker, market_date, price, moving_avg_price, volume, moving_avg_volume) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 6\n# Groups:   ticker [2]\n   ticker market_date  price moving_avg_price  volume moving_avg_volume\n   <chr>  <date>       <dbl>            <dbl>   <dbl>             <dbl>\n 1 BTC    2021-08-01  39878.           40053.   80330           103646.\n 2 BTC    2021-08-02  39168.           40323.   74810            88957.\n 3 BTC    2021-08-03  38130.           40134.     260            74674.\n 4 BTC    2021-08-04  39737.           40096.   79220            64717.\n 5 BTC    2021-08-05  40867.           40220.  130600            72617.\n 6 BTC    2021-08-06  42795.           40304.  111930            74543.\n 7 BTC    2021-08-07  44614.           40742.  112840            84284.\n 8 BTC    2021-08-08  43793.           41301.  105250            87844.\n 9 BTC    2021-08-09  46284.           42317.  117080            93883.\n10 BTC    2021-08-10  45594.           43384.   80550           105353.\n11 ETH    2021-08-01   2556.            2394. 1200000          1069824.\n12 ETH    2021-08-02   2608.            2448.  970670           938491.\n13 ETH    2021-08-03   2507.            2478.  158450           782556.\n14 ETH    2021-08-04   2725.            2539. 1230000           819850 \n15 ETH    2021-08-05   2827.            2602. 1650000           963743.\n16 ETH    2021-08-06   2889.            2664. 1060000           968029.\n17 ETH    2021-08-07   3158             2753.   64840           904851.\n18 ETH    2021-08-08   3012.            2818. 1250000           911994.\n19 ETH    2021-08-09   3163.            2897. 1440000           979041.\n20 ETH    2021-08-10   3141.            2988. 1120000          1116406.\n\n\n\n\n\nQuestion 4.3\nCalculate the monthly cumulative volume traded for each ticker in 2020\n\nSort the output by ticker in chronological order with the month_start as the first day of each month\n\n\nWITH cte_monthly_volume AS (\n  SELECT\n    ticker,\n    DATE_TRUNC('MON', market_date)::DATE AS month_start,\n    SUM(\n      CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END\n  ) AS monthly_volume\n  FROM trading.prices\n  WHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\n  GROUP BY ticker, month_start\n)\n\nSELECT\n  ticker,\n  month_start,\n  SUM(monthly_volume) OVER (\n    PARTITION BY ticker\n    ORDER BY month_start\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n  ) AS cumulative_monthly_volume\nFROM cte_monthly_volume\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\ncumulative_monthly_volume\n\n\n\n\nBTC\n2020-01-01\n23451920\n\n\nBTC\n2020-02-01\n46839130\n\n\nBTC\n2020-03-01\n94680450\n\n\nBTC\n2020-04-01\n134302740\n\n\nBTC\n2020-05-01\n172687010\n\n\nBTC\n2020-06-01\n188026610\n\n\nBTC\n2020-07-01\n201272600\n\n\nBTC\n2020-08-01\n216762630\n\n\nBTC\n2020-09-01\n300641440\n\n\nBTC\n2020-10-01\n303060020\n\n\n\n\n\n\nQuestion 4.3 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  filter(market_date >= \"2020-01-01\", market_date <= \"2020-12-31\") %>% \n  mutate(month_start = lubridate::floor_date(market_date, unit = \"month\")) %>% \n  group_by(ticker, month_start) %>% \n  summarise(monthly_volume = sum(\n    case_when(\n      str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n      str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n      str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n    )\n  ) %>% \n  ungroup() %>% \n  group_by(ticker) %>% \n  mutate(cumulative_monthly_volume = cumsum(monthly_volume)) %>% \n  ungroup()\n\n# A tibble: 24 × 4\n   ticker month_start monthly_volume cumulative_monthly_volume\n   <chr>  <date>               <dbl>                     <dbl>\n 1 BTC    2020-01-01        23451920                  23451920\n 2 BTC    2020-02-01        23387210                  46839130\n 3 BTC    2020-03-01        47841320                  94680450\n 4 BTC    2020-04-01        39622290                 134302740\n 5 BTC    2020-05-01        38384270                 172687010\n 6 BTC    2020-06-01        15339600                 188026610\n 7 BTC    2020-07-01        13245990                 201272600\n 8 BTC    2020-08-01        15490030                 216762630\n 9 BTC    2020-09-01        83878810                 300641440\n10 BTC    2020-10-01         2418580                 303060020\n# … with 14 more rows\n\n\n\n\n\nQuestion 4.4\nCalculate the daily percentage change in volume for each ticker in the trading.prices table\n\nPercentage change can be calculated as (current - previous) / previous\nMultiply the percentage by 100 and round the value to 2 decimal places\nReturn data for the first 10 days of August 2021\n\n\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\ncte_previous_volume AS (\n  SELECT\n    ticker,\n    market_date,\n    volume,\n    LAG(volume) OVER (PARTITION BY ticker ORDER BY market_date) AS previous_volume\n  FROM cte_adjusted_prices\n  WHERE volume != 0\n)\n  \nSELECT\n  ticker,\n  market_date,\n  volume,\n  previous_volume,\n  ROUND(100 * (volume - previous_volume) / previous_volume, 2) AS daily_change\nFROM cte_previous_volume\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date\n\n\n20 records\n\n\nticker\nmarket_date\nvolume\nprevious_volume\ndaily_change\n\n\n\n\nBTC\n2021-08-01\n80330\n44650\n79.91\n\n\nBTC\n2021-08-02\n74810\n80330\n-6.87\n\n\nBTC\n2021-08-03\n260\n74810\n-99.65\n\n\nBTC\n2021-08-04\n79220\n260\n30369.23\n\n\nBTC\n2021-08-05\n130600\n79220\n64.86\n\n\nBTC\n2021-08-06\n111930\n130600\n-14.30\n\n\nBTC\n2021-08-07\n112840\n111930\n0.81\n\n\nBTC\n2021-08-08\n105250\n112840\n-6.73\n\n\nBTC\n2021-08-09\n117080\n105250\n11.24\n\n\nBTC\n2021-08-10\n80550\n117080\n-31.20\n\n\nETH\n2021-08-01\n1200000\n507080\n136.65\n\n\nETH\n2021-08-02\n970670\n1200000\n-19.11\n\n\nETH\n2021-08-03\n158450\n970670\n-83.68\n\n\nETH\n2021-08-04\n1230000\n158450\n676.27\n\n\nETH\n2021-08-05\n1650000\n1230000\n34.15\n\n\nETH\n2021-08-06\n1060000\n1650000\n-35.76\n\n\nETH\n2021-08-07\n64840\n1060000\n-93.88\n\n\nETH\n2021-08-08\n1250000\n64840\n1827.82\n\n\nETH\n2021-08-09\n1440000\n1250000\n15.20\n\n\nETH\n2021-08-10\n1120000\n1440000\n-22.22\n\n\n\n\n\n\nQuestion 4.4 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  arrange(ticker, market_date) %>% \n  mutate(volume_prev_day = lag(volume)) %>% \n  mutate(daily_change_perc = round(100*(volume - volume_prev_day)/volume_prev_day, 2)) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 5\n   ticker market_date  volume volume_prev_day daily_change_perc\n   <chr>  <date>        <dbl>           <dbl>             <dbl>\n 1 BTC    2021-08-01    80330           44650             79.9 \n 2 BTC    2021-08-02    74810           80330             -6.87\n 3 BTC    2021-08-03      260           74810            -99.6 \n 4 BTC    2021-08-04    79220             260          30369.  \n 5 BTC    2021-08-05   130600           79220             64.9 \n 6 BTC    2021-08-06   111930          130600            -14.3 \n 7 BTC    2021-08-07   112840          111930              0.81\n 8 BTC    2021-08-08   105250          112840             -6.73\n 9 BTC    2021-08-09   117080          105250             11.2 \n10 BTC    2021-08-10    80550          117080            -31.2 \n11 ETH    2021-08-01  1200000          507080            137.  \n12 ETH    2021-08-02   970670         1200000            -19.1 \n13 ETH    2021-08-03   158450          970670            -83.7 \n14 ETH    2021-08-04  1230000          158450            676.  \n15 ETH    2021-08-05  1650000         1230000             34.2 \n16 ETH    2021-08-06  1060000         1650000            -35.8 \n17 ETH    2021-08-07    64840         1060000            -93.9 \n18 ETH    2021-08-08  1250000           64840           1828.  \n19 ETH    2021-08-09  1440000         1250000             15.2 \n20 ETH    2021-08-10  1120000         1440000            -22.2"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 5️⃣: Table Joins",
    "text": "PART 5️⃣: Table Joins\n\nQuestion 5.1 - Inner Joins\nWhich top 3 mentors have the most Bitcoin quantity? Return the first_name of the mentors and sort the output from highest to lowest total_quantity.\n\nSELECT\n  m.first_name,\n  SUM(\n    CASE\n      WHEN t.txn_type = 'BUY' THEN t.quantity \n      WHEN t.txn_type = 'SELL' THEN -t.quantity \n      END\n  ) AS total_quantity\nFROM trading.transactions t\nINNER JOIN trading.members m\n  ON t.member_id = m.member_id\nWHERE ticker = 'BTC' \nGROUP By m.first_name\nORDER BY total_quantity DESC\nLIMIT 3\n\n\n3 records\n\n\nfirst_name\ntotal_quantity\n\n\n\n\nNandita\n4160.220\n\n\nLeah\n4046.091\n\n\nAyush\n3945.198\n\n\n\n\n\n\n\nQuestion 5.2 - Left Joins\nShow the market_date values which have less than 5 transactions? Sort the output in reverse chronological order.\n\nSELECT \n  p.market_date,\n  COUNT(t.txn_id) AS transaction_count\nFROM trading.prices p\nLEFT JOIN trading.transactions t\n  ON p.market_date = t.txn_date\n  AND p.ticker = t.ticker\nGROUP BY p.market_date\nHAVING COUNT(t.txn_id) < 5\nORDER BY p.market_date DESC\n\n\n8 records\n\n\nmarket_date\ntransaction_count\n\n\n\n\n2021-08-29\n0\n\n\n2021-08-28\n0\n\n\n2021-07-17\n3\n\n\n2021-01-06\n4\n\n\n2020-01-17\n4\n\n\n2019-07-15\n4\n\n\n2019-06-14\n3\n\n\n2018-10-20\n4\n\n\n\n\n\n\n\nQuestion 5.3 - Multiple Table Joins\nPart 1: Calculate the Dollar Cost Average\nWhat is the dollar cost average (btc_dca) for all Bitcoin purchases by region for each calendar year?\n\nCreate a column called year_start and use the start of the calendar year\nThe dollar cost average calculation is btc_dca = SUM(quantity x price) / SUM(quantity)\n\nPart 2: Yearly Dollar Cost Average Ranking\nUse this btc_dca value to generate a dca_ranking column for each year\n\nThe region with the lowest btc_dca each year has a rank of 1\n\nPart 3: Dollar Cost Average Yearly Percentage Change\nCalculate the yearly percentage change in DCA for each region to 2 decimal places\n\nThis calculation is (current - previous) / previous\n\nFinally order the output by region and year_start columns.\n\nWITH cte_dollar_cost_average AS (\n  SELECT\n    DATE_TRUNC('YEAR', transactions.txn_date)::DATE AS year_start,\n    members.region,\n    SUM(transactions.quantity * prices.price) / SUM(transactions.quantity) AS btc_dca\n  FROM trading.transactions\n  INNER JOIN trading.prices\n    ON transactions.ticker = prices.ticker\n    AND transactions.txn_date = prices.market_date\n  INNER JOIN trading.members\n    ON transactions.member_id = members.member_id\n  WHERE transactions.ticker = 'BTC'\n    AND transactions.txn_type = 'BUY'\n  GROUP BY year_start, members.region\n),\n  \ncte_window_functions AS (\n  SELECT\n    year_start,\n    region,\n    btc_dca,\n    RANK() OVER (PARTITION BY year_start ORDER BY btc_dca) AS dca_ranking,\n    LAG(btc_dca) OVER (PARTITION BY region ORDER BY year_start) AS previous_btc_dca\n  FROM cte_dollar_cost_average\n)\n  \nSELECT\n  year_start,\n  region,\n  btc_dca,\n  dca_ranking,\n  ROUND(\n    (100 * (btc_dca - previous_btc_dca) / previous_btc_dca)::NUMERIC,\n    2\n  ) AS dca_percentage_change\nFROM cte_window_functions\nORDER BY region, year_start\n\n\nDisplaying records 1 - 10\n\n\nyear_start\nregion\nbtc_dca\ndca_ranking\ndca_percentage_change\n\n\n\n\n2017-01-01\nAfrica\n3987.626\n4\nNA\n\n\n2018-01-01\nAfrica\n7690.713\n3\n92.86\n\n\n2019-01-01\nAfrica\n7368.820\n4\n-4.19\n\n\n2020-01-01\nAfrica\n11114.125\n3\n50.83\n\n\n2021-01-01\nAfrica\n44247.215\n2\n298.12\n\n\n2017-01-01\nAsia\n4002.939\n5\nNA\n\n\n2018-01-01\nAsia\n7829.999\n4\n95.61\n\n\n2019-01-01\nAsia\n7267.679\n1\n-7.18\n\n\n2020-01-01\nAsia\n10759.621\n2\n48.05\n\n\n2021-01-01\nAsia\n44570.901\n4\n314.24\n\n\n\n\n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "",
    "text": "The Bike Sales Database represents a bicycle manufacturer, including tables for products (bikes), customers (bike shops), and transactions (orders).\nIt consists of 3 tables:\n\nbikes table, which includes bicycle models, descriptions, and unit prices that are produced by the manufacturer.\nbikeshops table, which includes customers that the bicycle manufacturer has sold to.\norderlines table, which includes transactional data such as order ID, order line, date, customer, product, and quantity sold.\n\nbike_sales database is the local Postgres database stored on my machine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\nlibrary(lubridate)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database.\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"bike_sales\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> bike_sales@localhost:5432\n\n\n\n# list the database table names\ndbListTables(mycon)\n\n[1] \"bikeshops\"  \"bikes\"      \"orderlines\"\n\n\n\n# read the bikeshops table\ndbReadTable(mycon, \"bikeshops\") %>% head()\n\n  bikeshop.id                bikeshop.name       location\n1           1 Pittsburgh Mountain Machines Pittsburgh, PA\n2           2     Ithaca Mountain Climbers     Ithaca, NY\n3           3      Columbus Race Equipment   Columbus, OH\n4           4               Detroit Cycles    Detroit, MI\n5           5             Cincinnati Speed Cincinnati, OH\n6           6    Louisville Race Equipment Louisville, KY\n\n\n\n# read the bikes table\ndbReadTable(mycon, \"bikes\") %>% head()\n\n  bike.id                          model                description price\n1       1        Supersix Evo Black Inc. Road - Elite Road - Carbon 12790\n2       2       Supersix Evo Hi-Mod Team Road - Elite Road - Carbon 10660\n3       3 Supersix Evo Hi-Mod Dura Ace 1 Road - Elite Road - Carbon  7990\n4       4 Supersix Evo Hi-Mod Dura Ace 2 Road - Elite Road - Carbon  5330\n5       5     Supersix Evo Hi-Mod Utegra Road - Elite Road - Carbon  4260\n6       6               Supersix Evo Red Road - Elite Road - Carbon  3940\n\n\n\n# read the orderlines table\ndbReadTable(mycon, \"orderlines\") %>% head()\n\n  order.id order.line order.date customer.id product.id quantity\n1        1          1 2011-01-07           2         48        1\n2        1          2 2011-01-07           2         52        1\n3        2          1 2011-01-10          10         76        1\n4        2          2 2011-01-10          10         52        1\n5        3          1 2011-01-10           6          2        1\n6        3          2 2011-01-10           6         50        1\n\n\n\n# a simple query example\ndbGetQuery(mycon, \n          \"SELECT model, price \n           FROM bikes WHERE price > 10000 \n           ORDER BY price DESC\")\n\n                     model price\n1  Supersix Evo Black Inc. 12790\n2    Scalpel-Si Black Inc. 12790\n3  Habit Hi-Mod Black Inc. 12250\n4          F-Si Black Inc. 11190\n5 Supersix Evo Hi-Mod Team 10660\n\n\n\n\n\nIn all three tables there are dots in column names. This is not a good practice and I first had to figure out how to join the tables without an error! Here is the solution:\n\nbike_orderlines_joined <- dbGetQuery(mycon, \n\n'SELECT * \nFROM orderlines \nLEFT JOIN bikes\nON orderlines.\"product.id\" = bikes.\"bike.id\"\nLEFT JOIN bikeshops\nON orderlines.\"customer.id\" = bikeshops.\"bikeshop.id\"')\n\nhead(bike_orderlines_joined)\n\n  order.id order.line order.date customer.id product.id quantity bike.id\n1        1          1 2011-01-07           2         48        1      48\n2        1          2 2011-01-07           2         52        1      52\n3        2          1 2011-01-10          10         76        1      76\n4        2          2 2011-01-10          10         52        1      52\n5        3          1 2011-01-10           6          2        1       2\n6        3          2 2011-01-10           6         50        1      50\n                     model                       description price bikeshop.id\n1          Jekyll Carbon 2 Mountain - Over Mountain - Carbon  6070           2\n2         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970           2\n3      Beast of the East 1       Mountain - Trail - Aluminum  2770          10\n4         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970          10\n5 Supersix Evo Hi-Mod Team        Road - Elite Road - Carbon 10660           6\n6          Jekyll Carbon 4 Mountain - Over Mountain - Carbon  3200           6\n              bikeshop.name        location\n1  Ithaca Mountain Climbers      Ithaca, NY\n2  Ithaca Mountain Climbers      Ithaca, NY\n3         Kansas City 29ers Kansas City, KS\n4         Kansas City 29ers Kansas City, KS\n5 Louisville Race Equipment  Louisville, KY\n6 Louisville Race Equipment  Louisville, KY\n\n\n\nglimpse(bike_orderlines_joined)\n\nRows: 15,644\nColumns: 13\n$ order.id      <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,…\n$ order.line    <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1,…\n$ order.date    <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-01…\n$ customer.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ product.id    <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ quantity      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,…\n$ bike.id       <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ model         <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Eas…\n$ description   <chr> \"Mountain - Over Mountain - Carbon\", \"Mountain - Over Mo…\n$ price         <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570, …\n$ bikeshop.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ bikeshop.name <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\", …\n$ location      <chr> \"Ithaca, NY\", \"Ithaca, NY\", \"Kansas City, KS\", \"Kansas C…\n\n\nDisconnecting from the database.\n\ndbDisconnect(mycon)\n\n\n\n\n\nbike_orderlines <- bike_orderlines_joined %>% \n  # rename columns - replacing \".\" with \"_\"\n  set_names(names(.) %>% str_replace_all(\"\\\\.\", \"_\")) %>% \n  # remove the unnecessary columns \n  select(-c(customer_id, product_id, bike_id, bikeshop_id)) %>% \n  # separate description into category_1, category_2, and frame_material\n  separate(description, \n           c(\"category_1\", \"category_2\", \"frame_material\"),\n           sep = \" - \") %>% \n  # separate location into city and state\n  separate(location,\n           c(\"city\", \"state\"),\n           sep = \", \") %>%\n  # create a new column total_price\n  mutate(total_price = price * quantity) %>% \n  # reorder columns\n  select(contains(c(\"date\", \"id\", \"order\")),\n         quantity, price, total_price,\n         everything()) \n\nbike_orderlines %>% head()\n\n  order_date order_id order_line quantity price total_price\n1 2011-01-07        1          1        1  6070        6070\n2 2011-01-07        1          2        1  5970        5970\n3 2011-01-10        2          1        1  2770        2770\n4 2011-01-10        2          2        1  5970        5970\n5 2011-01-10        3          1        1 10660       10660\n6 2011-01-10        3          2        1  3200        3200\n                     model category_1    category_2 frame_material\n1          Jekyll Carbon 2   Mountain Over Mountain         Carbon\n2         Trigger Carbon 2   Mountain Over Mountain         Carbon\n3      Beast of the East 1   Mountain         Trail       Aluminum\n4         Trigger Carbon 2   Mountain Over Mountain         Carbon\n5 Supersix Evo Hi-Mod Team       Road    Elite Road         Carbon\n6          Jekyll Carbon 4   Mountain Over Mountain         Carbon\n              bikeshop_name        city state\n1  Ithaca Mountain Climbers      Ithaca    NY\n2  Ithaca Mountain Climbers      Ithaca    NY\n3         Kansas City 29ers Kansas City    KS\n4         Kansas City 29ers Kansas City    KS\n5 Louisville Race Equipment  Louisville    KY\n6 Louisville Race Equipment  Louisville    KY\n\n\n\nbike_orderlines %>% glimpse()\n\nRows: 15,644\nColumns: 13\n$ order_date     <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-0…\n$ order_id       <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7…\n$ order_line     <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1…\n$ quantity       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1…\n$ price          <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ total_price    <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ model          <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Ea…\n$ category_1     <chr> \"Mountain\", \"Mountain\", \"Mountain\", \"Mountain\", \"Road\",…\n$ category_2     <chr> \"Over Mountain\", \"Over Mountain\", \"Trail\", \"Over Mounta…\n$ frame_material <chr> \"Carbon\", \"Carbon\", \"Aluminum\", \"Carbon\", \"Carbon\", \"Ca…\n$ bikeshop_name  <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\",…\n$ city           <chr> \"Ithaca\", \"Ithaca\", \"Kansas City\", \"Kansas City\", \"Loui…\n$ state          <chr> \"NY\", \"NY\", \"KS\", \"KS\", \"KY\", \"KY\", \"KY\", \"KY\", \"KY\", \"…"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Lollipop Chart: Top N Customers",
    "text": "Lollipop Chart: Top N Customers\nQuestion: How much purchasing power is in top 10 customers?\nGoal is to visualize top N customers in terms of Revenue, including cumulative percentage.\n\nData manipulation\n\nn <- 10\n\ntop_customers <- bike_orderlines %>% \n  select(bikeshop_name, total_price) %>% \n  mutate(bikeshop_name = as_factor(bikeshop_name) %>% fct_lump_n(n = n, w = total_price)) %>% \n  group_by(bikeshop_name) %>% \n  summarise(revenue = sum(total_price)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_reorder(revenue)) %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_relevel(\"Other\", after = 0)) %>% \n  arrange(desc(bikeshop_name)) %>% \n  # revenue text\n  mutate(revenue_text = scales::dollar(revenue, scale = 1e-06, suffix = \"M\")) %>% \n  # cumulative percent\n  mutate(cum_pct = cumsum(revenue) / sum(revenue)) %>% \n  mutate(cum_pct_text = scales::percent(cum_pct)) %>% \n  # rank\n  mutate(rank = row_number()) %>% \n  mutate(rank = ifelse(rank == max(rank), NA_integer_, rank)) %>% \n  # label text\n  mutate(label_text = str_glue(\"Rank: {rank}\\nRev: {revenue_text}\\nCumPct: {cum_pct_text}\")) \n\ntop_customers\n\n# A tibble: 11 × 7\n   bikeshop_name                 revenue revenue…¹ cum_pct cum_p…²  rank label…³\n   <fct>                           <dbl> <chr>       <dbl> <chr>   <int> <glue> \n 1 Kansas City 29ers            11535455 $11.54M     0.162 16.2%       1 Rank: …\n 2 Denver Bike Shop              7697670 $7.70M      0.271 27.1%       2 Rank: …\n 3 Ithaca Mountain Climbers      6299335 $6.30M      0.359 35.9%       3 Rank: …\n 4 Phoenix Bi-peds               4168535 $4.17M      0.418 41.8%       4 Rank: …\n 5 Oklahoma City Race Equipment  3450040 $3.45M      0.467 46.7%       5 Rank: …\n 6 Las Vegas Cycles              3073615 $3.07M      0.510 51.0%       6 Rank: …\n 7 New Orleans Velocipedes       2761825 $2.76M      0.549 54.9%       7 Rank: …\n 8 Wichita Speed                 2380385 $2.38M      0.582 58.2%       8 Rank: …\n 9 Miami Race Equipment          2057130 $2.06M      0.611 61.1%       9 Rank: …\n10 Minneapolis Bike Shop         2023220 $2.02M      0.640 64.0%      10 Rank: …\n11 Other                        25585120 $25.59M     1     100.0%     NA Rank: …\n# … with abbreviated variable names ¹​revenue_text, ²​cum_pct_text, ³​label_text\n\n\n\n\nData visualization\n\ntop_customers %>% \n  ggplot(aes(revenue, bikeshop_name)) +\n  # geometries\n  geom_segment(aes(xend = 0, yend = bikeshop_name), \n               color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1],\n               size = 1) +\n  geom_point(color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1], \n             size = 3) +\n  geom_label(aes(label = label_text), \n             hjust = \"left\", \n             size = 3,\n             nudge_x = 0.8e+06) +\n  # formatting\n  scale_x_continuous(labels = scales::dollar_format(scale = 1e-06, suffix = \"M\")) +\n  labs(title = str_glue(\"Top {n} customers in terms of revenue, with cumulative percentage\"),\n       subtitle = str_glue(\"Top {n} customers contribute {top_customers$cum_pct_text[n]} of purchasing power.\"),\n       x = \"Revenue ($M)\",\n       y = \"Customer\",\n       caption = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\")) +\n  expand_limits(x = max(top_customers$revenue) + 6e+06) +\n  # theme\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Heatmap: Customers’ Purchasing Habits",
    "text": "Heatmap: Customers’ Purchasing Habits\nQuestion: Do specific customers have a purchasing preference?\nGoal is to visualize heatmap of proportion of sales by Secondary Product Category.\n\nData manipulation\n\npct_sales_by_customer <- bike_orderlines %>% \n  select(bikeshop_name, category_1, category_2, quantity) %>% \n  group_by(bikeshop_name, category_1, category_2) %>% \n  summarise(total_qty = sum(quantity)) %>% \n  ungroup() %>% \n  group_by(bikeshop_name) %>% \n  mutate(pct = total_qty / sum(total_qty)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = as.factor(bikeshop_name) %>% fct_rev()) %>%  \n  mutate(bikeshop_name_num = as.numeric(bikeshop_name))\n    \npct_sales_by_customer   \n\n# A tibble: 270 × 6\n   bikeshop_name      category_1 category_2         total_qty    pct bikeshop_…¹\n   <fct>              <chr>      <chr>                  <dbl>  <dbl>       <dbl>\n 1 Albuquerque Cycles Mountain   Cross Country Race        48 0.168           30\n 2 Albuquerque Cycles Mountain   Fat Bike                   9 0.0315          30\n 3 Albuquerque Cycles Mountain   Over Mountain             13 0.0455          30\n 4 Albuquerque Cycles Mountain   Sport                     35 0.122           30\n 5 Albuquerque Cycles Mountain   Trail                     38 0.133           30\n 6 Albuquerque Cycles Road       Cyclocross                 7 0.0245          30\n 7 Albuquerque Cycles Road       Elite Road                69 0.241           30\n 8 Albuquerque Cycles Road       Endurance Road            54 0.189           30\n 9 Albuquerque Cycles Road       Triathalon                13 0.0455          30\n10 Ann Arbor Speed    Mountain   Cross Country Race        32 0.0532          29\n# … with 260 more rows, and abbreviated variable name ¹​bikeshop_name_num\n\n\n\n\nData visualization\n\npct_sales_by_customer %>% \n  ggplot(aes(category_2, bikeshop_name)) + \n  # geometries\n  geom_tile(aes(fill = pct)) +\n  geom_text(aes(label = scales::percent(pct, accuracy = 0.1)),\n            size = 3,\n            color = ifelse(pct_sales_by_customer$pct >= 0.15, \"white\", \"black\")) +\n  facet_wrap(~ category_1, scales = \"free_x\") + \n  # formatting\n  scale_fill_gradient(low = \"white\", high = tidyquant::palette_light()[1]) + \n  labs(title = \"Heatmap of Purchasing Habits\", \n       subtitle = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\"),\n       x = \"Bike Type\",\n       y = \"Customer\") + \n  # theme\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = tidyquant::palette_light()[1], \n                                        color = \"white\"), \n        strip.text = element_text(color = \"white\", size = 11), \n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\nTop 3 customers that prefer mountain bikes:\n\nIthaca Mountain Climbers\nPittsburgh Mountain Machines\nTampa 29ers\n\nTop 3 customers that prefer road bikes:\n\nAnn Arbor Speed\nAustin Cruisers\nIndianapolis Velocipedes\n\n\nThat’s it! I hope you like it. For those wondering where I learned to make plots like this… in a fabulous course Data Science for Business Part 1 by Matt Dancho. This is probably the best course on R and I highly recommend it."
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#problem-statement",
    "href": "posts/customer-segmentation/cust-segm-report.html#problem-statement",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Problem Statement",
    "text": "Problem Statement\nMarketing team would like to increase email campaign engagement by segmenting the customer-base using their buying habits."
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#solution-summary",
    "href": "posts/customer-segmentation/cust-segm-report.html#solution-summary",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Solution Summary",
    "text": "Solution Summary\nThe 4 customer segments were identified and given descriptions based on the customer’s top product purchases.\n\nSegment 1 Preferences: Road Bikes, Below $3200 (Economical Models) - 27%\nSegment 2 Preferences: Mountain Bikes, Above $3200 (Premium Models) - 10%\nSegment 3 Preferences: Road Bikes, Above $3200 (Premium Models) - 20%\nSegment 4 Preferences: Both Road and Mountain, Below $3200 (Economical Models) - 43%"
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#customer-preferences",
    "href": "posts/customer-segmentation/cust-segm-report.html#customer-preferences",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Customer Preferences",
    "text": "Customer Preferences\n\nHeat Map\nOur customer-base consists of 30 bike shops. Several customers have purchasing preferences for Road or Mountain Bikes based on the proportion of bikes purchased by category (mountain or road) and sub-category (Over Mountain, Trail, Elite Road, etc).\n\n\n\n\n\n\n\n\nCustomer Segmentation\nThis is a 2D Projection based on customer similarity that exposes 4 clusters, which are key segments in the customer base.\n\n\n\n\n\n\n\n\nCustomer Preferences By Segment\nThe 4 customer segments were given descriptions based on the customer’s top product purchases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: The table below is sortable. You can sort a column by clicking on its header."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html",
    "href": "posts/shiny-mass-shootings/mass-shootings.html",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "",
    "text": "This post will be regularly updated with each new case.\nLast update on June 02, 2023."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#introduction",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#introduction",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Introduction",
    "text": "Introduction\nMass shootings have been a topic of intense discussion in the United States. A public “database” of mass shootings since 1982 has been made available by the Mother Jones, a non-profit organization. This “database” is stored in a Google spreadsheet. You can access it here and download as a CSV file.\nThere are many definitions of mass shooting. Here is what Britannica has to say:\n\nMass shooting, also called active shooter incident, as defined by the U.S. Federal Bureau of Investigation (FBI), an event in which one or more individuals are “actively engaged in killing or attempting to kill people in a populated area. Implicit in this definition is the shooter’s use of a firearm.” The FBI has not set a minimum number of casualties to qualify an event as a mass shooting, but U.S. statute (the Investigative Assistance for Violent Crimes Act of 2012) defines a “mass killing” as “3 or more killings in a single incident”."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data overview",
    "text": "Data overview\n\nlibrary(tidyverse)\nlibrary(tidygeocoder)\nlibrary(plotly)\ntheme_set(theme_classic())\n\nmass_shootings <- read_csv(\"data/mass_shootings_usa_1982-2023.csv\")\n\nmass_shootings %>% glimpse()\n\nRows: 144\nColumns: 24\n$ case                             <chr> \"New Mexico neighborhood shooting\", \"…\n$ location...2                     <chr> \"Farmington, New Mexico\", \"Allen, Tex…\n$ date                             <chr> \"5/15/23\", \"5/6/23\", \"4/10/23\", \"3/27…\n$ summary                          <chr> \"Beau Wilson, 18, opened fire in a re…\n$ fatalities                       <dbl> 3, 8, 5, 6, 3, 7, 11, 6, 5, 3, 5, 3, …\n$ injured                          <dbl> 6, 7, 8, 6, 5, 1, 10, 6, 25, 2, 2, 2,…\n$ total_victims                    <dbl> 9, 15, 13, 12, 8, 8, 21, 12, 30, 5, 7…\n$ location...8                     <chr> \"Other\", \"Other\", \"workplace\", \"Schoo…\n$ age_of_shooter                   <chr> \"18\", \"33\", \"25\", \"28\", \"43\", \"67\", \"…\n$ prior_signs_mental_health_issues <chr> \"yes\", \"yes\", \"yes\", \"-\", \"-\", \"-\", \"…\n$ mental_health_details            <chr> \"-\", \"Reportedly had a history of men…\n$ weapons_obtained_legally         <chr> \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"-…\n$ where_obtained                   <chr> \"-\", \"-\", \"gun dealership in Louisvil…\n$ weapon_type                      <chr> \"semiautiomatic rifle; semiautomatic …\n$ weapon_details                   <chr> \"AR-15-style rifle\", \"AR-15-style rif…\n$ race                             <chr> \"White\", \"Latino\", \"White\", \"White\", …\n$ gender                           <chr> \"M\", \"M\", \"M\", \"F (\\\"identifies as tr…\n$ sources                          <chr> \"https://www.cbsnews.com/news/farming…\n$ mental_health_sources            <chr> \"https://www.nbcnews.com/news/us-news…\n$ sources_additional_age           <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ latitude                         <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ longitude                        <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ type                             <chr> \"mass\", \"Mass\", \"Mass\", \"Mass\", \"Mass…\n$ year                             <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2…\n\n\nWe have 144 cases, described with 24 variables. At first glance, this dataset clearly needs extensive cleaning."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n🧹 Step 1. Initial cleaning\nThe first cleaning step includes:\n\nselecting columns of interest,\nreplacing the character value \"-\" with NA in all columns with character data type,\nconverting date column from character to date data type,\nrenaming location columns,\nconverting character data type to numeric for specific columns.\n\n\nmass_shootings_cln <- mass_shootings %>% \n  select(1:6, 8:10, 12, 16, 17, 21:24) %>% \n  mutate(across(where(is.character), ~na_if(., \"-\"))) %>% \n  mutate(date = lubridate::mdy(date)) %>% \n  rename(location = location...2, location_2 = location...8) %>% \n  mutate_at(c(\"injured\", \"age_of_shooter\", \"latitude\", \"longitude\"), as.numeric)\n  \nmass_shootings_cln %>% glimpse()\n\nRows: 144\nColumns: 16\n$ case                             <chr> \"New Mexico neighborhood shooting\", \"…\n$ location                         <chr> \"Farmington, New Mexico\", \"Allen, Tex…\n$ date                             <date> 2023-05-15, 2023-05-06, 2023-04-10, …\n$ summary                          <chr> \"Beau Wilson, 18, opened fire in a re…\n$ fatalities                       <dbl> 3, 8, 5, 6, 3, 7, 11, 6, 5, 3, 5, 3, …\n$ injured                          <dbl> 6, 7, 8, 6, 5, 1, 10, 6, 25, 2, 2, 2,…\n$ location_2                       <chr> \"Other\", \"Other\", \"workplace\", \"Schoo…\n$ age_of_shooter                   <dbl> 18, 33, 25, 28, 43, 67, 72, 31, 22, 2…\n$ prior_signs_mental_health_issues <chr> \"yes\", \"yes\", \"yes\", NA, NA, NA, \"yes…\n$ weapons_obtained_legally         <chr> \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", NA…\n$ race                             <chr> \"White\", \"Latino\", \"White\", \"White\", …\n$ gender                           <chr> \"M\", \"M\", \"M\", \"F (\\\"identifies as tr…\n$ latitude                         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ longitude                        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ type                             <chr> \"mass\", \"Mass\", \"Mass\", \"Mass\", \"Mass…\n$ year                             <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2…\n\n\n🔎 Are there any duplicates? No.\n\nsum(duplicated(mass_shootings))\n\n[1] 0\n\n\n🔎 Number of missing values, NA, per column.\n\nmass_shootings_cln %>% \n  summarise_all(~sum(is.na(.))) %>% \n  # transposing for better visibility\n  pivot_longer(cols = everything(), names_to = \"column\", values_to = \"n_missing\")\n\n# A tibble: 16 × 2\n   column                           n_missing\n   <chr>                                <int>\n 1 case                                     0\n 2 location                                 0\n 3 date                                     0\n 4 summary                                  0\n 5 fatalities                               0\n 6 injured                                  0\n 7 location_2                               0\n 8 age_of_shooter                           2\n 9 prior_signs_mental_health_issues        28\n10 weapons_obtained_legally                16\n11 race                                    12\n12 gender                                   0\n13 latitude                                18\n14 longitude                               18\n15 type                                     0\n16 year                                     0\n\n\n18 of the most recent cases don’t have location coordinates at all. We’ll address this in the final cleanup step.\n\n\n🧹 Step 2. Fixing unique values of categorical variables\n🔎 Let’s take a look at the unique values of the gender column.\n\nmass_shootings_cln %>% \n  count(gender, sort = TRUE) \n\n# A tibble: 6 × 2\n  gender                                                                       n\n  <chr>                                                                    <int>\n1 \"Male\"                                                                      70\n2 \"M\"                                                                         68\n3 \"Female\"                                                                     2\n4 \"Male & Female\"                                                              2\n5 \"F\"                                                                          1\n6 \"F (\\\"identifies as transgender\\\" and \\\"Audrey Hale is a biological wom…     1\n\n\nAlmost all categorical variables need unique values correction.\nTo make a long story short, I’ll correct them all in one step using case_when function, and we’ll look at them later during the analysis.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  mutate(gender = case_when(gender == \"F\" ~ \"Female\",\n                            gender == \"M\" ~ \"Male\", \n                            gender %>% str_detect(\"transgender\")~\"Female (transgender)\",\n                            TRUE ~ gender),\n         race = case_when(race == \"white\" ~ \"White\",\n                          race == \"black\" ~ \"Black\",\n                          race == \"unclear\" ~ \"Unclear\",\n                          TRUE ~ race),\n         location_2 = \n           case_when(location_2 %in% c(\"workplace\", \"\\nWorkplace\") ~ \"Workplace\",\n                                location_2 == \"Other\\n\" ~ \"Other\",\n                                location_2 == \"religious\" ~ \"Religious\",\n                                TRUE ~ location_2),\n         prior_signs_mental_health_issues = \n           case_when(prior_signs_mental_health_issues == \"yes\" ~ \"Yes\",\n                     prior_signs_mental_health_issues == \"TBD\" ~ \"To be determined\",\n                     TRUE ~ prior_signs_mental_health_issues),\n         weapons_obtained_legally = \n           case_when(weapons_obtained_legally %in% c(\"yes\", \"\\nYes\") ~ \"Yes\",\n                     weapons_obtained_legally == \"TBD\" ~ \"To be determined\",\n                     weapons_obtained_legally %>% str_detect(\"Kelley\") ~ \"Unknown\",\n                     weapons_obtained_legally %>% str_detect(\"some\") ~ \"Partially\",\n                     TRUE ~ weapons_obtained_legally))\n\n\n\n🧹 Step 3. Geocoding locations with missing coordinates\nThere are 17 cases with missing location coordinates. In this step we’ll convert locations to coordinates with geocoding. and use them later to create a leaflet map for a shiny app.\nThe tidygeocoder package provides geocoding services. It’s designed to work easily with the tidyverse. It also provides access to several different geocoding services, including LocationIQ which I’m going to use here. LocationIQ is a freemium service that provides a free tier, which doesn’t require you to give them your billing details. When you sign up to LocationIQ, they’ll take you to the Manage Your API Access Tokens page, which is where we obtain our API token. Next, you need to provide the tidygeocoder package with your API key.\nYou can also use the Nominatim (“osm”) geocoding service (OpenStreetMap) which can be specified with the method argument (method = \"osm\"). I found LocationIQ to be faster.\nThe first step is to select only locations with missing coordinates and geocode them.\n\ngeocoded_locations <- mass_shootings_cln %>% \n  filter(is.na(latitude) | is.na(longitude)) %>% \n  select(location) %>% \n  geocode(location, method = \"iq\")\n\ngeocoded_locations %>% \n  mutate(across(where(is.numeric), ~ num(., digits = 6)))\n\n# A tibble: 18 × 3\n   location                         lat        long\n   <chr>                      <num:.6!>   <num:.6!>\n 1 Farmington, New Mexico     36.729115 -108.205445\n 2 Allen, Texas               33.103174  -96.670550\n 3 Louisville, Kentucky       38.254238  -85.759407\n 4 Nashville, Tennessee       36.162277  -86.774298\n 5 East Lansing, Michigan     42.732031  -84.472168\n 6 Half Moon Bay, California  37.463552 -122.428586\n 7 Monterey Park, California  34.051522 -118.129807\n 8 Chesapeake, Virginia       36.718371  -76.246680\n 9 Colorado Springs, Colorado 38.833958 -104.825348\n10 Charlottesville, Virginia  38.029306  -78.476678\n11 Hedingham, North Carolina  35.808108  -78.541245\n12 Greenwood, Indiana         39.613658  -86.106653\n13 Highland Park, Illinois    42.181692  -87.800344\n14 Birmingham, Alabama        33.520682  -86.802433\n15 Smithsburg, Maryland       39.654819  -77.572768\n16 Tulsa, Oklahoma            36.156312  -95.992744\n17 Uvalde, Texas              29.300357  -99.773318\n18 Buffalo, New York          42.886717  -78.878392\n\n\nThe next step is to join mass shootings table with geocoded locations and replace missing latitudes and longitudes with geocoded.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  left_join(geocoded_locations, by = \"location\") %>% \n  mutate(latitude = ifelse(is.na(latitude), lat, latitude),\n         longitude = ifelse(is.na(longitude), long, longitude))\n\n🔎 Checking for null values.\n\nsum(is.na(mass_shootings_cln$latitude))\n\n[1] 0\n\nsum(is.na(mass_shootings_cln$longitude))\n\n[1] 0\n\n\nOK, this looks fine."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\n❕ Writing a function\nTo count unique values for all categorical variables separately, I’ll write a function, count_unique, to avoid copying and pasting a block of code several times.\nHere we have a special case where we have to pass a dataframe column name (variable) to a function argument. The solution is to embrace the argument by surrounding it in doubled braces, like group_by({{ var }}).\n\ncount_unique <- function(data, var) {\n  \n  data %>%\n    group_by({{ var }}) %>%    \n    summarise(count = n(), .groups = \"drop\") %>% \n    mutate(percent = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n    arrange(desc(count))\n\n}\n\n\n\n📄 Breakdown by categorical variables\n\nGender\n\ncount_unique(mass_shootings_cln, gender)\n\n# A tibble: 4 × 3\n  gender               count percent\n  <chr>                <int> <chr>  \n1 Male                   138 95.8%  \n2 Female                   3 2.1%   \n3 Male & Female            2 1.4%   \n4 Female (transgender)     1 0.7%   \n\n\n\n\nRace\n\ncount_unique(mass_shootings_cln, race) \n\n# A tibble: 8 × 3\n  race            count percent\n  <chr>           <int> <chr>  \n1 White              76 52.8%  \n2 Black              25 17.4%  \n3 Latino             12 8.3%   \n4 <NA>               12 8.3%   \n5 Asian              10 6.9%   \n6 Other               5 3.5%   \n7 Native American     3 2.1%   \n8 Unclear             1 0.7%   \n\n\n\n\nSpecific location\n\ncount_unique(mass_shootings_cln, location_2)\n\n# A tibble: 6 × 3\n  location_2 count percent\n  <chr>      <int> <chr>  \n1 Other         55 38.2%  \n2 Workplace     52 36.1%  \n3 School        22 15.3%  \n4 Religious      8 5.6%   \n5 Military       6 4.2%   \n6 Airport        1 0.7%   \n\n\n\n\nPrior signs of mental health issues\n\ncount_unique(mass_shootings_cln, prior_signs_mental_health_issues)\n\n# A tibble: 6 × 3\n  prior_signs_mental_health_issues count percent\n  <chr>                            <int> <chr>  \n1 Yes                                 69 47.9%  \n2 <NA>                                28 19.4%  \n3 Unclear                             24 16.7%  \n4 No                                  17 11.8%  \n5 To be determined                     5 3.5%   \n6 Unknown                              1 0.7%   \n\n\n\n\nWeapons obtained legally\n\ncount_unique(mass_shootings_cln, weapons_obtained_legally)\n\n# A tibble: 6 × 3\n  weapons_obtained_legally count percent\n  <chr>                    <int> <chr>  \n1 Yes                         97 67.4%  \n2 No                          16 11.1%  \n3 <NA>                        16 11.1%  \n4 To be determined             7 4.9%   \n5 Unknown                      7 4.9%   \n6 Partially                    1 0.7%   \n\n\n\n\nType\n\ncount_unique(mass_shootings_cln, type)\n\n# A tibble: 3 × 3\n  type  count percent\n  <chr> <int> <chr>  \n1 Mass    122 84.7%  \n2 Spree    21 14.6%  \n3 mass      1 0.7%   \n\n\nNote: Spree shootings here have three or more victims in a short time in multiple locations.\n\n\n\n📊 Age of shooter distribution\n\n\nCode for creating the age_group column\n# create \"age group\" column\nmass_shootings_cln <-  mass_shootings_cln %>% \n  mutate(age_group = case_when(\n    age_of_shooter >= 10 & age_of_shooter <= 14 ~ \"10-15\",\n    age_of_shooter <= 19 ~ \"15-20\",\n    age_of_shooter <= 24 ~ \"20-25\",\n    age_of_shooter <= 29 ~ \"25-30\",\n    age_of_shooter <= 34 ~ \"30-35\",\n    age_of_shooter <= 39 ~ \"35-40\",\n    age_of_shooter <= 44 ~ \"40-45\",\n    age_of_shooter <= 49 ~ \"45-50\",\n    age_of_shooter <= 54 ~ \"50-55\",\n    age_of_shooter <= 59 ~ \"55-60\",\n    age_of_shooter <= 64 ~ \"60-65\",\n    age_of_shooter <= 69 ~ \"65-70\",\n    age_of_shooter <= 74 ~ \"70-75\"))\n\n\n\np1 <- mass_shootings_cln %>% \n  filter(!is.na(age_group)) %>% \n  group_by(age_group) %>% \n  summarise(count = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n  mutate(label_text = str_glue(\"Age group: {age_group}\n                               Count: {count}\n                               Percent: {percent}\")) %>%\n  ggplot(aes(x = age_group, y = count, text = label_text)) +\n  geom_col(width = 0.7, fill = \"indianred\") +\n  labs(title = \"Age Distribution\", x = \"age group\") \n\nggplotly(p1, tooltip = \"text\")\n\n\n\n\n\n\n\nThe vast majority of shooters were between 15 and 50 years old.\nThe age distribution is bimodal, with one mode around 23 years of age and a second mode around 41 years of age.\nMost shooters were in the 20-25 age group (18.3 %), followed by 40-45 (16.2 %) and 25-30 (16.2 %).\n\n🔎 Who was the youngest shooter?\n\nindex <- which.min(mass_shootings_cln$age_of_shooter)\n\nmass_shootings_cln[index, ] %>% \n  select(case, date, summary, fatalities) %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\ncase\ndate\nsummary\nfatalities\n\n\n\n\nWestside Middle School killings\n1998-03-24\nMitchell Scott Johnson, 13, and Andrew Douglas Golden, 11, two juveniles, ambushed students and teachers as they left the school; they were apprehended by police at the scene.\n5\n\n\n\n\n\n\n\n📊 Number of cases per year\n\np2 <- mass_shootings_cln %>%\n  group_by(year) %>%\n  summarise(count = n()) %>% \n  ggplot(aes(year, count)) +\n  geom_col(fill = \"steelblue\") + \n  geom_smooth(method = \"loess\", se = FALSE, color = \"indianred\", size = 0.7) +\n  # geom_vline(xintercept = 2012, color = \"black\", linetype = \"dotted\") +\n  labs(title = \"Number of Cases per Year\") \n\nggplotly(p2)\n\n\n\n\n\n\n\nWe can see an increase in mass shootings in the last 12 years.\n2020 has a smaller number of cases probably due to Covid restrictions.\nThe data for 2023 is incomplete, but 7 cases in the first five months seems a lot.\n\n\n\n📊 Fatalities-Injured relationship\n\np3 <- mass_shootings_cln %>%\n  ggplot(aes(x = fatalities, y = injured)) +\n  geom_jitter() +\n  scale_y_sqrt() +\n  labs(title = \"Fatalities-Injured Relationship\")\n  \nggplotly(p3)\n\n\n\n\n\n\nPlease note that the Injured values are square root scaled for better visibility, but you can see the actual values by hovering over the points.\nSummary of fatalities\n\nsummary(mass_shootings_cln$fatalities)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   4.000   6.000   7.757   8.000  58.000 \n\n\nSummary of injured people\n\nsummary(mass_shootings_cln$injured)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    3.00   11.15   10.00  546.00 \n\n\n\n\n📊 Total fatalities by state\n\n🛠️ Data manipulation\n\n# create us states with abbreviations tibble\nstates_with_abbr <- \n  tibble(state = state.name, abbr = state.abb) %>% \n  bind_rows(tibble(state = \"District of Columbia\", abbr = \"DC\"))\n\n# data manipulation\nby_state <- mass_shootings_cln %>% \n  # recode D.d. to District of Columbia\n  mutate(location = ifelse(location == \"Washington, D.C.\", \n                           \"Washington, District of Columbia\", \n                           location)) %>% \n  # separate location into city and state\n  separate(location, c(\"city\", \"state\"), sep = \", \") %>% \n  # group and summarize\n  group_by(state) %>% \n  summarise(total_cases = n(),\n            total_fatalities = sum(fatalities), .groups = \"drop\") %>% \n  # add us states abbreviations\n  left_join(states_with_abbr, by = \"state\") %>% \n  # rearrange columns\n  select(state, abbr, everything())\n\n\n\n📈 Top ten states regarding number of cases and fatalities\n\nby_state %>% \n  arrange(-total_cases, -total_fatalities) %>% \n  head(10)\n\n# A tibble: 10 × 4\n   state        abbr  total_cases total_fatalities\n   <chr>        <chr>       <int>            <dbl>\n 1 California   CA             25              175\n 2 Texas        TX             13              159\n 3 Florida      FL             12              126\n 4 Colorado     CO              8               53\n 5 Washington   WA              7               37\n 6 New York     NY              5               40\n 7 Wisconsin    WI              5               28\n 8 Pennsylvania PA              5               27\n 9 Illinois     IL              5               25\n10 Virginia     VA              4               53\n\n\n\n\n📊 Total fatalities by state visualization\n\nby_state %>% \n  plot_geo(locationmode = 'USA-states') %>% \n  add_trace(z = ~total_fatalities,\n            locations = ~abbr,\n            color = ~total_fatalities,\n            colors = ~\"Reds\") %>% \n  layout(\n    geo = list(\n      scope = \"usa\",\n      projection = list(type = \"albers usa\"),\n      lakecolor = toRGB(\"white\")\n    )\n  )"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Shiny app",
    "text": "Shiny app\nThe app you can see below is embedded in this quarto document since my website is static. It was originally published on shinyapps.io and you can also interact with it here.\nA quick note: With a free account I have 25 active hours (when my applications are not idle). If these 25 active hours are exceeded, my app will not be available again until the following month cycle. Hope you get lucky! 😊\n📢 By clicking on each circle, you can read a summary of the mass shooting case.\n\n\n\n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "title": "Titanic Survival Exercises",
    "section": "",
    "text": "After auditing the HarvardX’s Data Science: Visualization course I’ve found this assessment way too interesting and fun. So I decided to put all my new skills together to perform exploratory data analysis on a classic machine learning dataset: Titanic survival! My goal is to provide answers entirely through visualizations."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "title": "Titanic Survival Exercises",
    "section": "Background",
    "text": "Background\nThe Titanic was a British ocean liner that struck an iceberg and sunk on its maiden voyage in 1912 from the United Kingdom to New York. More than 1,500 of the estimated 2,224 passengers and crew died in the accident, making this one of the largest maritime disasters ever outside of war. The ship carried a wide range of passengers of all ages and both genders, from luxury travelers in first-class to immigrants in the lower classes. However, not all passengers were equally likely to survive the accident. We use real data about a selection of 891 passengers to learn who was on the Titanic and which passengers were more likely to survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "title": "Titanic Survival Exercises",
    "section": "Libraries, Customizations, and Data",
    "text": "Libraries, Customizations, and Data\n\nlibrary(tidyverse)\nlibrary(titanic)\n\noptions(digits = 3)  \ntheme_set(theme_classic())\ncolors_sex <- c(\"mediumorchid1\", \"dodgerblue\")\ncolors_survived <- c(\"gray65\", \"lightgreen\")\n\nDefining the titanic dataset.\n\ntitanic <- titanic_train %>%\n  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare) %>%\n  mutate(Survived = factor(Survived),\n         Pclass = factor(Pclass),\n         Sex = factor(Sex))\n\nhead(titanic)\n\n  Survived Pclass    Sex Age SibSp Parch  Fare\n1        0      3   male  22     1     0  7.25\n2        1      1 female  38     1     0 71.28\n3        1      3 female  26     0     0  7.92\n4        1      1 female  35     1     0 53.10\n5        0      3   male  35     0     0  8.05\n6        0      3   male  NA     0     0  8.46\n\nstr(titanic)\n\n'data.frame':   891 obs. of  7 variables:\n $ Survived: Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass  : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Sex     : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age     : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp   : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch   : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Fare    : num  7.25 71.28 7.92 53.1 8.05 ..."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "title": "Titanic Survival Exercises",
    "section": "Question 1: Variable Types",
    "text": "Question 1: Variable Types\nInstructions: Inspect the data and also use ?titanic_train to learn more about the variables in the dataset. Match these variables from the dataset to their variable type. There is at least one variable of each type (ordinal categorical, non-ordinal (nominal) categorical, continuous, discrete).\nChecking if Age variable is discrete or continuous…\n\nunique(titanic$Age)\n\n [1] 22.00 38.00 26.00 35.00    NA 54.00  2.00 27.00 14.00  4.00 58.00 20.00\n[13] 39.00 55.00 31.00 34.00 15.00 28.00  8.00 19.00 40.00 66.00 42.00 21.00\n[25] 18.00  3.00  7.00 49.00 29.00 65.00 28.50  5.00 11.00 45.00 17.00 32.00\n[37] 16.00 25.00  0.83 30.00 33.00 23.00 24.00 46.00 59.00 71.00 37.00 47.00\n[49] 14.50 70.50 32.50 12.00  9.00 36.50 51.00 55.50 40.50 44.00  1.00 61.00\n[61] 56.00 50.00 36.00 45.50 20.50 62.00 41.00 52.00 63.00 23.50  0.92 43.00\n[73] 60.00 10.00 64.00 13.00 48.00  0.75 53.00 57.00 80.00 70.00 24.50  6.00\n[85]  0.67 30.50  0.42 34.50 74.00\n\n\nAge is a continuous variable.\n\n\n\nVariable\nDescription\nVariable Type\n\n\n\n\nSurvived\nPassenger Survival Indicator\nnominal categorical\n\n\nPclass\nPassenger Class\nordinal categorical\n\n\nSex\nSex\nnominal categorical\n\n\nAge\nAge\ncontinuous\n\n\nSibSp\nNumber of Siblings/Spouses Aboard\ndiscrete\n\n\nParch\nNumber of Parents/Children Aboard\ndiscrete\n\n\nFare\nPassenger Fare\ncontinuous"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "title": "Titanic Survival Exercises",
    "section": "Question 2: Demographics of Titanic Passengers",
    "text": "Question 2: Demographics of Titanic Passengers\nInstructions: Make density plots of age grouped by sex. Try experimenting with combinations of faceting, alpha blending, stacking and using variable counts on the y-axis to answer the following questions. Some questions may be easier to answer with different versions of the density plot.\n\ntitanic %>% \n  ggplot(aes(Age)) +\n  geom_density(aes(color = Sex), size = 0.7) +\n  scale_color_manual(values = colors_sex) +\n  geom_vline(xintercept = c(18, 35), linetype = 2) +\n  geom_text(aes(x = 18, y = 0.031, label= \"18\", hjust = 1.5)) +\n  geom_text(aes(x = 35, y = 0.031, label= \"35\", hjust = -0.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, fill = Sex)) +\n  geom_density(alpha = 0.3) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 17, linetype = 2) +\n  geom_text(aes(x = 17, y = 0.031, label= \"17\", hjust = 1.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Sex)) +\n  geom_density(alpha = 0.7) +\n  facet_grid(Sex ~ .) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 40, linetype = 2) + \n  geom_text(aes(x = 40, y = 14, label= \"40\", hjust = -0.5)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers\n\n✅ Females and males had the same general shape of age distribution.\n✅ } The age distribution was bimodal, with one mode around 25 years of age and a second - - smaller mode around 5 years of age.\n❌ There were more females than males.\n✅ The count of males of age 40 was higher than the count of females of age 40.\n✅ The proportion of males age 18-35 was higher than the proportion of females age 18-35.\n✅ The proportion of females under age 17 was higher than the proportion of males under age 17.\n❌ The oldest passengers were female."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "title": "Titanic Survival Exercises",
    "section": "Question 3: QQ-plot of Age Distribution",
    "text": "Question 3: QQ-plot of Age Distribution\nInstructions: Use geom_qq() to make a QQ-plot of passenger age and add an identity line with geom_abline(). Filter out any individuals with an age of NA first.\n\nparams <- titanic %>%\n  filter(!is.na(Age)) %>%\n  summarize(mean = mean(Age), sd = sd(Age))\n\nparams\n\n  mean   sd\n1 29.7 14.5\n\ntitanic %>% ggplot(aes(sample = Age)) + \n  geom_qq(dparams = params) +\n  geom_abline()\n\n\n\n\nWhich of the following is the correct plot according to the instructions above?\n\n✅ The plot above."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "title": "Titanic Survival Exercises",
    "section": "Question 4: Survival by Sex",
    "text": "Question 4: Survival by Sex\nInstructions: To answer the following questions, make barplots of the Survived and Sex variables using geom_bar(). Try plotting one variable and filling by the other variable. You may want to try the default plot, then try adding position = position_dodge() to geom_bar() to make separate bars for each group.\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Sex)) +\n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_sex)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(width =  0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Less than half of passengers survived.\n✅ Most of the survivors were female.\n❌ Most of the males survived.\n✅ Most of the females survived."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "title": "Titanic Survival Exercises",
    "section": "Question 5: Survival by Age",
    "text": "Question 5: Survival by Age\nInstructions: Make a density plot of age filled by survival status. Change the y-axis to count and set alpha = 0.2.\nThe following answers were offered for all three questions:\n\n0-8\n10-18\n18-30\n30-50\n50-70\n70-80\n\n\nWhich age group is the only group more likely to survive than die?\n\ntitanic %>% \n  ggplot(aes(Age, y = ..count.., fill = Survived)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = colors_survived) +\n  geom_vline(xintercept = 8, linetype = 2) +\n  geom_text(aes(x = 8, y = 14, label= \"8\", hjust = -0.5))\n\n\n\n\n\n✅ Age group 0-8.\n\n\n\nWhich age group had the most deaths?\nIt’s hard to tell from the previews plot I’ll have to make a new column Age group based on the offered answers.\n\ntitanic2 <- titanic %>%\n  filter(!is.na(Age)) %>% \n  mutate(`Age group` = case_when(\n    Age > 0 & Age <= 8 ~ \"0-8\",\n    Age > 10 & Age <= 18 ~ \"10-18\",\n    Age > 18 & Age <= 30 ~ \"18-30\",\n    Age > 30 & Age <= 50 ~ \"30-50\",\n    Age > 50 & Age <= 70 ~ \"50-70\",\n    Age > 70 & Age <= 80 ~ \"70-80\"\n    )\n  )\n\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 18-30.\n\n\n\nWhich age group had the highest proportion of deaths?\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(position = \"fill\", width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 70-80"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "title": "Titanic Survival Exercises",
    "section": "Question 6: Survival by Fare",
    "text": "Question 6: Survival by Fare\nInstructions: Filter the data to remove individuals who paid a fare of 0. Make a boxplot of fare grouped by survival status. Try a log2 transformation of fares. Add the data points with jitter and alpha blending.\n\nset.seed(123)\n\ntitanic %>% filter(Fare != 0) %>% \n  ggplot(aes(Survived, Fare)) +\n  geom_boxplot(fill = colors_survived, width = 0.5, alpha = 0.5) + \n  geom_jitter(width = 0.1, alpha = 0.2) +\n  scale_y_continuous(trans = \"log2\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Passengers who survived generally payed higher fares than those who did not survive.\n❌ The interquartile range for fares was smaller for passengers who survived.\n✅ The median fare was lower for passengers who did not survive.\n❌ Only one individual paid a fare around $500. That individual survived. (3 individuals survived)\n✅ Most individuals who paid a fare around $8 did not survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 7: Survival by Passenger Class",
    "text": "Question 7: Survival by Passenger Class\nInstructions: The Pclass variable corresponds to the passenger class. Make three barplots. For the first, make a basic barplot of passenger class filled by survival. For the second, make the same barplot but use the argument position = position_fill() to show relative proportions in each group instead of counts. For the third, make a barplot of survival filled by passenger class using position = position_fill()\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Pclass)) + \n  geom_bar(width = 0.7) +\n  geom_text(aes(label = ..count..), stat = \"count\", vjust = -1) +\n  expand_limits(y = 530) +\n  ylab(\"count\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Pclass)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") \n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ There were more third class passengers than passengers in the first two classes combined.\n❌ There were the fewest passengers in first class, second-most passengers in second class, and most passengers in third class.\n✅ Survival proportion was highest for first class passengers, followed by second class. Third-class had the lowest survival proportion.\n✅ Most passengers in first class survived. Most passengers in other classes did not survive.\n❌ The majority of survivors were from first class.\n✅ The majority of those who did not survive were from third class."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 8: Survival by Age, Sex and Passenger Class",
    "text": "Question 8: Survival by Age, Sex and Passenger Class\nInstructions: Create a grid of density plots for age, filled by survival status, with count on the y-axis, faceted by sex and passenger class.\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Pclass)) +\n  geom_density(alpha=0.5) \n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Survived)) +\n  geom_density(alpha=0.5) +\n  facet_grid((Sex ~ Pclass)) +\n  scale_fill_manual(values = colors_survived) + \n  theme(panel.border = element_rect(colour = \"black\", fill = NA)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ The largest group of passengers was third-class males.\n❌ The age distribution is the same across passenger classes.\n❌ The gender distribution is the same across passenger classes.\n✅ Most first-class and second-class females survived.\n✅ Almost all second-class males did not survive, with the exception of children.\n\n\nThat’s all. Thanks for reading!"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "",
    "text": "The goal of this project is to scrape data on the 200 best movies of the last decade from the Rotten Tomatoes website with the R rvest package, and finally create a dashboard in Tableau. The idea is to show all the movies in one place. Hovering over the movie should reveal relevant data in the tooltip for quick overview. Clicking on the movie should open the movie’s website for more information.\nI’ve learned so much while working on this project (like web scraping, writing functions, iteration, …). The purrr package for functional programming is super-cool. It allows iteration with just one line of code (a very handy replacement for for loops).\nI hope you’ll enjoy the process as much as I did. At times it was quite challenging, but that’s how we learn!\n\n\n\n# loading packages\nlibrary(tidyverse)\nlibrary(rvest)\n\nAre we allowed to scrape data from the Rotten Tomatoes website?\n\nrobotstxt::paths_allowed(\"https://www.rottentomatoes.com/\")\n\n[1] TRUE"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "Plan",
    "text": "Plan\nThe data will be scraped from this page. Since it doesn’t contain all the data I am interested in, I have to visit every movie’s web page on the list and scrape data from there. Here is the plan:\n\nScrape data from the main page: the urls of movies, and the urls of images.\nScrape title, year_genre_runtime, critics_score, audiaece_score, and synopsis from the first movie to develop the code.\nWrite a function that scrapes data based on movie’s URL.\nIteration - use this function to scrape data from each individual movie and create a data frame with the columns title, year_genre_runtime, critics_score, audiaece_score, synopsis, and url.\nDownload images\nPrepare data for Tableau\nCreate a dashboard in Tableau"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "1. Scraping data from the main page",
    "text": "1. Scraping data from the main page\nReading the main page with read_html().\n\nmain_url <- \"https://editorial.rottentomatoes.com/guide/the-200-best-movies-of-the-2010s/\"\nmain_page <- read_html(main_url)\n\n\n\n\n\n\n\nFigure 1: The main page\n\n\n\n\nI make use of the SelectorGadget to identify the tags for the relevant nodes. Here is the link for Chrome (recommended).\n\nExtracting urls of movies\nThe same nodes that contain the text for the titles also contain information on the links to individual movie pages for each title. We can extract this information using the html_attr() function, which extracts attributes.\n\nmovie_urls <- main_page %>% \n  html_nodes(\".article_movie_title a\") %>% \n  html_attr(\"href\")\n\nmovie_urls %>% head()\n\n[1] \"https://www.rottentomatoes.com/m/12_years_a_slave\"    \n[2] \"https://www.rottentomatoes.com/m/20_feet_from_stardom\"\n[3] \"https://www.rottentomatoes.com/m/45_years\"            \n[4] \"https://www.rottentomatoes.com/m/all_is_lost_2013\"    \n[5] \"https://www.rottentomatoes.com/m/amazing_grace_2018\"  \n[6] \"https://www.rottentomatoes.com/m/american_hustle\"     \n\n\n\n\nExtracting urls of images\n\nimage_urls <- main_page %>% \n  html_nodes(\".article_poster\") %>% \n  html_attr(\"src\")\n\nLet’s check the image for the 6th title.\n\nknitr::include_graphics(image_urls[6])"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "2. Scraping data for the first movie on the list",
    "text": "2. Scraping data for the first movie on the list\nNow I’m going to scrape data for the movie 12 Years a Slave in order to develop the code.\nReading page for the first movie.\n\nurl <- \"https://www.rottentomatoes.com/m/12_years_a_slave\"\nmovie_page <- read_html(url)\n\n\n\n\n\n\n\nFigure 2: Title, year, genre, runtime, critics and audience score\n\n\n\n\nScroll down the page and you’ll find the movie synopsis.\n\n\n\n\n\nFigure 3: Synopsis\n\n\n\n\n\nExtracting title\n\ntitle <- movie_page %>% \n  html_node(\".scoreboard__title\") %>% \n  html_text()\n\ntitle\n\n[1] \"12 Years a Slave\"\n\n\n\n\nExtracting year, genre, and runtime\n\nyear_genre_runtime <- movie_page %>% \n  html_node(\".scoreboard__info\") %>% \n  html_text()\n\nyear_genre_runtime\n\n[1] \"2013, History/Drama, 2h 14m\"\n\n\n\n\nExtracting critics score\nThe next two are tricky. I had to look at the page source and find them manually.\n\ncritics_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"tomatometerscore\") %>% \n  str_c(.,\"%\")\n\ncritics_score\n\n[1] \"95%\"\n\n\n\n\nExtracting audience score\n\naudience_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"audiencescore\") %>% \n  str_c(.,\"%\")\n\naudience_score\n\n[1] \"90%\"\n\n\n\n\nExtracting movie synopsis\n\nsynopsis <- movie_page %>% \n  html_node(\"#movieSynopsis\") %>% \n  html_text2()\n\nsynopsis\n\n[1] \"In the years before the Civil War, Solomon Northup (Chiwetel Ejiofor), a free black man from upstate New York, is kidnapped and sold into slavery in the South. Subjected to the cruelty of one malevolent owner (Michael Fassbender), he also finds unexpected kindness from another, as he struggles continually to survive and maintain some of his dignity. Then in the 12th year of the disheartening ordeal, a chance meeting with an abolitionist from Canada changes Solomon's life forever.\"\n\n\n\n\nMakinging a data frame of extracted elements\n\nmovie  <- tibble(title = title, \n                 year_genre_runtime = year_genre_runtime,\n                 critics_score = critics_score,\n                 audience_score = audience_score,\n                 synopsis = synopsis,  \n                 url = url)\n\nmovie %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"12 Years a Slave\"\n$ year_genre_runtime <chr> \"2013, History/Drama, 2h 14m\"\n$ critics_score      <chr> \"95%\"\n$ audience_score     <chr> \"90%\"\n$ synopsis           <chr> \"In the years before the Civil War, Solomon Northup…\n$ url                <chr> \"https://www.rottentomatoes.com/m/12_years_a_slave\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "3. Writing a function",
    "text": "3. Writing a function\nInstead of manually scraping individual movies, I’ll write a function to do the same.\n\nscrape_movie <- function(x, ...){\n  \n  movie_page <- read_html(x)\n  \n  title <- movie_page %>% \n    html_node(\".scoreboard__title\") %>% \n    html_text()\n  \n  year_genre_runtime <- movie_page %>% \n    html_node(\".scoreboard__info\") %>% \n    html_text()\n  \n  critics_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"tomatometerscore\") %>% \n    str_c(.,\"%\")\n  \n  audience_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"audiencescore\") %>% \n    str_c(.,\"%\")\n  \n  synopsis <- movie_page %>% \n    html_node(\"#movieSynopsis\") %>% \n    html_text2()\n  \n  movie_df <- tibble(title = title, \n                     year_genre_runtime = year_genre_runtime,\n                     critics_score = critics_score,\n                     audience_score = audience_score,\n                     synopsis = synopsis,\n                     url = x)\n  \n  return(movie_df)\n  \n}\n\n\nFunction in action\nNow that we have the scrape_movie() function, let’s scrape data for the movie “American Hustle”.\n\nscrape_movie(movie_urls[6]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"American Hustle\"\n$ year_genre_runtime <chr> \"2013, Crime/Drama, 2h 18m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"74%\"\n$ synopsis           <chr> \"Irving Rosenfeld (Christian Bale) dabbles in forge…\n$ url                <chr> \"https://www.rottentomatoes.com/m/american_hustle\"\n\n\nOr “Ex Machina” (an interesting SF movie).\n\n scrape_movie(movie_urls[53]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"Ex Machina\"\n$ year_genre_runtime <chr> \"2014, Sci-fi/Mystery & thriller, 1h 47m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"86%\"\n$ synopsis           <chr> \"Caleb Smith (Domhnall Gleeson) a programmer at a h…\n$ url                <chr> \"https://www.rottentomatoes.com/m/ex_machina\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "4. Iteration",
    "text": "4. Iteration\nTo make my workflow a little more efficient, I make use of the map_dfr() function from the purrr package to iterate over all movie pages. map_dfr() will apply the scrape_movie()function to each element in the vector of links, and return a data frame created by row-binding. It’s as simple as that.\n\nmovies <- map_dfr(movie_urls, scrape_movie)\n\nmovies \n\n# A tibble: 200 × 6\n   title                year_genre_runtime         criti…¹ audie…² synop…³ url  \n   <chr>                <chr>                      <chr>   <chr>   <chr>   <chr>\n 1 12 Years a Slave     2013, History/Drama, 2h 1… 95%     90%     In the… http…\n 2 20 Feet From Stardom 2013, Documentary, 1h 30m  99%     82%     Filmma… http…\n 3 45 Years             2015, Drama, 1h 33m        97%     67%     As the… http…\n 4 All Is Lost          2013, Adventure/Mystery &… 94%     64%     During… http…\n 5 Amazing Grace        2018, Documentary/Music, … 99%     80%     Singer… http…\n 6 American Hustle      2013, Crime/Drama, 2h 18m  92%     74%     Irving… http…\n 7 Amy                  2015, Documentary/Biograp… 95%     87%     Archiv… http…\n 8 Anomalisa            2015, Comedy/Drama, 1h 30m 91%     71%     An ins… http…\n 9 Ant-Man and The Wasp 2018, Action/Adventure, 1… 87%     80%     Scott … http…\n10 Apollo 11            2019, Documentary/History… 99%     90%     Never-… http…\n# … with 190 more rows, and abbreviated variable names ¹​critics_score,\n#   ²​audience_score, ³​synopsis"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "5. Downloading images",
    "text": "5. Downloading images\nI’ve already extracted urls of images in the first step and saved them to image_urls. Now I’m going to create a directory and directory paths for the images.\n\nfs::dir_create(\"images/top_200_images/\")\n\npaths <- c(str_c(\"images/top_200_images/\", sprintf(\"%0.3d\", 1:200), \".jpg\"))\n\npaths %>% head()\n\n[1] \"images/top_200_images/001.jpg\" \"images/top_200_images/002.jpg\"\n[3] \"images/top_200_images/003.jpg\" \"images/top_200_images/004.jpg\"\n[5] \"images/top_200_images/005.jpg\" \"images/top_200_images/006.jpg\"\n\n\nSince Tableau sorts images alphabetically (1, 11, 111, 2, 22, …) by default, these leading zeros will help Tableau to correctly match the images with the data so I don’t have to do it manually.\n\nDownloading images\nThis time I’ll use map2() function from the purrr package, It will apply the download.file() function to pairs of elements from two vectors, image_urls and paths.\n\nmap2(image_urls, paths, function(.x, .y) download.file(.x, .y, mode=\"wb\")) \n\nAre the images properly saved? Let’s read in the image for the first movie.\n\nknitr::include_graphics(\"images/top_200_images/001.jpg\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "6. Data wrangling",
    "text": "6. Data wrangling\nPreparing the final dataset for Tableau.\n\nmovies <- movies %>% \n  \n  # separate year_genre_runtime column into year, genre, and runtime\n  separate(year_genre_runtime, sep = \", \", into = c(\"year\", \"genre\", \"runtime\")) %>% \n  mutate(year = as.factor(year)) %>% \n  \n  # separate genre column into primary and secondary genre\n  separate(genre, sep = \"/\", into = c(\"genre_1\", \"genre_2\"), remove = FALSE) %>% \n  \n  # create id column with leading zeroes so Tableau can automatically match the images\n  mutate(id = sprintf(\"%0.3d\", 1:200)) %>% \n  select(id, everything())\n\nmovies %>% head()\n\n# A tibble: 6 × 11\n  id    title  year  genre genre_1 genre_2 runtime criti…¹ audie…² synop…³ url  \n  <chr> <chr>  <fct> <chr> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>\n1 001   12 Ye… 2013  Hist… History Drama   2h 14m  95%     90%     In the… http…\n2 002   20 Fe… 2013  Docu… Docume… <NA>    1h 30m  99%     82%     Filmma… http…\n3 003   45 Ye… 2015  Drama Drama   <NA>    1h 33m  97%     67%     As the… http…\n4 004   All I… 2013  Adve… Advent… Myster… 1h 45m  94%     64%     During… http…\n5 005   Amazi… 2018  Docu… Docume… Music   1h 27m  99%     80%     Singer… http…\n6 006   Ameri… 2013  Crim… Crime   Drama   2h 18m  92%     74%     Irving… http…\n# … with abbreviated variable names ¹​critics_score, ²​audience_score, ³​synopsis\n\n\n\n# number of unique values in genre column\nmovies$genre %>% unique() %>% length()\n\n[1] 59\n\n\n\n# unique values in genre_1\nmovies$genre_1 %>% unique()\n\n [1] \"History\"            \"Documentary\"        \"Drama\"             \n [4] \"Adventure\"          \"Crime\"              \"Comedy\"            \n [7] \"Action\"             \"Sci-fi\"             \"Romance\"           \n[10] \"Horror\"             \"Biography\"          \"Mystery & thriller\"\n[13] \"Kids & family\"      \"War\"                \"Fantasy\"           \n[16] \"Musical\"            \"Western\"           \n\n\n\n# unique values in genre_2\nmovies$genre_2 %>% unique()\n\n [1] \"Drama\"              NA                   \"Mystery & thriller\"\n [4] \"Music\"              \"Biography\"          \"Adventure\"         \n [7] \"History\"            \"Romance\"            \"Comedy\"            \n[10] \"Lgbtq+\"             \"Action\"             \"War\"               \n[13] \"Fantasy\"            \"Sci-fi\"             \"Crime\"             \n[16] \"Musical\"            \"Western\"            \"Anime\"             \n[19] \"Horror\"            \n\n\nFinding values in genre_2, that are not in genre_1. This will help when creating a list parameter for filtering by primary or secondary genre.\n\nsetdiff(movies$genre_2, movies$genre_1)\n\n[1] NA       \"Music\"  \"Lgbtq+\" \"Anime\" \n\n\n\nDT table\nIf you prefer to search a table for data, then this one is for you!\n\nmovies %>% \n  select(1:9) %>% \n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\n\n\nWriting file\nI choose to save the data in an excel file only because the csv will remove the leading zeros in the id column.\n\nmovies %>% writexl::write_xlsx(\"datasets/top_200_movies_2010s_rotten_tomatoes.xlsx\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "7. Tableau dashboard",
    "text": "7. Tableau dashboard\nThe final dashboard is created in Tableau. It’s actually a jitter plot, which separates overlapping movies with the same critics’ score.\nTo avoid two filters, one for primary and one for secondary genre, a list parameter is created that filters movies by primary or secondary genre, or “All” values.\nFor the best viewing experience, please click on the full screen in the bottom right corner.\nYou can nteract with the embedded dashboard below or go to Tableau Public. Enjoy!"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html",
    "href": "posts/credit-card-retention/canalysis.html",
    "title": "Bank Customer Retention Analysis",
    "section": "",
    "text": "In this hypothetical case study I’ve been hired as a financial analyst by the marketing department within Globe Bank International.\nThey are faces with more and more customers leaving their credit card services. They would really like to understand what characteristics lend themselves to someone who is going to churn so they can proactively go to the customer to provide them better services and turn customers’ decisions in the opposite direction.\nAs a part of my first analysis, they’ve asked me to take a look at the available data and help them understand how to increase customer retention."
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#problem-statement",
    "href": "posts/credit-card-retention/canalysis.html#problem-statement",
    "title": "Bank Customer Retention Analysis",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nWhat marketing campaigns could we implement to help reduce customer churn?"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#imports",
    "href": "posts/credit-card-retention/canalysis.html#imports",
    "title": "Bank Customer Retention Analysis",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nimport altair as alt\n\n# Show all columns (instead of cascading columns in the middle)\npd.set_option(\"display.max_columns\", None)\n# Don't show numbers in scientific notation\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\n\n# Enable plotting more than 5000 points\nalt.data_transformers.enable('default', max_rows=None)\n\nDataTransformerRegistry.enable('default')"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#dataset-overview",
    "href": "posts/credit-card-retention/canalysis.html#dataset-overview",
    "title": "Customer Retention Analysis",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe dataset consists of about 10,000 customers mentioning their age, salary, marital status, credit card limit, credit card category, etc. it’s available on Kaggle at the following link.\n\nData Dictionary\n\nCLIENTNUM - Unique identifier for the customer holding the account\nAttrition_Flag - Internal event (customer activity) variable - if the account is closed then 1 else 0\nCustomer_Age - Customer’s Age in Years\nGender - Male = M, Female = F\nDependent_count - Number of dependents\nEducation_Level - Educational Qualification of the account holder (example: high school, college graduate, etc.)\nMarital_Status - Married, Single, Divorced, Unknown\nIncome_Category - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, >\nCard_Category - Type of Card (Blue, Silver, Gold, Platinum)\nMonths_on_book - Period of relationship with bank\nTotal_Relationship_count - Total number of products held by the customer\nMonths_Inactive_12_mon - Noumber of months inactive in the last 12 months\nContacts_Count_12_mon - Number of Contacts in the last 12 months\nCredit_Limit - Credit Limit on the Credit Card\nTotal_Revolving_Bal - Total Revolving Balance on the Credit Card\nAvg_Open_To_Buy- Open to Buy Credit Line (Average of last 12 months)\nTotal_Amt_Chng_Q4_Q1 - Change in Transaction Amount (Q4 over Q1)\nTotal_Trans_Amt - Total Transaction Amount (Last 12 months)\nTotal_Trans_Ct - Total Transaction Count (Last 12 months)\nTotal_Ct_Chng_Q4_Q1 - Change in Transaction Count (Q4 over Q1)\nAvg_Utilization_Ratio - Average Card Utilization Ratio\n\n\n# Read in the data and remove the last two irrelevant columns\ndata = pd.read_csv(\"data/BankChurners.csv\").iloc[: , :-2]\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Attrition_Flag\n      Customer_Age\n      Gender\n      Dependent_count\n      Education_Level\n      Marital_Status\n      Income_Category\n      Card_Category\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n    \n  \n  \n    \n      0\n      768805383\n      Existing Customer\n      45\n      M\n      3\n      High School\n      Married\n      $60K - $80K\n      Blue\n      39\n      5\n      1\n      3\n      12691.00\n      777\n      11914.00\n      1.33\n      1144\n      42\n      1.62\n      0.06\n    \n    \n      1\n      818770008\n      Existing Customer\n      49\n      F\n      5\n      Graduate\n      Single\n      Less than $40K\n      Blue\n      44\n      6\n      1\n      2\n      8256.00\n      864\n      7392.00\n      1.54\n      1291\n      33\n      3.71\n      0.10\n    \n    \n      2\n      713982108\n      Existing Customer\n      51\n      M\n      3\n      Graduate\n      Married\n      $80K - $120K\n      Blue\n      36\n      4\n      1\n      0\n      3418.00\n      0\n      3418.00\n      2.59\n      1887\n      20\n      2.33\n      0.00\n    \n    \n      3\n      769911858\n      Existing Customer\n      40\n      F\n      4\n      High School\n      Unknown\n      Less than $40K\n      Blue\n      34\n      3\n      4\n      1\n      3313.00\n      2517\n      796.00\n      1.41\n      1171\n      20\n      2.33\n      0.76\n    \n    \n      4\n      709106358\n      Existing Customer\n      40\n      M\n      3\n      Uneducated\n      Married\n      $60K - $80K\n      Blue\n      21\n      5\n      1\n      0\n      4716.00\n      0\n      4716.00\n      2.17\n      816\n      28\n      2.50\n      0.00\n    \n  \n\n\n\n\n\ndata.shape\n\n(10127, 21)\n\n\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nData columns (total 21 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   CLIENTNUM                 10127 non-null  int64  \n 1   Attrition_Flag            10127 non-null  object \n 2   Customer_Age              10127 non-null  int64  \n 3   Gender                    10127 non-null  object \n 4   Dependent_count           10127 non-null  int64  \n 5   Education_Level           10127 non-null  object \n 6   Marital_Status            10127 non-null  object \n 7   Income_Category           10127 non-null  object \n 8   Card_Category             10127 non-null  object \n 9   Months_on_book            10127 non-null  int64  \n 10  Total_Relationship_Count  10127 non-null  int64  \n 11  Months_Inactive_12_mon    10127 non-null  int64  \n 12  Contacts_Count_12_mon     10127 non-null  int64  \n 13  Credit_Limit              10127 non-null  float64\n 14  Total_Revolving_Bal       10127 non-null  int64  \n 15  Avg_Open_To_Buy           10127 non-null  float64\n 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float64\n 17  Total_Trans_Amt           10127 non-null  int64  \n 18  Total_Trans_Ct            10127 non-null  int64  \n 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float64\n 20  Avg_Utilization_Ratio     10127 non-null  float64\ndtypes: float64(5), int64(10), object(6)\nmemory usage: 1.6+ MB"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#data-preprocessing-binning-customer-age",
    "href": "posts/credit-card-retention/canalysis.html#data-preprocessing-binning-customer-age",
    "title": "Customer Retention Analysis",
    "section": "Data Preprocessing (Binning Customer Age)",
    "text": "Data Preprocessing (Binning Customer Age)\n\nprint(data['Customer_Age'].min())\nprint(data['Customer_Age'].max())\n\n26\n73\n\n\n\nbins = [25, 30, 40, 50, 60, 70, 80]\nlabels = ['20s', '30s', '40s', '50s', '60s', '70s']\n\ndata['Customer_Age_bins'] = pd.cut(\n    data['Customer_Age'], \n    bins=bins, \n    labels=labels, \n    include_lowest=True, \n    right=False\n)\n\nLet’s verify that the behavior we expected from this method resulted in the right output. Since we overrode the inclusive values, we should see that 30 should show in the 30s bin vs the 20s bin.\n\ndata[data['Customer_Age'] == 30].head(3)\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Attrition_Flag\n      Customer_Age\n      Gender\n      Dependent_count\n      Education_Level\n      Marital_Status\n      Income_Category\n      Card_Category\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n      Customer_Age_bins\n    \n  \n  \n    \n      789\n      754654383\n      Existing Customer\n      30\n      M\n      2\n      High School\n      Married\n      Less than $40K\n      Blue\n      23\n      4\n      2\n      0\n      2997.00\n      1393\n      1604.00\n      1.12\n      1577\n      35\n      0.52\n      0.47\n      30s\n    \n    \n      803\n      711748983\n      Existing Customer\n      30\n      M\n      1\n      High School\n      Married\n      $60K - $80K\n      Blue\n      17\n      3\n      3\n      2\n      7906.00\n      1631\n      6275.00\n      0.89\n      1946\n      45\n      0.55\n      0.21\n      30s\n    \n    \n      890\n      779149683\n      Existing Customer\n      30\n      F\n      0\n      Graduate\n      Divorced\n      Unknown\n      Blue\n      13\n      5\n      1\n      2\n      7772.00\n      1890\n      5882.00\n      0.75\n      2585\n      75\n      0.83\n      0.24\n      30s"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#exploratory-data-analysis-eda",
    "href": "posts/credit-card-retention/canalysis.html#exploratory-data-analysis-eda",
    "title": "Bank Customer Retention Analysis",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n🔍 Percentage of churned customers\n\ndata['Attrition_Flag'].value_counts()   \n\nExisting Customer    8500\nAttrited Customer    1627\nName: Attrition_Flag, dtype: int64\n\n\nWe’ll use normalize=True to get percentages of churned and existing customers.\n\nchurned = data['Attrition_Flag'].value_counts(normalize=True)['Attrited Customer']\n\nprint(round(churned * 100 , 2), '% of customers have churned.')\n\n16.07 % of customers have churned.\n\n\n\n\n📝 Summary Statistics\n\ndata.describe()\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Customer_Age\n      Dependent_count\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n    \n  \n  \n    \n      count\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n    \n    \n      mean\n      739177606.33\n      46.33\n      2.35\n      35.93\n      3.81\n      2.34\n      2.46\n      8632.00\n      1162.81\n      7469.17\n      0.76\n      4404.09\n      64.86\n      0.71\n      0.27\n    \n    \n      std\n      36903783.45\n      8.02\n      1.30\n      7.99\n      1.55\n      1.01\n      1.11\n      9088.76\n      814.99\n      9090.68\n      0.22\n      3397.13\n      23.47\n      0.24\n      0.28\n    \n    \n      min\n      708082083.00\n      26.00\n      0.00\n      13.00\n      1.00\n      0.00\n      0.00\n      1438.30\n      0.00\n      3.00\n      0.00\n      510.00\n      10.00\n      0.00\n      0.00\n    \n    \n      25%\n      713036770.50\n      41.00\n      1.00\n      31.00\n      3.00\n      2.00\n      2.00\n      2555.00\n      359.00\n      1324.50\n      0.63\n      2155.50\n      45.00\n      0.58\n      0.02\n    \n    \n      50%\n      717926358.00\n      46.00\n      2.00\n      36.00\n      4.00\n      2.00\n      2.00\n      4549.00\n      1276.00\n      3474.00\n      0.74\n      3899.00\n      67.00\n      0.70\n      0.18\n    \n    \n      75%\n      773143533.00\n      52.00\n      3.00\n      40.00\n      5.00\n      3.00\n      3.00\n      11067.50\n      1784.00\n      9859.00\n      0.86\n      4741.00\n      81.00\n      0.82\n      0.50\n    \n    \n      max\n      828343083.00\n      73.00\n      5.00\n      56.00\n      6.00\n      6.00\n      6.00\n      34516.00\n      2517.00\n      34516.00\n      3.40\n      18484.00\n      139.00\n      3.71\n      1.00\n    \n  \n\n\n\n\n\nHere we can see:\n\nThe longest customer in this dataset has been around for 56 months or about 4 years and a half. (Max)\nThe average number of relationships a customer has is ~4. (Mean and median agree here)\nThe average credit limit is $8.6K, but the median credit limit is much lower at $4.5K. (signals some skew in this variable)\n\n\nprint('The average Total_Relationship_Count is', round(np.mean(data['Total_Relationship_Count']), 2), 'and the median is', np.median(data['Total_Relationship_Count']))\n\nThe average Total_Relationship_Count is 3.81 and the median is 4.0\n\n\n\nprint('The average Credit_Limit is $', round(np.mean(data['Credit_Limit']), 2), 'and the median is $', np.median(data['Credit_Limit']))\n\nThe average Credit_Limit is $ 8632.0 and the median is $ 4549.0\n\n\n\n\n📊 Distributions\n\nHistograms\nLet’s take a quick look at the distribution for Months_on_book.\n\nalt.Chart(data).mark_bar().encode(   \n    alt.X('Months_on_book', bin=alt.Bin(maxbins=30), title=\"Months on book\"),\n    alt.Y('count()')\n).properties(width=500)\n\n\n\n\n\n\nIt looks pretty normal outside of a very strong peak at ~36 months (roughly 3500 customers). If we had access to the owner to ask more questions about this, we would want to know if this was a data error (default months for those without a record), or if there was a major marketing campaign that brought in a lot of users 36 months ago.\nNow let’s take a look at the Credit_Limit variable that we looked at before:\n\nhist = alt.Chart(data).mark_bar().encode(   \n    alt.X('Credit_Limit', bin=alt.Bin(step=1500), title=\"Credit Limit\"),\n    alt.Y('count()')\n).properties(width=500)\n\naggregates = alt.Chart(data).transform_aggregate(\n    mean='mean(Credit_Limit)',\n    median='median(Credit_Limit)',\n).transform_fold(\n    ['mean', 'median']\n).mark_rule().encode(\n    x='value:Q',\n    color='key:N',\n    size=alt.value(2)\n)\n\nhist + aggregates\n\n\n\n\n\n\nNow, it’s clear that this Credit Limit is a skewed variable, being skewed higher (or to the right) by a few customers with high Credit Limits.\nBased on its relation to our feet, we know this is right skewed.\nLet’s try Total_Trans_Ct that looked to have a somewhat similar mean and median.\n\nalt.Chart(data).mark_bar().encode(   \n    alt.X('Total_Trans_Ct', bin=alt.Bin(maxbins=30), title=\"Total Transaction Count\"),\n    alt.Y('count()')\n).properties(width=500)\n\n\n\n\n\n\nIt might not be obvious that this is a bimodal distribution, let’s increase the bin size. Default is 10. Bimodal data typically will show two peaks of data– which usually indicates you’ve got two different groups.\n\n\nBoxplot\nLet’s look at Total Transaction Count by Gender.\n\ncolor_scale = alt.Scale(domain=['M', 'F'],\n                        range=['#1f77b4', '#e377c2'])\n                        \nalt.Chart(data).mark_boxplot(size=25).encode(\n    alt.X('Gender', axis=alt.Axis(labelAngle=-0)),\n    alt.Y('Total_Trans_Ct'),\n    alt.Color('Gender', scale=color_scale)\n).properties(width=150)\n\n\n\n\n\n\n\n\nPyramid chart\nWhat if the question we wanted to know is if there was a difference in the distribution of customers by age and gender? A Pyramid chart could get us there!\nTo get the data in the format we need, we will need to aggregate the data up to the Age and Gender level.\n\npyramid_df = (data\n    .groupby(['Gender', 'Customer_Age_bins'])\n    ['CLIENTNUM']\n    .nunique()\n    .reset_index()\n)\n\nleft = alt.Chart(pyramid_df[pyramid_df['Gender']==\"F\"]).mark_bar().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.X('CLIENTNUM', title='Customers', sort='descending'),\n    alt.Color('Gender', scale=color_scale, legend=None)\n).properties(title='Female', width=350)\n\nmiddle = alt.Chart(pyramid_df).mark_text().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.Text('Customer_Age_bins'),\n).properties(width=20)\n\nright = alt.Chart(pyramid_df[pyramid_df['Gender']==\"M\"]).mark_bar().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.X('CLIENTNUM', title='Customers'),\n    alt.Color('Gender', scale=color_scale, legend=None)\n).properties(title='Male', width=300)\n\nleft | middle | right\n\n\n\n\n\n\n\n\n\n📊 Comparing Categories\nWe will look at average Credit_Limit by age group.\n\nbar = alt.Chart(data).mark_bar().encode(\n    alt.X('Customer_Age_bins', axis=alt.Axis(labelAngle=-0), title='Customer age group'),\n    alt.Y('mean(Credit_Limit)'),\n    alt.Color('Customer_Age_bins', legend=None)\n).properties(width=300)\n\nerror_bars = alt.Chart(data).mark_errorbar(extent='ci').encode(  \n    x='Customer_Age_bins',\n    y='Credit_Limit',\n    strokeWidth=alt.value(2)\n)\n\nbar + error_bars\n\n\n\n\n\n\nAnd now we’ll look at Credit_Limit by age group and gender.\n\ngrouped_bar = alt.Chart(data).mark_bar().encode(\n    alt.X('Gender', axis=alt.Axis(title=None, labels=False, ticks=False)),\n    alt.Y('mean(Credit_Limit)', title=\"Mean Credit Limit\", axis=alt.Axis(grid=False)),\n    alt.Color('Gender', scale=color_scale)\n).properties(width=50)\n\nerror_bars = alt.Chart(data).mark_errorbar(extent='ci').encode(  \n    x='Gender',\n    y=alt.Y('Credit_Limit', title='Credit Limit'),\n    strokeWidth=alt.value(2)\n)\n\n(grouped_bar + error_bars).facet(\n    column=alt.Column('Customer_Age_bins', title='Customer age group')\n).configure_headerColumn(\n    titleOrient='bottom', \n    labelOrient='bottom'\n).configure_view(\n    stroke='transparent'\n)\n\n\n\n\n\n\nNotice how Females are getting credit limits much lower than Males!\nMedian Credit_Limit by age group and gender\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('Gender', axis=alt.Axis(title=None, labels=False, ticks=False)),\n    alt.Y('median(Credit_Limit)',title=\"Median Credit Limit\",axis=alt.Axis(grid=False)),\n    color=alt.Color('Gender', scale=color_scale)\n).properties(width=50\n).facet(\n    column=alt.Column('Customer_Age_bins', title='Customer age group')\n).configure_headerColumn(\n    titleOrient='bottom', \n    labelOrient='bottom'\n).configure_view(\n    stroke='transparent'\n)\n\n\n\n\n\n\nLooks like it’s true that men on average are getting accepted for much more than women are for our company.\n\n\n📝 Data table\nComing back to the business problem, we want to understand something about customers who are leaving. Let’s see if we can see anything obvious about their behavior.\n\n(data\n    .groupby(['Attrition_Flag'])\n    .agg({'CLIENTNUM':'nunique',  # number of unique customers in each group\n          'Customer_Age': 'mean',  # the rest are looking at the mean per group\n          'Dependent_count': 'mean',\n          'Months_on_book': 'mean',\n          'Total_Relationship_Count': 'mean',\n          'Months_Inactive_12_mon': 'mean',\n          'Contacts_Count_12_mon': 'mean',\n          'Credit_Limit': 'mean',\n          'Total_Revolving_Bal': 'mean',\n          'Avg_Open_To_Buy': 'mean',\n          'Total_Amt_Chng_Q4_Q1': 'mean',\n          'Total_Trans_Amt': 'mean',\n          'Total_Trans_Ct': 'mean',\n          'Total_Ct_Chng_Q4_Q1': 'mean',\n          'Avg_Utilization_Ratio': 'mean'})\n    .transpose()\n    .assign(Diff = (lambda x: (x['Attrited Customer'] / x['Existing Customer'] - 1)))\n    .sort_values('Diff')\n    .rename_axis(None, axis=1)\n)\n\n\n\n\n\n  \n    \n      \n      Attrited Customer\n      Existing Customer\n      Diff\n    \n  \n  \n    \n      CLIENTNUM\n      1627.00\n      8500.00\n      -0.81\n    \n    \n      Total_Revolving_Bal\n      672.82\n      1256.60\n      -0.46\n    \n    \n      Avg_Utilization_Ratio\n      0.16\n      0.30\n      -0.45\n    \n    \n      Total_Trans_Ct\n      44.93\n      68.67\n      -0.35\n    \n    \n      Total_Trans_Amt\n      3095.03\n      4654.66\n      -0.34\n    \n    \n      Total_Ct_Chng_Q4_Q1\n      0.55\n      0.74\n      -0.25\n    \n    \n      Total_Relationship_Count\n      3.28\n      3.91\n      -0.16\n    \n    \n      Total_Amt_Chng_Q4_Q1\n      0.69\n      0.77\n      -0.10\n    \n    \n      Credit_Limit\n      8136.04\n      8726.88\n      -0.07\n    \n    \n      Avg_Open_To_Buy\n      7463.22\n      7470.27\n      -0.00\n    \n    \n      Months_on_book\n      36.18\n      35.88\n      0.01\n    \n    \n      Customer_Age\n      46.66\n      46.26\n      0.01\n    \n    \n      Dependent_count\n      2.40\n      2.34\n      0.03\n    \n    \n      Months_Inactive_12_mon\n      2.69\n      2.27\n      0.18\n    \n    \n      Contacts_Count_12_mon\n      2.97\n      2.36\n      0.26\n    \n  \n\n\n\n\nWe can see that the Total_Revolving_Bal and Avg_Utilization_Ratio along with Total_Trans_Ct and Total_Trans_Amt show the biggest differences, all showing 30% or more below Existing Customers.\n\n\n📊 Transaction Amounts and Counts relationship\nScatter plot with Transaction Amounts and Counts looks interesting.\n\nalt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n) #.properties(width=500, height=300)\n\n\n\n\n\n\nThis plot clearly shows three distinct groups! Let’s see if anything shows up when we add the Attrition_Flag as a color.\n\nincome_cat_order = ['Less than $40K', \n                    '$40K - $60K', \n                    '$60K - $80K', \n                    '$80K - $120K', \n                    '$120K +', \n                    'Unknown']\n\nalt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n    alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=300, height=200\n).facet(column=alt.Column('Income_Category', sort=income_cat_order))\n\n\n\n\n\n\n\nCreating three different charts, one scatter plot and two histograms.\n\nscatter = alt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n    alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=625, height=400)\n\nhist_amt = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('Total_Trans_Amt', bin=alt.Bin(step=500), title='', \n          scale=alt.Scale(domain=[0, 20000])),\n    alt.Y('count()', stack=False, title=''),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=625, height=150)\n\nhist_ct = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('count()', stack=False, title=''), \n    alt.Y('Total_Trans_Ct', bin=alt.Bin(maxbins=30), title='',\n          scale=alt.Scale(domain=[0, 140])),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=100, height=400)\n\n# putting them all together!\n\nplot_title = alt.TitleParams(\n    \"Transaction Amounts and Counts for Customers\",\n    subtitle = \"No Churned Customers above $11K of Spend\"\n)\n\n(hist_amt & (scatter | hist_ct)).properties(title=plot_title\n).configure_title(fontSize=16, dy=-10\n).configure_legend(\n    offset=-120,\n    strokeColor='gray',\n    padding=10,\n    cornerRadius=10,\n)\n\n\n\n\n\n\nLooks like the top group doesn’t have a single attrited customer! And that number looks to be around $11K.\nFinding #1: No attrited customer above $11K of spend\n\n\n📊 Can we influence the Q4 to Q1 dip?\nThe Q4-Q1 change is significantly different for Churned customers too! This indicates that churned customers are spending significantly less (-25% lower than their counterparts) after the holiday season, pulling back much more sigificantly. This is something we can probably build a recommendation off of. If we can impact this variable, we can also directly impact the Total_Trans_Ct!\n\nalt.Chart(data).transform_density(\n     'Total_Ct_Chng_Q4_Q1',\n     groupby=['Attrition_Flag'],\n     as_=['Total_Ct_Chng_Q4_Q1', 'density']\n).mark_area(opacity=0.7, clip=True).encode(                \n     alt.X('Total_Ct_Chng_Q4_Q1', scale=alt.Scale(domain=[0, 2])),   \n     alt.Y('density:Q', scale=alt.Scale(domain=[0, 3])),  \n     alt.Color('Attrition_Flag', sort='descending')\n).properties(title='Change in Transaction Count (Q4 over Q1)', width=500, height=300)\n\n\n\n\n\n\nRecommendation: Promotions during Q1 time to keep the spending levels high.\nTheory: the more you spend the more attached you feel to the card, it’s benefits, etc. A “prevent the cliff” campaign where all customers who have historically shown a strong drop off, will get targeted with this promo."
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#transaction-amounts-and-counts-for-custemers",
    "href": "posts/credit-card-retention/canalysis.html#transaction-amounts-and-counts-for-custemers",
    "title": "Customer Retention Analysis",
    "section": "Transaction Amounts and Counts for Custemers",
    "text": "Transaction Amounts and Counts for Custemers\n\norder = ['Existing Customer', 'Attrited Customer']\n\nscatter = alt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n    alt.Color('Attrition_Flag', sort=order, title='Attrition Flag')\n).properties(width=625, height=400)\n\nhist_amt = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('Total_Trans_Amt', bin=alt.Bin(step=500), title=''),\n    alt.Y('count()', stack=False, title=''),\n    alt.Color('Attrition_Flag', sort=order)\n).properties(width=625, height=150)\n\nhist_ct = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('count()', stack=False, title=''), \n    alt.Y('Total_Trans_Ct', bin=alt.Bin(maxbins=30), title=''),\n    alt.Color('Attrition_Flag', sort=order)\n).properties(width=100, height=400)\n\n\nplot_title = alt.TitleParams(\n    \"Transaction Amounts and Counts for Customers\",\n    subtitle = \"No Churned Customers above $11K of Spend\"\n)\n\n(hist_amt & (scatter | hist_ct)).properties(title=plot_title\n).configure_title(fontSize=16, dy=-10\n).configure_legend(\n    offset=-120,\n    strokeColor='gray',\n    #fillColor='#EEEEEE',  # #f9f9f9\n    padding=10,\n    cornerRadius=10,\n)\n\n\n\n\n\n\n\nCan we influence the Q4 to Q1 dip?\n\nalt.Chart(data).transform_density(\n     'Total_Ct_Chng_Q4_Q1',\n     groupby=['Attrition_Flag'],\n     as_=['Total_Ct_Chng_Q4_Q1', 'density'],\n     counts=True\n).mark_area(opacity=0.7).encode(                \n     x=alt.X('Total_Ct_Chng_Q4_Q1'),   # scale=alt.Scale(domain=[0, 2])\n     y=alt.Y('density:Q'),\n     color=alt.Color('Attrition_Flag', sort=order)\n).properties(title='Change in Transaction Count (Q4 over Q1)', width=700, height=300)"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#data-overview",
    "href": "posts/credit-card-retention/canalysis.html#data-overview",
    "title": "Bank Customer Retention Analysis",
    "section": "Data Overview",
    "text": "Data Overview\nThe dataset consists of about 10,000 customers mentioning their age, salary, marital status, credit card limit, credit card category, etc. it’s available on Kaggle at the following link.\n\nData Dictionary\n\nCLIENTNUM - Unique identifier for the customer holding the account\nAttrition_Flag - If the account is closed then Attrited Customer, else Existing Customer\nCustomer_Age - Customer’s Age in Years\nGender - Male = M, Female = F\nDependent_count - Number of dependents\nEducation_Level - Educational Qualification of the account holder (example: high school, college graduate, etc.)\nMarital_Status - Married, Single, Divorced, Unknown\nIncome_Category - Annual Income Category of the account holder (Less than $40K, $40K-$60K, $60K-$80K, $80K-$120K, $120K +\nCard_Category - Type of Card (Blue, Silver, Gold, Platinum)\nMonths_on_book - Period of relationship with bank\nTotal_Relationship_count - Total number of products held by the customer\nMonths_Inactive_12_mon - Number of months inactive in the last 12 months\nContacts_Count_12_mon - Number of Contacts in the last 12 months\nCredit_Limit - Credit Limit on the Credit Card\nTotal_Revolving_Bal - Total Revolving Balance on the Credit Card\nAvg_Open_To_Buy- Open to Buy Credit Line (Average of last 12 months)\nTotal_Amt_Chng_Q4_Q1 - Change in Transaction Amount (Q4 over Q1)\nTotal_Trans_Amt - Total Transaction Amount (Last 12 months)\nTotal_Trans_Ct - Total Transaction Count (Last 12 months)\nTotal_Ct_Chng_Q4_Q1 - Change in Transaction Count (Q4 over Q1)\nAvg_Utilization_Ratio - Average Card Utilization Ratio\n\n\n# Read in the data and remove the last two irrelevant columns\ndata = pd.read_csv(\"data/BankChurners.csv\").iloc[: , :-2]\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Attrition_Flag\n      Customer_Age\n      Gender\n      Dependent_count\n      Education_Level\n      Marital_Status\n      Income_Category\n      Card_Category\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n    \n  \n  \n    \n      0\n      768805383\n      Existing Customer\n      45\n      M\n      3\n      High School\n      Married\n      $60K - $80K\n      Blue\n      39\n      5\n      1\n      3\n      12691.00\n      777\n      11914.00\n      1.33\n      1144\n      42\n      1.62\n      0.06\n    \n    \n      1\n      818770008\n      Existing Customer\n      49\n      F\n      5\n      Graduate\n      Single\n      Less than $40K\n      Blue\n      44\n      6\n      1\n      2\n      8256.00\n      864\n      7392.00\n      1.54\n      1291\n      33\n      3.71\n      0.10\n    \n    \n      2\n      713982108\n      Existing Customer\n      51\n      M\n      3\n      Graduate\n      Married\n      $80K - $120K\n      Blue\n      36\n      4\n      1\n      0\n      3418.00\n      0\n      3418.00\n      2.59\n      1887\n      20\n      2.33\n      0.00\n    \n    \n      3\n      769911858\n      Existing Customer\n      40\n      F\n      4\n      High School\n      Unknown\n      Less than $40K\n      Blue\n      34\n      3\n      4\n      1\n      3313.00\n      2517\n      796.00\n      1.41\n      1171\n      20\n      2.33\n      0.76\n    \n    \n      4\n      709106358\n      Existing Customer\n      40\n      M\n      3\n      Uneducated\n      Married\n      $60K - $80K\n      Blue\n      21\n      5\n      1\n      0\n      4716.00\n      0\n      4716.00\n      2.17\n      816\n      28\n      2.50\n      0.00\n    \n  \n\n\n\n\n\ndata.shape\n\n(10127, 21)\n\n\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nData columns (total 21 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   CLIENTNUM                 10127 non-null  int64  \n 1   Attrition_Flag            10127 non-null  object \n 2   Customer_Age              10127 non-null  int64  \n 3   Gender                    10127 non-null  object \n 4   Dependent_count           10127 non-null  int64  \n 5   Education_Level           10127 non-null  object \n 6   Marital_Status            10127 non-null  object \n 7   Income_Category           10127 non-null  object \n 8   Card_Category             10127 non-null  object \n 9   Months_on_book            10127 non-null  int64  \n 10  Total_Relationship_Count  10127 non-null  int64  \n 11  Months_Inactive_12_mon    10127 non-null  int64  \n 12  Contacts_Count_12_mon     10127 non-null  int64  \n 13  Credit_Limit              10127 non-null  float64\n 14  Total_Revolving_Bal       10127 non-null  int64  \n 15  Avg_Open_To_Buy           10127 non-null  float64\n 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float64\n 17  Total_Trans_Amt           10127 non-null  int64  \n 18  Total_Trans_Ct            10127 non-null  int64  \n 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float64\n 20  Avg_Utilization_Ratio     10127 non-null  float64\ndtypes: float64(5), int64(10), object(6)\nmemory usage: 1.6+ MB"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#key-findings",
    "href": "posts/credit-card-retention/canalysis.html#key-findings",
    "title": "Bank Customer Retention Analysis",
    "section": "Key Findings",
    "text": "Key Findings"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#data-preprocessing",
    "href": "posts/credit-card-retention/canalysis.html#data-preprocessing",
    "title": "Bank Customer Retention Analysis",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nBinning Customer Age\n\nprint(data['Customer_Age'].min())\nprint(data['Customer_Age'].max())\n\n26\n73\n\n\n\nbins = [25, 30, 40, 50, 60, 70, 80]\nlabels = ['20s', '30s', '40s', '50s', '60s', '70s']\n\ndata['Customer_Age_bins'] = pd.cut(\n    data['Customer_Age'], \n    bins=bins, \n    labels=labels, \n    include_lowest=True, \n    right=False\n)\n\nLet’s verify that the behavior we expected from this method resulted in the right output. Since we overrode the inclusive values, we should see that 30 should show in the 30s bin vs the 20s bin.\n\ndata[data['Customer_Age'] == 30].head(3)\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Attrition_Flag\n      Customer_Age\n      Gender\n      Dependent_count\n      Education_Level\n      Marital_Status\n      Income_Category\n      Card_Category\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n      Customer_Age_bins\n    \n  \n  \n    \n      789\n      754654383\n      Existing Customer\n      30\n      M\n      2\n      High School\n      Married\n      Less than $40K\n      Blue\n      23\n      4\n      2\n      0\n      2997.00\n      1393\n      1604.00\n      1.12\n      1577\n      35\n      0.52\n      0.47\n      30s\n    \n    \n      803\n      711748983\n      Existing Customer\n      30\n      M\n      1\n      High School\n      Married\n      $60K - $80K\n      Blue\n      17\n      3\n      3\n      2\n      7906.00\n      1631\n      6275.00\n      0.89\n      1946\n      45\n      0.55\n      0.21\n      30s\n    \n    \n      890\n      779149683\n      Existing Customer\n      30\n      F\n      0\n      Graduate\n      Divorced\n      Unknown\n      Blue\n      13\n      5\n      1\n      2\n      7772.00\n      1890\n      5882.00\n      0.75\n      2585\n      75\n      0.83\n      0.24\n      30s\n    \n  \n\n\n\n\n\n\nConverting data types for memory optimization\n\ndata.info(verbose=False, show_counts=False, memory_usage='deep') \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nColumns: 22 entries, CLIENTNUM to Customer_Age_bins\ndtypes: category(1), float64(5), int64(10), object(6)\nmemory usage: 4.9 MB\n\n\n\ndata = data.astype(\n    {   \n        'Attrition_Flag': 'category',    # categories\n        'Gender': 'category',           \n        'Education_Level': 'category', \n        'Marital_Status': 'category', \n        'Income_Category': 'category', \n        'Card_Category': 'category',\n        'Customer_Age': 'int8',          # integers\n        'Dependent_count': 'int8',\n        'Months_on_book': 'int8',\n        'Total_Relationship_Count':'int8',\n        'Months_Inactive_12_mon':'int8',\n        'Contacts_Count_12_mon':'int8',\n        'Total_Revolving_Bal':'int16',\n        'Total_Trans_Amt':'int16',\n        'Total_Trans_Ct':'int16',              \n        'Credit_Limit': 'float32',       # floats\n        'Avg_Open_To_Buy': 'float32', \n        'Total_Amt_Chng_Q4_Q1': 'float16', \n        'Total_Ct_Chng_Q4_Q1': 'float16', \n        'Avg_Utilization_Ratio': 'float16'\n    }\n)\n\n\ndata.info(memory_usage='deep') \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nData columns (total 22 columns):\n #   Column                    Non-Null Count  Dtype   \n---  ------                    --------------  -----   \n 0   CLIENTNUM                 10127 non-null  int64   \n 1   Attrition_Flag            10127 non-null  category\n 2   Customer_Age              10127 non-null  int8    \n 3   Gender                    10127 non-null  category\n 4   Dependent_count           10127 non-null  int8    \n 5   Education_Level           10127 non-null  category\n 6   Marital_Status            10127 non-null  category\n 7   Income_Category           10127 non-null  category\n 8   Card_Category             10127 non-null  category\n 9   Months_on_book            10127 non-null  int8    \n 10  Total_Relationship_Count  10127 non-null  int8    \n 11  Months_Inactive_12_mon    10127 non-null  int8    \n 12  Contacts_Count_12_mon     10127 non-null  int8    \n 13  Credit_Limit              10127 non-null  float32 \n 14  Total_Revolving_Bal       10127 non-null  int16   \n 15  Avg_Open_To_Buy           10127 non-null  float32 \n 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float16 \n 17  Total_Trans_Amt           10127 non-null  int16   \n 18  Total_Trans_Ct            10127 non-null  int16   \n 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float16 \n 20  Avg_Utilization_Ratio     10127 non-null  float16 \n 21  Customer_Age_bins         10127 non-null  category\ndtypes: category(7), float16(3), float32(2), int16(3), int64(1), int8(6)\nmemory usage: 408.7 KB"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#relationships",
    "href": "posts/credit-card-retention/canalysis.html#relationships",
    "title": "Bank Customer Retention Analysis",
    "section": "Relationships",
    "text": "Relationships\n\nalt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n    # alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=500, height=300)\n\n\n\n\n\n\n\n📊 Transaction Amounts and Counts for Custemers\nCreating three different charts, one scatter plot and two histograms.\n\nscatter = alt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n    alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=625, height=400)\n\nhist_amt = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('Total_Trans_Amt', bin=alt.Bin(step=500), title='', \n          scale=alt.Scale(domain=[0, 20000])),\n    alt.Y('count()', stack=False, title=''),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=625, height=150)\n\nhist_ct = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('count()', stack=False, title=''), \n    alt.Y('Total_Trans_Ct', bin=alt.Bin(maxbins=30), title='',\n          scale=alt.Scale(domain=[0, 140])),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=100, height=400)\n\nPutting them together!\n\nplot_title = alt.TitleParams(\n    \"Transaction Amounts and Counts for Customers\",\n    subtitle = \"No Churned Customers above $11K of Spend\"\n)\n\n(hist_amt & (scatter | hist_ct)).properties(title=plot_title\n).configure_title(fontSize=16, dy=-10\n).configure_legend(\n    offset=-120,\n    strokeColor='gray',\n    padding=10,\n    cornerRadius=10,\n)\n\n\n\n\n\n\n\n\n📊 Can we influence the Q4 to Q1 dip?\n\nalt.Chart(data).transform_density(\n     'Total_Ct_Chng_Q4_Q1',\n     groupby=['Attrition_Flag'],\n     as_=['Total_Ct_Chng_Q4_Q1', 'density']\n).mark_area(opacity=0.7).encode(                \n     alt.X('Total_Ct_Chng_Q4_Q1'),   \n     alt.Y('density:Q', scale=alt.Scale(domain=[0, 3])),  \n     alt.Color('Attrition_Flag', sort='descending')\n).properties(title='Change in Transaction Count (Q4 over Q1)', width=700, height=300)"
  },
  {
    "objectID": "posts/credit-card-retention/canalysis.html#recommendations",
    "href": "posts/credit-card-retention/canalysis.html#recommendations",
    "title": "Bank Customer Retention Analysis",
    "section": "Recommendations",
    "text": "Recommendations\nKey Findings"
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html",
    "href": "posts/credit-card-retention/analysis.html",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "",
    "text": "In this hypothetical case study I’ve been hired as a financial analyst by the marketing department within a bank.\nThey are faced with more and more customers leaving their credit card services. They would like to understand what characteristics lend themselves to someone who is going to churn so they can proactively go to the customer to provide them better services and turn customers’ decisions in the opposite direction.\nAs a part of my first analysis, they’ve asked me to take a look at the available data and help them understand how to increase customer retention."
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#problem-statement",
    "href": "posts/credit-card-retention/analysis.html#problem-statement",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nWhat marketing campaigns could we implement to help reduce customer churn?"
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#imports",
    "href": "posts/credit-card-retention/analysis.html#imports",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport numpy as np\nimport altair as alt\n\n# Show all columns (instead of cascading columns in the middle)\npd.set_option(\"display.max_columns\", None)\n# Don't show numbers in scientific notation\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\n\n# Enable plotting more than 5000 points\nalt.data_transformers.disable_max_rows()\n\nDataTransformerRegistry.enable('default')"
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#data-overview",
    "href": "posts/credit-card-retention/analysis.html#data-overview",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Data Overview",
    "text": "Data Overview\nThe dataset consists of about 10,000 customers described with their age, income level, marital status, credit card limit, credit card category, etc. it’s available on Kaggle at the following link.\n\nData Dictionary\n\nCLIENTNUM - Unique identifier for the customer holding the account\nAttrition_Flag - If the account is closed then Attrited Customer, else Existing Customer\nCustomer_Age - Customer’s age in years\nGender - Male = M, Female = F\nDependent_count - Number of dependents\nEducation_Level - Educational qualification of the account holder (example: high school, college graduate, etc.)\nMarital_Status - Married, Single, Divorced, Unknown\nIncome_Category - Annual income category of the account holder (Less than $40K, $40K-$60K, $60K-$80K, $80K-$120K, $120K +\nCard_Category - Type of card (Blue, Silver, Gold, Platinum)\nMonths_on_book - Period of relationship with bank\nTotal_Relationship_count - Total number of products held by the customer\nMonths_Inactive_12_mon - Number of months inactive in the last 12 months\nContacts_Count_12_mon - Number of contacts in the last 12 months\nCredit_Limit - Credit limit on the credit card\nTotal_Revolving_Bal - Total revolving balance on the credit card\nAvg_Open_To_Buy- Open to buy credit line (average of last 12 months)\nTotal_Amt_Chng_Q4_Q1 - Change in transaction amount (Q4 over Q1)\nTotal_Trans_Amt - Total transaction amount (last 12 months)\nTotal_Trans_Ct - Total transaction count (last 12 months)\nTotal_Ct_Chng_Q4_Q1 - Change in transaction count (Q4 over Q1)\nAvg_Utilization_Ratio - Average card utilization ratio\n\n\n# Read in the data and remove the last two columns irrelevant for the analysis\ndata = pd.read_csv(\"data/BankChurners.csv\").iloc[: , :-2]\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Attrition_Flag\n      Customer_Age\n      Gender\n      Dependent_count\n      Education_Level\n      Marital_Status\n      Income_Category\n      Card_Category\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n    \n  \n  \n    \n      0\n      768805383\n      Existing Customer\n      45\n      M\n      3\n      High School\n      Married\n      $60K - $80K\n      Blue\n      39\n      5\n      1\n      3\n      12691.00\n      777\n      11914.00\n      1.33\n      1144\n      42\n      1.62\n      0.06\n    \n    \n      1\n      818770008\n      Existing Customer\n      49\n      F\n      5\n      Graduate\n      Single\n      Less than $40K\n      Blue\n      44\n      6\n      1\n      2\n      8256.00\n      864\n      7392.00\n      1.54\n      1291\n      33\n      3.71\n      0.10\n    \n    \n      2\n      713982108\n      Existing Customer\n      51\n      M\n      3\n      Graduate\n      Married\n      $80K - $120K\n      Blue\n      36\n      4\n      1\n      0\n      3418.00\n      0\n      3418.00\n      2.59\n      1887\n      20\n      2.33\n      0.00\n    \n    \n      3\n      769911858\n      Existing Customer\n      40\n      F\n      4\n      High School\n      Unknown\n      Less than $40K\n      Blue\n      34\n      3\n      4\n      1\n      3313.00\n      2517\n      796.00\n      1.41\n      1171\n      20\n      2.33\n      0.76\n    \n    \n      4\n      709106358\n      Existing Customer\n      40\n      M\n      3\n      Uneducated\n      Married\n      $60K - $80K\n      Blue\n      21\n      5\n      1\n      0\n      4716.00\n      0\n      4716.00\n      2.17\n      816\n      28\n      2.50\n      0.00\n    \n  \n\n\n\n\n\ndata.shape\n\n(10127, 21)\n\n\nWe have a total of 10127 customers described with 21 attributes.\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nData columns (total 21 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   CLIENTNUM                 10127 non-null  int64  \n 1   Attrition_Flag            10127 non-null  object \n 2   Customer_Age              10127 non-null  int64  \n 3   Gender                    10127 non-null  object \n 4   Dependent_count           10127 non-null  int64  \n 5   Education_Level           10127 non-null  object \n 6   Marital_Status            10127 non-null  object \n 7   Income_Category           10127 non-null  object \n 8   Card_Category             10127 non-null  object \n 9   Months_on_book            10127 non-null  int64  \n 10  Total_Relationship_Count  10127 non-null  int64  \n 11  Months_Inactive_12_mon    10127 non-null  int64  \n 12  Contacts_Count_12_mon     10127 non-null  int64  \n 13  Credit_Limit              10127 non-null  float64\n 14  Total_Revolving_Bal       10127 non-null  int64  \n 15  Avg_Open_To_Buy           10127 non-null  float64\n 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float64\n 17  Total_Trans_Amt           10127 non-null  int64  \n 18  Total_Trans_Ct            10127 non-null  int64  \n 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float64\n 20  Avg_Utilization_Ratio     10127 non-null  float64\ndtypes: float64(5), int64(10), object(6)\nmemory usage: 1.6+ MB\n\n\nThis dataset is actually very clean. We’ll only do some preprocessing steps before the analysis."
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#data-preprocessing",
    "href": "posts/credit-card-retention/analysis.html#data-preprocessing",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nBinning Customer Age\nWe’ll create specific age categories and place ages within a range in these categories. For instance, category 20s will containe ages between 20 and 29, and so on.\n\n# looking for min and max age\nprint(data['Customer_Age'].min())\nprint(data['Customer_Age'].max())\n\n26\n73\n\n\n\nbins = [25, 30, 40, 50, 60, 70, 80]\nlabels = ['20s', '30s', '40s', '50s', '60s', '70s']\n\ndata['Customer_Age_bins'] = pd.cut(\n    data['Customer_Age'], \n    bins=bins, \n    labels=labels, \n    include_lowest=True, \n    right=False\n)\n\nLet’s verify that the behavior we expected from this method resulted in the right output. Since we overrode the inclusive values, we should see that 30 should show in the 30s bin vs the 20s bin.\n\ndata[data['Customer_Age'] == 30].head(3)\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Attrition_Flag\n      Customer_Age\n      Gender\n      Dependent_count\n      Education_Level\n      Marital_Status\n      Income_Category\n      Card_Category\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n      Customer_Age_bins\n    \n  \n  \n    \n      789\n      754654383\n      Existing Customer\n      30\n      M\n      2\n      High School\n      Married\n      Less than $40K\n      Blue\n      23\n      4\n      2\n      0\n      2997.00\n      1393\n      1604.00\n      1.12\n      1577\n      35\n      0.52\n      0.47\n      30s\n    \n    \n      803\n      711748983\n      Existing Customer\n      30\n      M\n      1\n      High School\n      Married\n      $60K - $80K\n      Blue\n      17\n      3\n      3\n      2\n      7906.00\n      1631\n      6275.00\n      0.89\n      1946\n      45\n      0.55\n      0.21\n      30s\n    \n    \n      890\n      779149683\n      Existing Customer\n      30\n      F\n      0\n      Graduate\n      Divorced\n      Unknown\n      Blue\n      13\n      5\n      1\n      2\n      7772.00\n      1890\n      5882.00\n      0.75\n      2585\n      75\n      0.83\n      0.24\n      30s\n    \n  \n\n\n\n\n\n\nConverting data types for memory optimization\n\ndata.info(verbose=False, show_counts=False, memory_usage='deep') \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nColumns: 22 entries, CLIENTNUM to Customer_Age_bins\ndtypes: category(1), float64(5), int64(10), object(6)\nmemory usage: 4.9 MB\n\n\nCurrently, the total deep memory usage of the DataFrame is 4.9 MB. We’ll reduce it with smaller datatypes.\n\ndata = data.astype(\n    {   \n        'Attrition_Flag': 'category',    # categories\n        'Gender': 'category',           \n        'Education_Level': 'category', \n        'Marital_Status': 'category', \n        'Income_Category': 'category', \n        'Card_Category': 'category',\n        'Customer_Age': 'int8',          # integers\n        'Dependent_count': 'int8',\n        'Months_on_book': 'int8',\n        'Total_Relationship_Count':'int8',\n        'Months_Inactive_12_mon':'int8',\n        'Contacts_Count_12_mon':'int8',\n        'Total_Revolving_Bal':'int16',\n        'Total_Trans_Amt':'int16',\n        'Total_Trans_Ct':'int16',              \n        'Credit_Limit': 'float32',       # floats\n        'Avg_Open_To_Buy': 'float32', \n        'Total_Amt_Chng_Q4_Q1': 'float16', \n        'Total_Ct_Chng_Q4_Q1': 'float16', \n        'Avg_Utilization_Ratio': 'float16'\n    }\n)\n\n\ndata.info(memory_usage='deep') \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nData columns (total 22 columns):\n #   Column                    Non-Null Count  Dtype   \n---  ------                    --------------  -----   \n 0   CLIENTNUM                 10127 non-null  int64   \n 1   Attrition_Flag            10127 non-null  category\n 2   Customer_Age              10127 non-null  int8    \n 3   Gender                    10127 non-null  category\n 4   Dependent_count           10127 non-null  int8    \n 5   Education_Level           10127 non-null  category\n 6   Marital_Status            10127 non-null  category\n 7   Income_Category           10127 non-null  category\n 8   Card_Category             10127 non-null  category\n 9   Months_on_book            10127 non-null  int8    \n 10  Total_Relationship_Count  10127 non-null  int8    \n 11  Months_Inactive_12_mon    10127 non-null  int8    \n 12  Contacts_Count_12_mon     10127 non-null  int8    \n 13  Credit_Limit              10127 non-null  float32 \n 14  Total_Revolving_Bal       10127 non-null  int16   \n 15  Avg_Open_To_Buy           10127 non-null  float32 \n 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float16 \n 17  Total_Trans_Amt           10127 non-null  int16   \n 18  Total_Trans_Ct            10127 non-null  int16   \n 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float16 \n 20  Avg_Utilization_Ratio     10127 non-null  float16 \n 21  Customer_Age_bins         10127 non-null  category\ndtypes: category(7), float16(3), float32(2), int16(3), int64(1), int8(6)\nmemory usage: 408.7 KB\n\n\nWe can see at the bottom that the memory usage is now only 408.7 KB (10 times lower)."
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#exploratory-data-analysis-eda",
    "href": "posts/credit-card-retention/analysis.html#exploratory-data-analysis-eda",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n🔍 Percentage of churned customers\n\ndata['Attrition_Flag'].value_counts()   \n\nExisting Customer    8500\nAttrited Customer    1627\nName: Attrition_Flag, dtype: int64\n\n\nWe’ll use normalize=True to get the share of churned and existing customers.\n\nchurned = data['Attrition_Flag'].value_counts(normalize=True)['Attrited Customer']\n\nprint(round(churned * 100 , 2), '% of customers have churned.')\n\n16.07 % of customers have churned.\n\n\n\n\n📝 Summary Statistics\n\ndata.describe()\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Customer_Age\n      Dependent_count\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n    \n  \n  \n    \n      count\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n    \n    \n      mean\n      739177606.33\n      46.33\n      2.35\n      35.93\n      3.81\n      2.34\n      2.46\n      8632.00\n      1162.81\n      7469.17\n      0.76\n      4404.09\n      64.86\n      0.71\n      0.27\n    \n    \n      std\n      36903783.45\n      8.02\n      1.30\n      7.99\n      1.55\n      1.01\n      1.11\n      9088.76\n      814.99\n      9090.68\n      0.22\n      3397.13\n      23.47\n      0.24\n      0.28\n    \n    \n      min\n      708082083.00\n      26.00\n      0.00\n      13.00\n      1.00\n      0.00\n      0.00\n      1438.30\n      0.00\n      3.00\n      0.00\n      510.00\n      10.00\n      0.00\n      0.00\n    \n    \n      25%\n      713036770.50\n      41.00\n      1.00\n      31.00\n      3.00\n      2.00\n      2.00\n      2555.00\n      359.00\n      1324.50\n      0.63\n      2155.50\n      45.00\n      0.58\n      0.02\n    \n    \n      50%\n      717926358.00\n      46.00\n      2.00\n      36.00\n      4.00\n      2.00\n      2.00\n      4549.00\n      1276.00\n      3474.00\n      0.74\n      3899.00\n      67.00\n      0.70\n      0.18\n    \n    \n      75%\n      773143533.00\n      52.00\n      3.00\n      40.00\n      5.00\n      3.00\n      3.00\n      11067.50\n      1784.00\n      9859.00\n      0.86\n      4741.00\n      81.00\n      0.82\n      0.50\n    \n    \n      max\n      828343083.00\n      73.00\n      5.00\n      56.00\n      6.00\n      6.00\n      6.00\n      34516.00\n      2517.00\n      34516.00\n      3.40\n      18484.00\n      139.00\n      3.71\n      1.00\n    \n  \n\n\n\n\n\nHere we can see:\n\nThe longest customer in this dataset has been around for 56 months or about 4 years and a half. (Max)\nThe average number of relationships a customer has is ~4. (Mean and median agree here)\nThe average credit limit is $8.6K, but the median credit limit is much lower at $4.5K. (signals some skew in this variable)\n\n\n\n📊 Distributions\n\nHistograms\nLet’s take a look at the distribution for the Months_on_book variable.\n\nalt.Chart(data).mark_bar().encode(   \n    alt.X('Months_on_book', bin=alt.Bin(maxbins=30), title=\"Months on book\"),\n    alt.Y('count()')\n).properties(width=500)\n\n\n\n\n\n\nIt looks pretty normal outside of a very strong peak at ~36 months (roughly 2800 customers). If we had access to the owner to ask more questions about this, we would want to know if this was a data error (default months for those without a record), or if there was a major marketing campaign that brought in a lot of users 36 months ago.\nNow let’s take a look at the Credit_Limit variable that we looked at before.\n\nhist = alt.Chart(data).mark_bar().encode(   \n    alt.X('Credit_Limit', bin=alt.Bin(step=1500), title=\"Credit Limit\"),\n    alt.Y('count()')\n).properties(width=500)\n\naggregates = alt.Chart(data).transform_aggregate(\n    mean='mean(Credit_Limit)',\n    median='median(Credit_Limit)',\n).transform_fold(\n    ['mean', 'median']\n).mark_rule().encode(\n    x='value:Q',\n    color='key:N',\n    size=alt.value(2)\n)\n\nhist + aggregates\n\n\n\n\n\n\nIt’s clear that the Credit_Limit is a skewed variable, being skewed higher (or to the right) by a few customers with high Credit Limits.\nLet’s try Total_Trans_Ct that looked to have a somewhat similar mean and median.\n\nalt.Chart(data).mark_bar().encode(   \n    alt.X('Total_Trans_Ct', bin=alt.Bin(maxbins=30), title=\"Total Transaction Count\"),\n    alt.Y('count()')\n).properties(width=500)\n\n\n\n\n\n\nThis is a bimodal distribution, which usually indicates two different groups.\n\n\nBoxplot\nLet’s look at the total number of transactions by gender.\n\ncolor_scale = alt.Scale(domain=['M', 'F'],\n                        range=['#1f77b4', '#e377c2'])\n                        \nalt.Chart(data).mark_boxplot(size=25).encode(\n    alt.X('Gender', axis=alt.Axis(labelAngle=-0)),\n    alt.Y('Total_Trans_Ct'),\n    alt.Color('Gender', scale=color_scale)\n).properties(width=150)\n\n\n\n\n\n\nThis plot quickly shows us that the median transaction count is slightly higher for females than males and outliers seem to be present among both groups.\n\n\nPyramid chart\nWas there a difference in the distribution of customers by age and gender? A pyramid chart can help us find the answer.\nTo get the data in the format we need, we will need to aggregate the data up to the age and gender level.\n\npyramid_df = (data\n    .groupby(['Gender', 'Customer_Age_bins'])\n    ['CLIENTNUM']\n    .nunique()\n    .reset_index()\n)\n\nleft = alt.Chart(pyramid_df[pyramid_df['Gender']==\"F\"]).mark_bar().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.X('CLIENTNUM', title='Customers', sort='descending'),\n    alt.Color('Gender', scale=color_scale, legend=None)\n).properties(title='Female', width=350)\n\nmiddle = alt.Chart(pyramid_df).mark_text().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.Text('Customer_Age_bins'),\n).properties(width=20)\n\nright = alt.Chart(pyramid_df[pyramid_df['Gender']==\"M\"]).mark_bar().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.X('CLIENTNUM', title='Customers'),\n    alt.Color('Gender', scale=color_scale, legend=None)\n).properties(title='Male', width=300)\n\nleft | middle | right\n\n\n\n\n\n\nHere we see there is no real difference in the distribution of customers by age and gender. The majority of customers are in their 40s. We have only two male customers in their 70s which is hardly visible here.\n\n\n\n📊 Categorical variables\nNext we’ll see how attrited and existing customers are represented in all category groups.\n\n# get the list of vategorical variables, remove `Attrition_flag`\ncat_list = sorted((list(set(data.columns) - set(data._get_numeric_data().columns))))[1:]\ncat_list\n\n['Card_Category',\n 'Customer_Age_bins',\n 'Education_Level',\n 'Gender',\n 'Income_Category',\n 'Marital_Status']\n\n\n\ndef cat_column_bar(cat_column):\n\n    return alt.Chart(data).mark_bar().encode(\n        alt.X('count()', stack=\"normalize\", axis=alt.Axis(format='%'), title='Percent'),\n        alt.Y(cat_column),\n        color=alt.Color('Attrition_Flag', sort=\"descending\")\n    ).properties(width=250)\n    \n(cat_column_bar(cat_list[0]) | \n cat_column_bar(cat_list[1]) | \n cat_column_bar(cat_list[2]) | \n cat_column_bar(cat_list[3]) | \n cat_column_bar(cat_list[4]) | \n cat_column_bar(cat_list[5]) \n)\n\n\n\n\n\n\n\nNothing alarming here. Churned customers are almost equally distributed along all category groups.\nIndividual values for customers with a platinum credit card and the age group 70s deviate from the average due to the small number of customers in these groups.\n\n\n📊 Comparing Categories\nWe will look at the average Credit_Limit by age group.\n\nbar = alt.Chart(data).mark_bar().encode(\n    alt.X('Customer_Age_bins', axis=alt.Axis(labelAngle=-0), title='Customer age group'),\n    alt.Y('mean(Credit_Limit)'),\n    alt.Color('Customer_Age_bins', legend=None)\n).properties(width=300)\n\nerror_bars = alt.Chart(data).mark_errorbar(extent='ci').encode(  \n    x='Customer_Age_bins',\n    y='Credit_Limit',\n    strokeWidth=alt.value(2)\n)\n\nbar + error_bars\n\n\n\n\n\n\nAnd now we’ll look at the Credit_Limit by age group and gender.\n\ngrouped_bar = alt.Chart(data).mark_bar().encode(\n    alt.X('Gender', axis=alt.Axis(title=None, labels=False, ticks=False)),\n    alt.Y('mean(Credit_Limit)', title=\"Mean Credit Limit\", axis=alt.Axis(grid=False)),\n    alt.Color('Gender', scale=color_scale)\n).properties(width=50)\n\nerror_bars = alt.Chart(data).mark_errorbar(extent='ci').encode(  \n    x='Gender',\n    y=alt.Y('Credit_Limit', title='Credit Limit'),\n    strokeWidth=alt.value(2)\n)\n\n(grouped_bar + error_bars).facet(\n    column=alt.Column('Customer_Age_bins', title='Customer age group')\n).configure_headerColumn(\n    titleOrient='bottom', \n    labelOrient='bottom'\n).configure_view(\n    stroke='transparent'\n)\n\n\n\n\n\n\nFemales on average are getting credit limits much lower than males! They probably have lower wages. Let’s check this out.\n\nincome_cat_order = ['Less than $40K', '$40K - $60K', '$60K - $80K', \n                    '$80K - $120K', '$120K +', 'Unknown']\n                    \nalt.Chart(data).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('Income_Category', sort=income_cat_order, title='Income Category'),\n    alt.Color('Gender', scale=color_scale)\n)\n\n\n\n\n\n\nThis is interesting. There is not a single woman in this dataset who earns $60K-$80K, $80K-$120K, or $120K + a year.\nDoes this follow their education level?\n\nedu_cat_order = ['Uneducated', 'High School', 'College', 'Graduate', \n                 'Post-Graduate', 'Doctorate', 'Unknown']\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('Education_Level', sort=edu_cat_order, title='Education Level'),\n    alt.Color('Gender', scale=color_scale)\n)\n\n\n\n\n\n\nCertainly not. Men and women are equally educated. But compared to women in general, men are clearly more represented in better-paid jobs and in better-paid positions.\n\n\n📝 Pivot table\nComing back to the business problem, we want to understand something about customers who are leaving. Let’s see if we can see anything obvious about their behavior.\n\n(data\n    .groupby(['Attrition_Flag'])\n    .agg({'CLIENTNUM':'nunique',  # number of unique customers in each group\n          'Customer_Age': 'mean',  # the rest are looking at the mean per group\n          'Dependent_count': 'mean',\n          'Months_on_book': 'mean',\n          'Total_Relationship_Count': 'mean',\n          'Months_Inactive_12_mon': 'mean',\n          'Contacts_Count_12_mon': 'mean',\n          'Credit_Limit': 'mean',\n          'Total_Revolving_Bal': 'mean',\n          'Avg_Open_To_Buy': 'mean',\n          'Total_Amt_Chng_Q4_Q1': 'mean',\n          'Total_Trans_Amt': 'mean',\n          'Total_Trans_Ct': 'mean',\n          'Total_Ct_Chng_Q4_Q1': 'mean',\n          'Avg_Utilization_Ratio': 'mean'})\n    .transpose()\n    .assign(Diff = (lambda x: (x['Attrited Customer'] / x['Existing Customer'] - 1)))\n    .sort_values('Diff')\n    .rename_axis(None, axis=1)\n)\n\n\n\n\n\n  \n    \n      \n      Attrited Customer\n      Existing Customer\n      Diff\n    \n  \n  \n    \n      CLIENTNUM\n      1627.00\n      8500.00\n      -0.81\n    \n    \n      Total_Revolving_Bal\n      672.82\n      1256.60\n      -0.46\n    \n    \n      Avg_Utilization_Ratio\n      0.16\n      0.30\n      -0.45\n    \n    \n      Total_Trans_Ct\n      44.93\n      68.67\n      -0.35\n    \n    \n      Total_Trans_Amt\n      3095.03\n      4654.66\n      -0.34\n    \n    \n      Total_Ct_Chng_Q4_Q1\n      0.55\n      0.74\n      -0.25\n    \n    \n      Total_Relationship_Count\n      3.28\n      3.91\n      -0.16\n    \n    \n      Total_Amt_Chng_Q4_Q1\n      0.69\n      0.77\n      -0.10\n    \n    \n      Credit_Limit\n      8136.04\n      8726.88\n      -0.07\n    \n    \n      Avg_Open_To_Buy\n      7463.22\n      7470.27\n      -0.00\n    \n    \n      Months_on_book\n      36.18\n      35.88\n      0.01\n    \n    \n      Customer_Age\n      46.66\n      46.26\n      0.01\n    \n    \n      Dependent_count\n      2.40\n      2.34\n      0.03\n    \n    \n      Months_Inactive_12_mon\n      2.69\n      2.27\n      0.18\n    \n    \n      Contacts_Count_12_mon\n      2.97\n      2.36\n      0.26\n    \n  \n\n\n\n\nWe can see that the Total_Revolving_Bal and Avg_Utilization_Ratio show the biggest differences (45% below existing customers) along with Total_Trans_Ct and Total_Trans_Amt (35% below existing customers), and Total_Ct_Chng_Q4_Q1 (25% below existing customers).\nAverage Transaction Value\nAverage Transaction Value is the average amount a customer spends on a single purchase.\nWe can calculate it by dividing the Total_Trans_Amt by the Total_Trans_Ct.\nFor churned customers it equels to $68.9, and for existing customers $67.8. They are almost equal in value.\nAverage Card Utilization Ratio\nIt represents the amount of revolving credit customer is using divided by the total credit available to them.\nLet’s look at the Avg_Utilization_Ratio distribution.\n\nalt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('Avg_Utilization_Ratio', bin=alt.Bin(step=0.025), \n          axis=alt.Axis(format='%'), title=\"Average Utilization Ratio\"),\n    alt.Y('count()', stack=False),\n    alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=700)\n\n\n\n\n\n\nWe can see that 57% (934) of churned customers have the average utilization ratio less than 2.5%. Actually, 55% (902) of churned customers have that ratio less than 1%. They just don’t use their credit card much which could be one of the main reasons for credit card cancellation!\n\n\n📊 Transaction Amounts and Counts relationship\nScatter plot with Transaction Amounts and Counts looks interesting.\n\nalt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n) #.properties(width=500, height=300)\n\n\n\n\n\n\nThis plot clearly shows three distinct groups! Let’s see if anything shows up when we add the Attrition_Flag as a color.\n\nscatter = alt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n    alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=625, height=400)\n\nhist_amt = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('Total_Trans_Amt', bin=alt.Bin(step=500), title='', \n          scale=alt.Scale(domain=[0, 20000])),\n    alt.Y('count()', stack=False, title=''),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=625, height=150)\n\nhist_ct = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('count()', stack=False, title=''), \n    alt.Y('Total_Trans_Ct', bin=alt.Bin(maxbins=30), title='',\n          scale=alt.Scale(domain=[0, 140])),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=100, height=400)\n\n# putting them all together!\n\nplot_title = alt.TitleParams(\n    \"Transaction Amounts and Counts for Customers\",\n    subtitle = \"No Churned Customers above $11K of Spend\"\n)\n\n(hist_amt & (scatter | hist_ct)).properties(title=plot_title\n).configure_title(fontSize=16, dy=-10\n).configure_legend(\n    offset=-120,\n    strokeColor='gray',\n    padding=10,\n    cornerRadius=10,\n)\n\n\n\n\n\n\nWe can see that there is no attrited customer above $11K of spend.\nThe question is: “How can we get more customers above the $11K threshold?”\n\n\n📊 Can we influence the Q4 to Q1 dip?\nThe Q4-Q1 change (Total_Ct_Chng_Q4_Q1) is significantly different for churned customers too! This indicates that churned customers are spending significantly less (-25% lower than their counterparts) after the holiday season, pulling back much more sigificantly. If we can impact this variable, we can also directly impact the Total Transaction Count!\n\nalt.Chart(data).transform_density(\n     'Total_Ct_Chng_Q4_Q1',\n     groupby=['Attrition_Flag'],\n     as_=['Total_Ct_Chng_Q4_Q1', 'density']\n).mark_area(opacity=0.7, clip=True).encode(                \n     alt.X('Total_Ct_Chng_Q4_Q1', scale=alt.Scale(domain=[0, 2]),\n           title='Total Transaction Count Change (Q4 to Q1)'),   \n     alt.Y('density:Q', scale=alt.Scale(domain=[0, 3])),  \n     alt.Color('Attrition_Flag', sort='descending')\n).properties(title='Change in Transaction Count (Q4 over Q1)', width=500, height=300\n).configure_title(fontSize=16, dy=-10, anchor='start')"
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#recommendations",
    "href": "posts/credit-card-retention/analysis.html#recommendations",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Recommendations",
    "text": "Recommendations\n\nPromotions during Q1 time to keep the spending levels high. The more we spend the more attached we feel to the card, it’s benefits, etc. A “prevent the cliff” campaign where all customers who have historically shown a strong drop off, will get targeted with this promo.\nCustomer surveys. For those who are spending above $11,000 to understand why they love our card and what keeps them around. If we were also able to get responses from some of our churn customers, we can develop stronger marketing campaigns around our findings.\nOffer loyalty points, cash back, etc.\nLook at any historical marketing campaigns to see what we can learn from what worked / didn’t work."
  }
]