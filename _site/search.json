[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Let’s be friends",
    "section": "",
    "text": "Hi and welcome to my personal blog. My name is Sandra Jurela and here I will be sharing my journey into Data Science. Recently I have developed a real passion for Data Science and this blog truly is a manifestation of that.\nI hold a Master’s degree in Civil Engineering (Hydrology and Water Resources Science). I started my career as a construction designer, and then for 10 years worked as a hydrologist at the Croatian Meteorological and Hydrological Service. I currently work for my family’s business, and freelance as a data analyst.\nI live in Zagreb, Croatia, with my two boys (life partner and our sweet son).\nIf you’d like to connect with me, please do. I hope you enjoy your visit!\n\n\nSince the bootstrap icon for Tableau is not yet available, here is the link to my Tableau Public repository."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sandra Jurela",
    "section": "",
    "text": "Test - Altair Viz\n\n\n\n\n\n\n\npython\n\n\naltair\n\n\n\n\nTesting Altair Viz rendering.\n\n\n\n\n\n\nMay 20, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nShiny App - Mass Shootings in the USA\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nEDA\n\n\ndata cleaning\n\n\n\n\nEDA with Shiny app on mass shootings between August 20th, 1982 and May 6th, 2023.\n\n\n\n\n\n\nMar 1, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Segmentation With K-Means and UMAP\n\n\n\n\n\n\n\nR\n\n\nk-means\n\n\nUMAP\n\n\n\n\nA report for the non-technical marketing team.\n\n\n\n\n\n\nDec 15, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCryptocurrency SQL Case Study\n\n\n\n\n\n\n\nSQL\n\n\nPostgreSQL\n\n\nEDA\n\n\n\n\nData with Danny - SQL masterclass - O’Reilly. With R solutions for Part 4: Window functions.\n\n\n\n\n\n\nNov 25, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nScraping 200 Best Movies of 2010s from Rotten Tomatoes\n\n\n\n\n\n\n\nweb scraping\n\n\ndata wrangling\n\n\nrvest\n\n\nR\n\n\nTableau\n\n\n\n\nScraping data on movies from Rotten Tomatoes and finally creating a dashboard in Tableau\n\n\n\n\n\n\nSep 1, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nTitanic Survival Exercises\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ndata visualization\n\n\n\n\nAssessment of visualization skills acquired in the HarvardX’s Data Science: Visualization course.\n\n\n\n\n\n\nJun 9, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Analysis – Advanced Plots With {ggplot2}\n\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\nSQL\n\n\nPostgreSQL\n\n\nR\n\n\n\n\nQuerying database in R code chunk and making some useful plots with ggplot2.\n\n\n\n\n\n\nMay 20, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nAnswering Business Questions Using SQL\n\n\n\n\n\n\n\nSQL\n\n\nSQLite\n\n\ndata visualization\n\n\nR\n\n\n\n\nMy SQL project for the “Intermediate SQL for Data Analysis” course at Dataquest.\n\n\n\n\n\n\nFeb 15, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html",
    "href": "posts/answering-business-questions-using-sql/chinook.html",
    "title": "Answering Business Questions Using SQL",
    "section": "",
    "text": "The aim of this project is to explore a modified version of the Chinook database using SQL and answer some business questions. The Chinook database represents a fictional digital media shop, based on real data from an iTunes Library and manually generated data. The database is provided as a SQLite database file called chinook.db.\nHere’s a schema diagram for the Chinook database:"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "href": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "title": "Answering Business Questions Using SQL",
    "section": "Connecting to the Database and Data Overview",
    "text": "Connecting to the Database and Data Overview\n\nlibrary(DBI)\n\ndb <- dbConnect(RSQLite::SQLite(), dbname = \"data/chinook.db\")\n\nListing all tables in the Chinook database.\n\nSELECT\n  name,\n  type\nFROM sqlite_master\nWHERE type IN (\"table\", \"view\")\n\n\n11 records\n\n\nname\ntype\n\n\n\n\nalbum\ntable\n\n\nartist\ntable\n\n\ncustomer\ntable\n\n\nemployee\ntable\n\n\ngenre\ntable\n\n\ninvoice\ntable\n\n\ninvoice_line\ntable\n\n\nmedia_type\ntable\n\n\nplaylist\ntable\n\n\nplaylist_track\ntable\n\n\ntrack\ntable\n\n\n\n\n\nThe database consists of 11 tables containing information about artists, albums, media tracks, playlists, invoices, customers, and shop employees. Let’s start by getting familiar with our data from the main tables:\nemployee table\n\nSELECT *\nFROM employee\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ncustomer table\n\nSELECT *\nFROM customer\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice table\n\nSELECT *\nFROM invoice\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice_line table\n\nSELECT *\nFROM invoice_line\nLIMIT 30\n\n\n\n\n\n  \n\n\n\ntrack table\n\nSELECT *\nFROM track\nLIMIT 20"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "href": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "title": "Answering Business Questions Using SQL",
    "section": "1. Selecting Albums to Purchase",
    "text": "1. Selecting Albums to Purchase\nThe Chinook record store has just signed a deal with a new record label, and you’ve been tasked with selecting the first three albums that will be added to the store, from a list of four. All four albums are by artists that don’t have any tracks in the store right now - we have the artist names, and the genre of music they produce:\n\n\n\nArtist Name\nGenre\n\n\n\n\nRegal\nHip-Hop\n\n\nRed Tone\nPunk\n\n\nMeteor and the Girls\nPop\n\n\nSlim Jim Bites\nBlues\n\n\n\nThe record label specializes in artists from the USA, and they have given Chinook some money to advertise the new albums in the USA, so we’re interested in finding out which genres sell the best in the USA.\nYou’ll need to write a query to find out which genres sell the most tracks in the USA, write up a summary of your findings, and make a recommendation for the three artists whose albums we should purchase for the store.\nInstructions\n\nWrite a query that returns each genre, with the number of tracks sold in the USA:\n\nin absolute numbers\nin percentages.\n\nWrite a paragraph that interprets the data and makes a recommendation for the three artists whose albums we should purchase for the store, based on sales of tracks from their genres.\n\n\nSELECT \n  g.name AS genre,\n  SUM(il.quantity) AS tracks_sold,\n  ROUND(CAST(SUM(il.quantity) AS FLOAT)/\n  (\n    SELECT SUM(il.quantity) \n    FROM invoice i\n    INNER JOIN invoice_line il\n    ON i.invoice_id = il.invoice_id\n    WHERE i.billing_country = 'USA'\n  ) \n  , 4) AS percentage_sold\nFROM invoice i\nINNER JOIN invoice_line il\nON i.invoice_id = il.invoice_id\nINNER JOIN track t \nON il.track_id = t.track_id\nINNER JOIN genre g\nON t.genre_id = g.genre_id\nWHERE i.billing_country = 'USA'\nGROUP BY genre\nORDER BY tracks_sold DESC\n\n\n\n\n\n\ngenre\ntracks_sold\npercentage_sold\n\n\n\n\nRock\n561\n0.5338\n\n\nAlternative & Punk\n130\n0.1237\n\n\nMetal\n124\n0.1180\n\n\nR&B/Soul\n53\n0.0504\n\n\nBlues\n36\n0.0343\n\n\nAlternative\n35\n0.0333\n\n\nPop\n22\n0.0209\n\n\nLatin\n22\n0.0209\n\n\nHip Hop/Rap\n20\n0.0190\n\n\nJazz\n14\n0.0133\n\n\nEasy Listening\n13\n0.0124\n\n\nReggae\n6\n0.0057\n\n\nElectronica/Dance\n5\n0.0048\n\n\nClassical\n4\n0.0038\n\n\nHeavy Metal\n3\n0.0029\n\n\nSoundtrack\n2\n0.0019\n\n\nTV Shows\n1\n0.0010\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_classic())\n\ngenre_of_interest <- c(\"Alternative & Punk\", \"Hip Hop/Rap\", \"Pop\", \"Blues\")\n\ngenre_perc %>% \n  mutate(of_interest = ifelse(genre %in% genre_of_interest, \"yes\", \"no\"),\n         perc_text = scales::percent(percentage_sold, accuracy = 0.1)) %>% \n  ggplot(aes(x=tracks_sold, y=reorder(genre, tracks_sold, sum), fill=of_interest)) + \n  geom_bar(stat = 'identity', width = 0.7) +\n  geom_text(aes(label = perc_text), hjust = -0.2) +\n  labs(title = \"Sold Tracks by Genre, USA\", x = \"Tracks Sold\", y = \"Genre\", \n       fill = \"Genre of Interest\") + \n  scale_fill_manual(values = c(\"gray74\", \"orange\")) +\n  scale_x_continuous(limits = c(0, 600)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe most popular genres in the USA are Rock, Alternative & Punk, and Metal, followed with a big gap by all the others. Since our choice is limited by Hip-Hop, Punk, Pop, and Blues genres, and since we have to choose 3 out of 4 albums, we should purchase the new albums by the following artists:\n\nRed Tone (Punk)\nSlim Jim Bites (Blues)\nMeteor and the Girls (Pop)"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "title": "Answering Business Questions Using SQL",
    "section": "2. Analyzing Employee Sales Performance",
    "text": "2. Analyzing Employee Sales Performance\nEach customer for the Chinook store gets assigned to a sales support agent within the company when they first make a purchase. You have been asked to analyze the purchases of customers belonging to each employee to see if any sales support agent is performing either better or worse than the others.\nYou might like to consider whether any extra columns from the employee table explain any variance you see, or whether the variance might instead be indicative of employee performance.\nInstructions\n\nWrite a query that finds the total dollar amount of sales assigned to each sales support agent within the company. Add any extra attributes for that employee that you find are relevant to the analysis.\nWrite a short statement describing your results, and providing a possible interpretation.\n\n\nSELECT \n  e.first_name || ' ' || e.last_name AS sales_support_agent,\n  e.hire_date,\n  COUNT(DISTINCT c.customer_id) AS customers,\n  SUM(i.total) AS total_sales\nFROM employee e\nINNER JOIN customer c\nON e.employee_id = c.support_rep_id\nINNER JOIN invoice i\nON c.customer_id = i.customer_id\nGROUP BY sales_support_agent\n\n\n3 records\n\n\nsales_support_agent\nhire_date\ncustomers\ntotal_sales\n\n\n\n\nJane Peacock\n2017-04-01 00:00:00\n21\n1731.51\n\n\nMargaret Park\n2017-05-03 00:00:00\n20\n1584.00\n\n\nSteve Johnson\n2017-10-17 00:00:00\n18\n1393.92\n\n\n\n\n\nWhile there is a 20% difference in sales between Jane (the top employee) and Steve (the bottom employee), the difference roughly corresponds with the differences in their hiring dates."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "title": "Answering Business Questions Using SQL",
    "section": "3. Analyzing Sales by Country",
    "text": "3. Analyzing Sales by Country\nYour next task is to analyze the sales data for customers from each different country. You have been given guidance to use the country value from the customers table, and ignore the country from the billing address in the invoice table.\nInstructions\n\nWrite a query that collates data on purchases from different countries.\nWhere a country has only one customer, collect them into an “Other” group.\nThe results should be sorted by the total sales from highest to lowest, with the “Other” group at the very bottom.\nFor each country, include:\n\ntotal number of customers\ntotal value of sales\naverage value of sales per customer\naverage order value\n\n\n\nWITH t1 AS (\n  SELECT\n    CASE\n      WHEN COUNT(DISTINCT c.customer_id) = 1 THEN 'Other'\n      ELSE c.country\n      END AS country,\n    COUNT(DISTINCT c.customer_id) AS customers,\n    SUM(i.total) AS total_sales,\n    SUM(i.total)/COUNT(DISTINCT c.customer_id) AS avg_sales_per_cust,\n    AVG(i.total) AS avg_order\n  FROM customer c\n  INNER JOIN invoice i\n  ON c.customer_id = i.customer_id\n  GROUP BY country\n)\n\nSELECT \n  country,\n  SUM(customers) AS customers,\n  SUM(total_sales) AS total_sales,\n  AVG(avg_sales_per_cust) AS avg_sales_per_cust,\n  AVG(avg_order) AS avg_order\nFROM \n  (\n    SELECT\n      t1.*,\n      CASE \n        WHEN country = 'Other' THEN 1\n        ELSE 0\n        END AS sorted\n    FROM t1\n  )\nGROUP BY country\nORDER BY sorted, total_sales DESC\n\n\n\n\n\n\ncountry\ncustomers\ntotal_sales\navg_sales_per_cust\navg_order\n\n\n\n\nUSA\n13\n1040.49\n80.03769\n7.942672\n\n\nCanada\n8\n535.59\n66.94875\n7.047237\n\n\nBrazil\n5\n427.68\n85.53600\n7.011147\n\n\nFrance\n5\n389.07\n77.81400\n7.781400\n\n\nGermany\n4\n334.62\n83.65500\n8.161463\n\n\nCzech Republic\n2\n273.24\n136.62000\n9.108000\n\n\nUnited Kingdom\n3\n245.52\n81.84000\n8.768571\n\n\nPortugal\n2\n185.13\n92.56500\n6.383793\n\n\nIndia\n2\n183.15\n91.57500\n8.721429\n\n\nOther\n15\n1094.94\n72.99600\n7.445071\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, customers, total_sales) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(customers = customers/sum(customers),\n         total_sales = total_sales/sum(total_sales)) %>%\n  pivot_longer(-country, names_to = \"variable\", values_to = \"value\") %>% \n  ggplot(aes(x=value, y=country, fill=variable)) +\n  geom_bar(stat = \"identity\", width = 0.65, position = position_dodge(0.8)) +\n  labs(title=\"Share of Customers and Sales by Country\", x=\"share\", fill=\"\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"gray77\", \"seagreen3\")) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, avg_order, avg_sales_per_cust) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(avg_order = (avg_order - mean(avg_order)) / mean(avg_order),\n         avg_sales_per_cust = (avg_sales_per_cust - mean(avg_sales_per_cust)) / \n                                 mean(avg_sales_per_cust)) %>%\n  rename(`Average Order` = avg_order,\n         `Average Sales per Customer` = avg_sales_per_cust) %>% \n  pivot_longer(-country, names_to = \"variable\", values_to = \"pct_diff_from_mean\") %>% \n  ggplot(aes(x=pct_diff_from_mean, y=country, fill=variable)) +\n  geom_col(width = 0.65, position = position_dodge(0.8)) +\n  facet_wrap(~ variable, scales = \"free_x\") + \n  labs(title=\"Average Order & Average Sales per Customer\", \n       subtitle = \"(Percent Difference from Mean)\", x=\"pct diff from mean\", fill=\"\") +\n  scale_fill_manual(values = c(\"steelblue\", \"lightskyblue2\")) +\n  scale_x_continuous(labels = scales::percent) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe USA has the largest customer base and, consequently, the highest total sales.\nBased on the data, there may be opportunity in the following countries:\n\nCzech Republic\nUnited Kingdom\nIndia\n\nIt’s worth keeping in mind that the amount of data from each of these countries is relatively low. Because of this, we should be cautious spending too much money on new marketing campaigns, as the sample size is not large enough to give us high confidence. A better approach would be to run small campaigns in these countries, collecting and analyzing the new customers to make sure that these trends hold with new customers."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "href": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "title": "Answering Business Questions Using SQL",
    "section": "4. Albums vs Individual Tracks",
    "text": "4. Albums vs Individual Tracks\nThe Chinook store is setup in a way that allows customer to make purchases in one of the two ways:\n\npurchase a whole album\npurchase a collection of one or more individual tracks.\n\nThe store does not let customers purchase a whole album, and then add individual tracks to that same purchase (unless they do that by choosing each track manually). When customers purchase albums they are charged the same price as if they had purchased each of those tracks separately.\nManagement are currently considering changing their purchasing strategy to save money. The strategy they are considering is to purchase only the most popular tracks from each album from record companies, instead of purchasing every track from an album.\nWe have been asked to find out what percentage of purchases are individual tracks vs whole albums, so that management can use this data to understand the effect this decision might have on overall revenue.\nInstructions\n\nWrite a query that categorizes each invoice as either an album purchase or not, and calculates the following summary statistics:\n\nNumber of invoices\nPercentage of invoices\n\nWrite one to two sentences explaining your findings, and making a prospective recommendation on whether the Chinook store should continue to buy full albums from record companies\n\n\nWITH cat_purchase AS (\n  SELECT\n    il.invoice_id,\n    CASE\n      WHEN\n      COUNT(DISTINCT t.album_id) = 1\n      AND \n      COUNT(DISTINCT t.track_id) = c.count_album_tracks\n      THEN 'album'\n      ELSE 'individual track(s)'\n      END AS purchase_type,\n      c.count_album_tracks\n    FROM track t\n    JOIN invoice_line il\n    ON il.track_id = t.track_id\n    JOIN (SELECT COUNT(*) AS count_album_tracks, album_id\n          FROM track\n          GROUP BY album_id) c\n    ON c.album_id = t.album_id\n    GROUP BY invoice_id\n)\n\nSELECT\n  purchase_type,\n  COUNT(*) AS number_of_invoices,\n  ROUND(CAST(COUNT(*) AS float) / CAST(\n    (SELECT COUNT(*)\n    FROM invoice) AS FLOAT), 2) AS percentage_of_invoices\nFROM cat_purchase\nGROUP BY purchase_type\n\n\n2 records\n\n\npurchase_type\nnumber_of_invoices\npercentage_of_invoices\n\n\n\n\nalbum\n114\n0.19\n\n\nindividual track(s)\n500\n0.81\n\n\n\n\n\nAlbum purchases account for 19% of all purchases. Based on this data, I would recommend against purchasing only the most popular tracks from each album from record companies, since there is a potential of losing a significant portion of revenue."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "href": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "title": "Answering Business Questions Using SQL",
    "section": "5. Which artist is used in the most playlists?",
    "text": "5. Which artist is used in the most playlists?\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY number_of_playlists DESC, unique_tracks DESC\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nEugene Ormandy\nClassical\n7\n3\n\n\nBerliner Philharmoniker & Herbert Von Karajan\nClassical\n6\n3\n\n\nThe King’s Singers\nClassical\n6\n2\n\n\nEnglish Concert & Trevor Pinnock\nClassical\n6\n2\n\n\nAcademy of St. Martin in the Fields & Sir Neville Marriner\nClassical\n6\n2\n\n\nMichael Tilson Thomas & San Francisco Symphony\nClassical\n5\n2\n\n\nYo-Yo Ma\nClassical\n5\n1\n\n\nWilhelm Kempff\nClassical\n5\n1\n\n\nTon Koopman\nClassical\n5\n1\n\n\nSir Georg Solti, Sumi Jo & Wiener Philharmoniker\nOpera\n5\n1\n\n\n\n\n\nEugene Ormandy takes the first place with only 3 unique tracks in 7 different playlists. His music belongs to the classical genre, which we have previously seen is one of the least popular genres in the USA.\nIf we order this table by number of unique tracks, we get a completely different list.\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY unique_tracks DESC, number_of_playlists DESC\n\n\nDisplaying records 1 - 10\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nLed Zeppelin\nRock\n3\n114\n\n\nMetallica\nMetal\n4\n112\n\n\nU2\nRock\n3\n112\n\n\nIron Maiden\nMetal\n4\n95\n\n\nDeep Purple\nRock\n3\n92\n\n\nIron Maiden\nRock\n3\n81\n\n\nPearl Jam\nRock\n4\n54\n\n\nVan Halen\nRock\n3\n52\n\n\nOs Paralamas Do Sucesso\nLatin\n3\n49\n\n\nLost\nTV Shows\n2\n48"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "href": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "title": "Answering Business Questions Using SQL",
    "section": "6. How many tracks have been purchased vs not purchased?",
    "text": "6. How many tracks have been purchased vs not purchased?\n\nWITH all_and_purchased_tracks AS (\n  SELECT \n    t.track_id AS all_tracks,\n    il.track_id AS purch_tracks\n  FROM track t\n  LEFT JOIN invoice_line il\n  ON t.track_id = il.track_id\n)\n  \nSELECT\n  COUNT(DISTINCT all_tracks) AS total_tracks,\n  COUNT(DISTINCT purch_tracks) AS pirchased,\n  COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) AS not_purchased,\n  ROUND(CAST(COUNT(DISTINCT purch_tracks) AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_purchased,\n  ROUND(CAST(COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) \n    AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_not_purchased\nFROM all_and_purchased_tracks\n\n\n1 records\n\n\n\n\n\n\n\n\n\ntotal_tracks\npirchased\nnot_purchased\nperc_purchased\nperc_not_purchased\n\n\n\n\n3503\n1806\n1697\n0.516\n0.484\n\n\n\n\n\n\n\nCode\npie(c(51.6, 48.4), labels = c(\"purchased\", \"not purchased\"), \n    main = \"Purchased vs not purchased tracks\")\n\n\n\n\n\nAlmost half of all the unique tracks available in the Chinook store were never bought, probably being of unpopular genre or unpopular artists."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "href": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "title": "Answering Business Questions Using SQL",
    "section": "7. Do protected vs non-protected media types have an effect on popularity?",
    "text": "7. Do protected vs non-protected media types have an effect on popularity?\nLet’s take a look at the media_type table.\n\nSELECT *\nFROM media_type\n\n\n5 records\n\n\nmedia_type_id\nname\n\n\n\n\n1\nMPEG audio file\n\n\n2\nProtected AAC audio file\n\n\n3\nProtected MPEG-4 video file\n\n\n4\nPurchased AAC audio file\n\n\n5\nAAC audio file\n\n\n\n\n\nThere are 2 out of 5 media types that are protected.\n\nWITH t AS (\n  SELECT \n    t.track_id,\n    CASE\n      WHEN mt.name LIKE \"%protected%\" THEN \"yes\" ELSE \"no\"\n      END AS protected\n  FROM track t\n  JOIN media_type mt ON mt.media_type_id = t.media_type_id\n)\n\nSELECT \n  protected,\n  COUNT(DISTINCT t.track_id) AS unique_tracks,\n  ROUND(CAST(COUNT(DISTINCT t.track_id) AS FLOAT) / (SELECT COUNT(*) FROM track), 2)\n    AS 'unique_tracks_%',\n  COUNT(DISTINCT il.track_id) AS sold_unique_tracks,\n  COUNT(il.track_id) AS sold_tracks,\n  ROUND(CAST(COUNT(il.track_id) AS FLOAT) / (SELECT COUNT(*) FROM invoice_line), 2)\n    AS 'sold_tracks_%'\nFROM t\nLEFT JOIN invoice_line il ON t.track_id = il.track_id\nGROUP BY protected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprotected\nunique_tracks\nunique_tracks_%\nsold_unique_tracks\nsold_tracks\nsold_tracks_%\n\n\n\n\nno\n3052\n0.87\n1652\n4315\n0.91\n\n\nyes\n451\n0.13\n154\n442\n0.09\n\n\n\n\n\n\n\nCode\nby_media_type %>% \n  select(protected, unique_tracks, sold_tracks) %>% \n  rename(unique = unique_tracks, \n         sold = sold_tracks) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  mutate(tracks = as.factor(tracks) %>% fct_rev()) %>% \n  group_by(tracks) %>% \n  mutate(pct = count/sum(count) %>% round(2)) %>% \n  ungroup() %>% \n  ggplot(aes(x=tracks, y=count, fill=protected)) +\n  geom_col(width = 0.5, position = \"stack\", color = \"white\") +\n  geom_text(aes(label = count), position = position_stack(vjust = .5),\n            color = \"white\", fontface = \"bold\")+\n  scale_fill_manual(values = c(\"#fbc02d\", \"#03a9f4\")) +\n  theme(legend.position = \"top\")\nby_media_type %>% \n  select(protected, unique_tracks, sold_unique_tracks) %>% \n  rename(unique = unique_tracks, \n         unique_sold = sold_unique_tracks) %>% \n  mutate(unique_unsold = unique - unique_sold) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  filter(tracks != \"unique\") %>% \n  group_by(protected) %>% \n  mutate(percentage = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n  ggplot(aes(x=protected, y=count, fill=tracks)) +\n  geom_col(width = 0.5, position = \"fill\", color = \"white\") +\n  scale_fill_manual(values = c(\"seagreen3\", \"tomato\")) +\n  geom_text(aes(label = percentage), position = position_fill(vjust = .5), \n            color = \"white\", fontface = \"bold\") +\n  labs(y=\"proportion\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can make the following observations:\n\nOnly 13% of all the unique tracks available in the Chinook store are of protected media types.\nAmong all the tracks that were sold, those of protected media types amounts only to 9%.\nFrom all the unique tracks of protected media types, only 34,1% were sold, while from those of non-protected ones 54,1%.\n\nWe can conclude that the tracks of protected media types are much less popular than those of non-protected."
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "title": "Cryptocurrency SQL Case Study",
    "section": "",
    "text": "On November 18, 2022 I attended the SQL masterclass “SQL and PostgreSQL for Data Analytics”, presented live by Danny Ma on O’Reilly platform.\nThis Github repository contains all the necessary data, sql scripts, and other materials.\nI’m posting some parts of it here for my own reference, but I strongly recommend that you register for that event that takes place every once in a while. It’s free and worth it. Danny Ma is an excellent teacher and his explanations are outstanding.\n\n\nFor the purposes of this project, I created the local trading Postgres database on my machine and ran the sql script to create and populate the tables.\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database (typically via the DBI::dbConnect() function).\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"trading\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> trading@localhost:5432"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "title": "Cryptocurrency SQL Case Study",
    "section": "Introduction",
    "text": "Introduction\nIn this fictitious case study - Danny’s data mentors from the Data With Danny team have been busy trading cryptocurrency markets since 2017.\nThe main purpose for this case study is to analyze the performance of the DWD mentors over time. We will accomplish this by writing SQL queries to utilize all available datasets to answer a series of realistic business questions.\n\nAvailable Datasets\nAll of our data for this case study exists within the trading schema in the PostgreSQL database.\nThere are 3 data tables available in this schema:\n\nmembers\nprices\ntransactions\n\n\n\nEntity Relationship Diagram\nI drew the ERD here.\n\n\n\nData Dictionary and Overview\nThe trading.members table consists of information about the mentors from the Data With Danny team.\n\ntrading.members table\n\n\nColumn name\nDescription\n\n\n\n\nmember_id\nunique id for each mentor\n\n\nfirst_name\nfirst name for each mentor\n\n\nregion\nregion where each mentor is from\n\n\n\n\nSELECT * FROM trading.members\n\n\n14 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\nc9f0f8\nAbe\nUnited States\n\n\n45c48c\nBen\nAustralia\n\n\nd3d944\nEnoch\nAfrica\n\n\n6512bd\nVikram\nIndia\n\n\nc20ad4\nLeah\nAsia\n\n\nc51ce4\nPavan\nAustralia\n\n\naab323\nSonia\nAustralia\n\n\n\n\n\nThe trading.prices table consists of daily price and volume information from January 2017 through to August 2021 for the 2 most popular cryptocurrency tickers: Bitcoin and Ethereum.\n\ntrading.prices table\n\n\nColumn name\nDescription\n\n\n\n\nticker\none of either BTC or ETH\n\n\nmarket_date\nthe date for each record\n\n\nprice\nclosing price at end of day\n\n\nopen\nthe opening price\n\n\nhigh\nthe highest price for that day\n\n\nlow\nthe lowest price for that day\n\n\nvolume\nthe total volume traded\n\n\nchange\n% change in daily price\n\n\n\nThe first 5 rows from this dataset.\n\nSELECT * FROM trading.prices LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nopen\nhigh\nlow\nvolume\nchange\n\n\n\n\nETH\n2021-08-29\n3177.84\n3243.96\n3282.21\n3162.79\n582.04K\n-2.04%\n\n\nETH\n2021-08-28\n3243.90\n3273.78\n3284.58\n3212.24\n466.21K\n-0.91%\n\n\nETH\n2021-08-27\n3273.58\n3093.78\n3279.93\n3063.37\n839.54K\n5.82%\n\n\nETH\n2021-08-26\n3093.54\n3228.03\n3249.62\n3057.48\n118.44K\n-4.17%\n\n\nETH\n2021-08-25\n3228.15\n3172.12\n3247.43\n3080.70\n923.13K\n1.73%\n\n\n\n\n\nThe trading.transactions table consists of buy and sell transactions data for each trade made by the DWD mentors.\n\ntrading.transactions table\n\n\nColumn name\nDescription\n\n\n\n\ntxn_id\nunique ID for each transaction\n\n\nmember_id\nmember identifier for each trade\n\n\nticker\nthe ticker for each trade\n\n\ntxn_date\nthe date for each transaction\n\n\ntxn_type\neither BUY or SELL\n\n\nquantity\nthe total quantity for each trade\n\n\npercentage_fee\n% of total amount charged as fees\n\n\ntxn_time\nthe timestamp for each trade\n\n\n\nThe first 5 frows from this transactions table.\n\nSELECT * FROM trading.transactions LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\ntxn_id\nmember_id\nticker\ntxn_date\ntxn_type\nquantity\npercentage_fee\ntxn_time\n\n\n\n\n1\nc81e72\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n2\neccbc8\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n3\na87ff6\nBTC\n2017-01-01\nBUY\n50\n0.0\n2017-01-01\n\n\n4\ne4da3b\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n5\n167909\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 1️⃣: Basic Data Analysis Techniques",
    "text": "PART 1️⃣: Basic Data Analysis Techniques\n\nQuestion 1.1\nShow only the top 5 rows from the trading.members table.\n\nSELECT * \nFROM trading.members\nLIMIT 5\n\n\n5 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n\n\n\n\n\nQuestion 1.2\nSort all the rows in the trading.members table by first_name in alphabetical order and show the top 3 rows with all columns.\n\nSELECT * \nFROM trading.members\nORDER BY first_name\nLIMIT 3\n\n\n3 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc9f0f8\nAbe\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n\n\n\n\n\nQuestion 1.3\nCount the number of records from the trading.members table which have United States as the region value.\n\nSELECT \n  COUNT(*) AS record_count\nFROM trading.members\nWHERE region = 'United States'\n\n\n1 records\n\n\nrecord_count\n\n\n\n\n7\n\n\n\n\n\n\n\nQuestion 1.4\nSelect only the first_name and region columns for mentors who are not from Australia.\n\nSELECT \n  first_name, \n  region\nFROM trading.members\nWHERE region != 'Australia'\n\n\nDisplaying records 1 - 10\n\n\nfirst_name\nregion\n\n\n\n\nVipul\nUnited States\n\n\nCharlie\nUnited States\n\n\nNandita\nUnited States\n\n\nRowan\nUnited States\n\n\nAyush\nUnited States\n\n\nAlex\nUnited States\n\n\nAbe\nUnited States\n\n\nEnoch\nAfrica\n\n\nVikram\nIndia\n\n\nLeah\nAsia\n\n\n\n\n\n\n\nQuestion 1.5\nReturn only the unique region values from the trading.members table and sort the output by reverse alphabetical order.\n\nSELECT DISTINCT region\nFROM trading.members\nORDER BY region DESC\n\n\n5 records\n\n\nregion\n\n\n\n\nUnited States\n\n\nIndia\n\n\nAustralia\n\n\nAsia\n\n\nAfrica"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 2️⃣: Aggregate Functions for Data Analysis",
    "text": "PART 2️⃣: Aggregate Functions for Data Analysis\n\nQuestion 2.1\nHow many records are there per ticker value in the trading.prices table?\n\nSELECT\n  ticker,\n  COUNT(*) AS record_count\nFROM trading.prices\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nrecord_count\n\n\n\n\nBTC\n1702\n\n\nETH\n1702\n\n\n\n\n\n\n\nQuestion 2.2\nWhat is the maximum, minimum values for the price column for both Bitcoin and Ethereum in 2020?\n\nSELECT \n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nmin_price\nmax_price\n\n\n\n\nBTC\n4826.0\n28949.4\n\n\nETH\n107.9\n751.8\n\n\n\n\n\n\n\nQuestion 2.3\nWhat is the annual minimum, maximum and average price for each ticker?\n\nInclude a calendar_year column with the year from 2017 through to 2021\nCalculate a spread column which calculates the difference between the min and max prices\nRound the average price output to 2 decimal places\nSort the output in chronological order with Bitcoin records before Ethereum within each year\n\n\nSELECT\n  EXTRACT(YEAR FROM market_date) AS calendar_year,\n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price,\n  ROUND(AVG(price)::NUMERIC, 2) AS avg_price,\n  MAX(price) - MIN(price) AS spread\nFROM trading.prices\nGROUP BY calendar_year, ticker\nORDER BY calendar_year, ticker\n\n\nDisplaying records 1 - 10\n\n\ncalendar_year\nticker\nmin_price\nmax_price\navg_price\nspread\n\n\n\n\n2017\nBTC\n785.40\n19345.50\n3981.07\n18560.10\n\n\n2017\nETH\n8.20\n799.98\n220.34\n791.78\n\n\n2018\nBTC\n3228.70\n17172.30\n7552.16\n13943.60\n\n\n2018\nETH\n83.81\n1380.00\n481.33\n1296.19\n\n\n2019\nBTC\n3397.70\n13063.80\n7371.82\n9666.10\n\n\n2019\nETH\n104.55\n338.54\n180.99\n233.99\n\n\n2020\nBTC\n4826.00\n28949.40\n11111.63\n24123.40\n\n\n2020\nETH\n107.90\n751.80\n307.30\n643.90\n\n\n2021\nBTC\n29359.90\n63540.90\n44353.55\n34181.00\n\n\n2021\nETH\n729.12\n4167.78\n2199.12\n3438.66\n\n\n\n\n\n\n\nQuestion 2.4\nWhat is the monthly average of the price column for each ticker from January 2020 and after?\n\nCreate a month_start column with the first day of each month\nSort the output by ticker in alphabetical order and months in chronological order\nRound the average_price column to 2 decimal places\n\n\nSELECT\n  ticker,\n  DATE_TRUNC('MON', market_date)::DATE AS month_start,\n  ROUND(AVG(price)::NUMERIC, 2) AS average_price\nFROM trading.prices\nWHERE market_date >= '2020-01-01'\nGROUP BY ticker, month_start\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\naverage_price\n\n\n\n\nBTC\n2020-01-01\n8378.80\n\n\nBTC\n2020-02-01\n9636.56\n\n\nBTC\n2020-03-01\n6863.11\n\n\nBTC\n2020-04-01\n7211.03\n\n\nBTC\n2020-05-01\n9253.55\n\n\nBTC\n2020-06-01\n9481.85\n\n\nBTC\n2020-07-01\n9592.10\n\n\nBTC\n2020-08-01\n11638.41\n\n\nBTC\n2020-09-01\n10643.33\n\n\nBTC\n2020-10-01\n11888.36"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 3️⃣: Case When Statements",
    "text": "PART 3️⃣: Case When Statements\n\nQuestion 3.1\nConvert the volume column in the trading.prices table with an adjusted integer value to take into the unit values\n\nReturn only the market_date, price, volume and adjusted_volume columns for the first 10 days of August 2021 for Ethereum only\n\n\nSELECT\n  market_date,\n  price,\n  volume,    \n  CASE\n    WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000 \n    WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000000 \n    WHEN volume = '-' THEN 0\n    END AS adjusted_volume\nFROM trading.prices\nWHERE ticker = 'ETH'\n  AND market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY market_date\n\n\nDisplaying records 1 - 10\n\n\nmarket_date\nprice\nvolume\nadjusted_volume\n\n\n\n\n2021-08-01\n2556.23\n1.20M\n1200000\n\n\n2021-08-02\n2608.04\n970.67K\n970670\n\n\n2021-08-03\n2506.65\n158.45K\n158450\n\n\n2021-08-04\n2725.29\n1.23M\n1230000\n\n\n2021-08-05\n2827.21\n1.65M\n1650000\n\n\n2021-08-06\n2889.43\n1.06M\n1060000\n\n\n2021-08-07\n3158.00\n64.84K\n64840\n\n\n2021-08-08\n3012.07\n1.25M\n1250000\n\n\n2021-08-09\n3162.93\n1.44M\n1440000\n\n\n2021-08-10\n3140.71\n1.12M\n1120000\n\n\n\n\n\n\n\nQuestion 3.2\nHow many “breakout” days were there in 2020 where the price column is greater than the open column for each ticker? In the same query also calculate the number of “non breakout” days where the price column was lower than or equal to the open column.\n\nSELECT\n  ticker,\n  SUM(CASE WHEN price > open THEN 1 ELSE 0 END) AS breakout_days,\n  SUM(CASE WHEN price <= open THEN 1 ELSE 0 END) AS non_breakout_days\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nbreakout_days\nnon_breakout_days\n\n\n\n\nBTC\n207\n159\n\n\nETH\n200\n166\n\n\n\n\n\n\n\nQuestion 3.3\nWhat was the final quantity Bitcoin and Ethereum held by all Data With Danny mentors based off the trading.transactions table?\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'SELL' THEN -quantity ELSE quantity END) AS final_btc_holding\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nfinal_btc_holding\n\n\n\n\nBTC\n42848.67\n\n\nETH\n32801.04\n\n\n\n\n\nDivided by quantity bought and quantity sold.\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'BUY' THEN quantity ELSE 0 END) AS qty_bought,\n  SUM(CASE WHEN txn_type = 'SELL' THEN quantity ELSE 0 END) AS qty_sold\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nqty_bought\nqty_sold\n\n\n\n\nBTC\n53250.15\n10401.485\n\n\nETH\n42599.20\n9798.154"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 4️⃣: Window Functions",
    "text": "PART 4️⃣: Window Functions\n\nQuestion 4.1\nWhat are the market_date, price and volume and price_rank values for the days with the top 5 highest price values for each tickers in the trading.prices table?\n\nThe price_rank column is the ranking for price values for each ticker with rank = 1 for the highest value.\nReturn the output for Bitcoin, followed by Ethereum in price rank order.\n\n\nWITH cte_rank AS (\n  SELECT\n    ticker,\n    market_date,\n    price, \n    volume,\n    RANK() OVER (PARTITION BY ticker ORDER BY price DESC) AS price_rank\n  FROM trading.prices\n)\n\nSELECT *\nFROM cte_rank\nWHERE price_rank <= 5\nORDER BY ticker, price_rank\n\n\nDisplaying records 1 - 10\n\n\nticker\nmarket_date\nprice\nvolume\nprice_rank\n\n\n\n\nBTC\n2021-04-13\n63540.90\n126.56K\n1\n\n\nBTC\n2021-04-15\n63216.00\n76.97K\n2\n\n\nBTC\n2021-04-14\n62980.40\n130.43K\n3\n\n\nBTC\n2021-04-16\n61379.70\n136.85K\n4\n\n\nBTC\n2021-03-13\n61195.30\n134.64K\n5\n\n\nETH\n2021-05-11\n4167.78\n1.27M\n1\n\n\nETH\n2021-05-14\n4075.38\n2.06M\n2\n\n\nETH\n2021-05-10\n3947.90\n2.70M\n3\n\n\nETH\n2021-05-09\n3922.23\n1.94M\n4\n\n\nETH\n2021-05-08\n3905.55\n1.34M\n5\n\n\n\n\n\n\nQuestion 4.1 -  solution\n\nmembers <- read_csv(\"data/members_tbl.csv\")\nprices <- read_csv(\"data/prices_tbl.csv\")\ntransactions <- read_csv(\"data/transactions_tbl.csv\")\n\nprices %>% head()\n\n# A tibble: 6 × 8\n  ticker market_date price  open  high   low volume  change\n  <chr>  <date>      <dbl> <dbl> <dbl> <dbl> <chr>   <chr> \n1 ETH    2021-08-29  3178. 3244. 3282. 3163. 582.04K -2.04%\n2 ETH    2021-08-28  3244. 3274. 3285. 3212. 466.21K -0.91%\n3 ETH    2021-08-27  3274. 3094. 3280. 3063. 839.54K 5.82% \n4 ETH    2021-08-26  3094. 3228. 3250. 3057. 118.44K -4.17%\n5 ETH    2021-08-25  3228. 3172. 3247. 3081. 923.13K 1.73% \n6 ETH    2021-08-24  3173. 3323. 3358. 3151. 988.82K -4.41%\n\n\n\n# R solution\nprices %>% \n  select(ticker, market_date, price, volume) %>% \n  group_by(ticker) %>% \n  arrange(ticker, desc(price)) %>% \n  mutate(price_rank = row_number()) %>% \n  filter(price_rank <=5)\n\n# A tibble: 10 × 5\n# Groups:   ticker [2]\n   ticker market_date  price volume  price_rank\n   <chr>  <date>       <dbl> <chr>        <int>\n 1 BTC    2021-04-13  63541. 126.56K          1\n 2 BTC    2021-04-15  63216  76.97K           2\n 3 BTC    2021-04-14  62980. 130.43K          3\n 4 BTC    2021-04-16  61380. 136.85K          4\n 5 BTC    2021-03-13  61195. 134.64K          5\n 6 ETH    2021-05-11   4168. 1.27M            1\n 7 ETH    2021-05-14   4075. 2.06M            2\n 8 ETH    2021-05-10   3948. 2.70M            3\n 9 ETH    2021-05-09   3922. 1.94M            4\n10 ETH    2021-05-08   3906. 1.34M            5\n\n\n\n\n\nQuestion 4.2\nCalculate a 7 day rolling average for the price and volume columns in the trading.prices table for each ticker.\n\nReturn only the first 10 days of August 2021\n\n\n-- Step 1 - Adjusted prices CTE\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\n-- Step 2 - Moving Averages CTE\ncte_moving_averages AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    AVG(price) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_price,\n    volume,\n    AVG(volume) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_volume\n  FROM cte_adjusted_prices\n)\n\n-- final output\nSELECT * FROM cte_moving_averages\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date;\n\n\n20 records\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nmoving_avg_price\nvolume\nmoving_avg_volume\n\n\n\n\nBTC\n2021-08-01\n39878.30\n40052.657\n80330\n103645.71\n\n\nBTC\n2021-08-02\n39168.40\n40322.914\n74810\n88957.14\n\n\nBTC\n2021-08-03\n38130.30\n40134.100\n260\n74674.29\n\n\nBTC\n2021-08-04\n39736.90\n40096.057\n79220\n64717.14\n\n\nBTC\n2021-08-05\n40867.20\n40219.743\n130600\n72617.14\n\n\nBTC\n2021-08-06\n42795.40\n40304.314\n111930\n74542.86\n\n\nBTC\n2021-08-07\n44614.20\n40741.529\n112840\n84284.29\n\n\nBTC\n2021-08-08\n43792.80\n41300.743\n105250\n87844.29\n\n\nBTC\n2021-08-09\n46284.30\n42317.300\n117080\n93882.86\n\n\nBTC\n2021-08-10\n45593.80\n43383.514\n80550\n105352.86\n\n\nETH\n2021-08-01\n2556.23\n2394.166\n1200000\n1069824.29\n\n\nETH\n2021-08-02\n2608.04\n2448.239\n970670\n938491.43\n\n\nETH\n2021-08-03\n2506.65\n2477.729\n158450\n782555.71\n\n\nETH\n2021-08-04\n2725.29\n2538.611\n1230000\n819850.00\n\n\nETH\n2021-08-05\n2827.21\n2602.366\n1650000\n963742.86\n\n\nETH\n2021-08-06\n2889.43\n2663.577\n1060000\n968028.57\n\n\nETH\n2021-08-07\n3158.00\n2752.979\n64840\n904851.43\n\n\nETH\n2021-08-08\n3012.07\n2818.099\n1250000\n911994.29\n\n\nETH\n2021-08-09\n3162.93\n2897.369\n1440000\n979041.43\n\n\nETH\n2021-08-10\n3140.71\n2987.949\n1120000\n1116405.71\n\n\n\n\n\n\nQuestion 4.2 -  solution\n\nprices %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  group_by(ticker) %>% \n  arrange(ticker, market_date) %>% \n  mutate(moving_avg_price = zoo::rollmean(price, k = 7, align = \"right\", fill = NA),\n         moving_avg_volume = zoo::rollmean(volume, k = 7, align = \"right\", fill = NA)) %>% \n  select(ticker, market_date, price, moving_avg_price, volume, moving_avg_volume) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 6\n# Groups:   ticker [2]\n   ticker market_date  price moving_avg_price  volume moving_avg_volume\n   <chr>  <date>       <dbl>            <dbl>   <dbl>             <dbl>\n 1 BTC    2021-08-01  39878.           40053.   80330           103646.\n 2 BTC    2021-08-02  39168.           40323.   74810            88957.\n 3 BTC    2021-08-03  38130.           40134.     260            74674.\n 4 BTC    2021-08-04  39737.           40096.   79220            64717.\n 5 BTC    2021-08-05  40867.           40220.  130600            72617.\n 6 BTC    2021-08-06  42795.           40304.  111930            74543.\n 7 BTC    2021-08-07  44614.           40742.  112840            84284.\n 8 BTC    2021-08-08  43793.           41301.  105250            87844.\n 9 BTC    2021-08-09  46284.           42317.  117080            93883.\n10 BTC    2021-08-10  45594.           43384.   80550           105353.\n11 ETH    2021-08-01   2556.            2394. 1200000          1069824.\n12 ETH    2021-08-02   2608.            2448.  970670           938491.\n13 ETH    2021-08-03   2507.            2478.  158450           782556.\n14 ETH    2021-08-04   2725.            2539. 1230000           819850 \n15 ETH    2021-08-05   2827.            2602. 1650000           963743.\n16 ETH    2021-08-06   2889.            2664. 1060000           968029.\n17 ETH    2021-08-07   3158             2753.   64840           904851.\n18 ETH    2021-08-08   3012.            2818. 1250000           911994.\n19 ETH    2021-08-09   3163.            2897. 1440000           979041.\n20 ETH    2021-08-10   3141.            2988. 1120000          1116406.\n\n\n\n\n\nQuestion 4.3\nCalculate the monthly cumulative volume traded for each ticker in 2020\n\nSort the output by ticker in chronological order with the month_start as the first day of each month\n\n\nWITH cte_monthly_volume AS (\n  SELECT\n    ticker,\n    DATE_TRUNC('MON', market_date)::DATE AS month_start,\n    SUM(\n      CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END\n  ) AS monthly_volume\n  FROM trading.prices\n  WHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\n  GROUP BY ticker, month_start\n)\n\nSELECT\n  ticker,\n  month_start,\n  SUM(monthly_volume) OVER (\n    PARTITION BY ticker\n    ORDER BY month_start\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n  ) AS cumulative_monthly_volume\nFROM cte_monthly_volume\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\ncumulative_monthly_volume\n\n\n\n\nBTC\n2020-01-01\n23451920\n\n\nBTC\n2020-02-01\n46839130\n\n\nBTC\n2020-03-01\n94680450\n\n\nBTC\n2020-04-01\n134302740\n\n\nBTC\n2020-05-01\n172687010\n\n\nBTC\n2020-06-01\n188026610\n\n\nBTC\n2020-07-01\n201272600\n\n\nBTC\n2020-08-01\n216762630\n\n\nBTC\n2020-09-01\n300641440\n\n\nBTC\n2020-10-01\n303060020\n\n\n\n\n\n\nQuestion 4.3 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  filter(market_date >= \"2020-01-01\", market_date <= \"2020-12-31\") %>% \n  mutate(month_start = lubridate::floor_date(market_date, unit = \"month\")) %>% \n  group_by(ticker, month_start) %>% \n  summarise(monthly_volume = sum(\n    case_when(\n      str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n      str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n      str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n    )\n  ) %>% \n  ungroup() %>% \n  group_by(ticker) %>% \n  mutate(cumulative_monthly_volume = cumsum(monthly_volume)) %>% \n  ungroup()\n\n# A tibble: 24 × 4\n   ticker month_start monthly_volume cumulative_monthly_volume\n   <chr>  <date>               <dbl>                     <dbl>\n 1 BTC    2020-01-01        23451920                  23451920\n 2 BTC    2020-02-01        23387210                  46839130\n 3 BTC    2020-03-01        47841320                  94680450\n 4 BTC    2020-04-01        39622290                 134302740\n 5 BTC    2020-05-01        38384270                 172687010\n 6 BTC    2020-06-01        15339600                 188026610\n 7 BTC    2020-07-01        13245990                 201272600\n 8 BTC    2020-08-01        15490030                 216762630\n 9 BTC    2020-09-01        83878810                 300641440\n10 BTC    2020-10-01         2418580                 303060020\n# … with 14 more rows\n\n\n\n\n\nQuestion 4.4\nCalculate the daily percentage change in volume for each ticker in the trading.prices table\n\nPercentage change can be calculated as (current - previous) / previous\nMultiply the percentage by 100 and round the value to 2 decimal places\nReturn data for the first 10 days of August 2021\n\n\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\ncte_previous_volume AS (\n  SELECT\n    ticker,\n    market_date,\n    volume,\n    LAG(volume) OVER (PARTITION BY ticker ORDER BY market_date) AS previous_volume\n  FROM cte_adjusted_prices\n  WHERE volume != 0\n)\n  \nSELECT\n  ticker,\n  market_date,\n  volume,\n  previous_volume,\n  ROUND(100 * (volume - previous_volume) / previous_volume, 2) AS daily_change\nFROM cte_previous_volume\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date\n\n\n20 records\n\n\nticker\nmarket_date\nvolume\nprevious_volume\ndaily_change\n\n\n\n\nBTC\n2021-08-01\n80330\n44650\n79.91\n\n\nBTC\n2021-08-02\n74810\n80330\n-6.87\n\n\nBTC\n2021-08-03\n260\n74810\n-99.65\n\n\nBTC\n2021-08-04\n79220\n260\n30369.23\n\n\nBTC\n2021-08-05\n130600\n79220\n64.86\n\n\nBTC\n2021-08-06\n111930\n130600\n-14.30\n\n\nBTC\n2021-08-07\n112840\n111930\n0.81\n\n\nBTC\n2021-08-08\n105250\n112840\n-6.73\n\n\nBTC\n2021-08-09\n117080\n105250\n11.24\n\n\nBTC\n2021-08-10\n80550\n117080\n-31.20\n\n\nETH\n2021-08-01\n1200000\n507080\n136.65\n\n\nETH\n2021-08-02\n970670\n1200000\n-19.11\n\n\nETH\n2021-08-03\n158450\n970670\n-83.68\n\n\nETH\n2021-08-04\n1230000\n158450\n676.27\n\n\nETH\n2021-08-05\n1650000\n1230000\n34.15\n\n\nETH\n2021-08-06\n1060000\n1650000\n-35.76\n\n\nETH\n2021-08-07\n64840\n1060000\n-93.88\n\n\nETH\n2021-08-08\n1250000\n64840\n1827.82\n\n\nETH\n2021-08-09\n1440000\n1250000\n15.20\n\n\nETH\n2021-08-10\n1120000\n1440000\n-22.22\n\n\n\n\n\n\nQuestion 4.4 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  arrange(ticker, market_date) %>% \n  mutate(volume_prev_day = lag(volume)) %>% \n  mutate(daily_change_perc = round(100*(volume - volume_prev_day)/volume_prev_day, 2)) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 5\n   ticker market_date  volume volume_prev_day daily_change_perc\n   <chr>  <date>        <dbl>           <dbl>             <dbl>\n 1 BTC    2021-08-01    80330           44650             79.9 \n 2 BTC    2021-08-02    74810           80330             -6.87\n 3 BTC    2021-08-03      260           74810            -99.6 \n 4 BTC    2021-08-04    79220             260          30369.  \n 5 BTC    2021-08-05   130600           79220             64.9 \n 6 BTC    2021-08-06   111930          130600            -14.3 \n 7 BTC    2021-08-07   112840          111930              0.81\n 8 BTC    2021-08-08   105250          112840             -6.73\n 9 BTC    2021-08-09   117080          105250             11.2 \n10 BTC    2021-08-10    80550          117080            -31.2 \n11 ETH    2021-08-01  1200000          507080            137.  \n12 ETH    2021-08-02   970670         1200000            -19.1 \n13 ETH    2021-08-03   158450          970670            -83.7 \n14 ETH    2021-08-04  1230000          158450            676.  \n15 ETH    2021-08-05  1650000         1230000             34.2 \n16 ETH    2021-08-06  1060000         1650000            -35.8 \n17 ETH    2021-08-07    64840         1060000            -93.9 \n18 ETH    2021-08-08  1250000           64840           1828.  \n19 ETH    2021-08-09  1440000         1250000             15.2 \n20 ETH    2021-08-10  1120000         1440000            -22.2"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 5️⃣: Table Joins",
    "text": "PART 5️⃣: Table Joins\n\nQuestion 5.1 - Inner Joins\nWhich top 3 mentors have the most Bitcoin quantity? Return the first_name of the mentors and sort the output from highest to lowest total_quantity.\n\nSELECT\n  m.first_name,\n  SUM(\n    CASE\n      WHEN t.txn_type = 'BUY' THEN t.quantity \n      WHEN t.txn_type = 'SELL' THEN -t.quantity \n      END\n  ) AS total_quantity\nFROM trading.transactions t\nINNER JOIN trading.members m\n  ON t.member_id = m.member_id\nWHERE ticker = 'BTC' \nGROUP By m.first_name\nORDER BY total_quantity DESC\nLIMIT 3\n\n\n3 records\n\n\nfirst_name\ntotal_quantity\n\n\n\n\nNandita\n4160.220\n\n\nLeah\n4046.091\n\n\nAyush\n3945.198\n\n\n\n\n\n\n\nQuestion 5.2 - Left Joins\nShow the market_date values which have less than 5 transactions? Sort the output in reverse chronological order.\n\nSELECT \n  p.market_date,\n  COUNT(t.txn_id) AS transaction_count\nFROM trading.prices p\nLEFT JOIN trading.transactions t\n  ON p.market_date = t.txn_date\n  AND p.ticker = t.ticker\nGROUP BY p.market_date\nHAVING COUNT(t.txn_id) < 5\nORDER BY p.market_date DESC\n\n\n8 records\n\n\nmarket_date\ntransaction_count\n\n\n\n\n2021-08-29\n0\n\n\n2021-08-28\n0\n\n\n2021-07-17\n3\n\n\n2021-01-06\n4\n\n\n2020-01-17\n4\n\n\n2019-07-15\n4\n\n\n2019-06-14\n3\n\n\n2018-10-20\n4\n\n\n\n\n\n\n\nQuestion 5.3 - Multiple Table Joins\nPart 1: Calculate the Dollar Cost Average\nWhat is the dollar cost average (btc_dca) for all Bitcoin purchases by region for each calendar year?\n\nCreate a column called year_start and use the start of the calendar year\nThe dollar cost average calculation is btc_dca = SUM(quantity x price) / SUM(quantity)\n\nPart 2: Yearly Dollar Cost Average Ranking\nUse this btc_dca value to generate a dca_ranking column for each year\n\nThe region with the lowest btc_dca each year has a rank of 1\n\nPart 3: Dollar Cost Average Yearly Percentage Change\nCalculate the yearly percentage change in DCA for each region to 2 decimal places\n\nThis calculation is (current - previous) / previous\n\nFinally order the output by region and year_start columns.\n\nWITH cte_dollar_cost_average AS (\n  SELECT\n    DATE_TRUNC('YEAR', transactions.txn_date)::DATE AS year_start,\n    members.region,\n    SUM(transactions.quantity * prices.price) / SUM(transactions.quantity) AS btc_dca\n  FROM trading.transactions\n  INNER JOIN trading.prices\n    ON transactions.ticker = prices.ticker\n    AND transactions.txn_date = prices.market_date\n  INNER JOIN trading.members\n    ON transactions.member_id = members.member_id\n  WHERE transactions.ticker = 'BTC'\n    AND transactions.txn_type = 'BUY'\n  GROUP BY year_start, members.region\n),\n  \ncte_window_functions AS (\n  SELECT\n    year_start,\n    region,\n    btc_dca,\n    RANK() OVER (PARTITION BY year_start ORDER BY btc_dca) AS dca_ranking,\n    LAG(btc_dca) OVER (PARTITION BY region ORDER BY year_start) AS previous_btc_dca\n  FROM cte_dollar_cost_average\n)\n  \nSELECT\n  year_start,\n  region,\n  btc_dca,\n  dca_ranking,\n  ROUND(\n    (100 * (btc_dca - previous_btc_dca) / previous_btc_dca)::NUMERIC,\n    2\n  ) AS dca_percentage_change\nFROM cte_window_functions\nORDER BY region, year_start\n\n\nDisplaying records 1 - 10\n\n\nyear_start\nregion\nbtc_dca\ndca_ranking\ndca_percentage_change\n\n\n\n\n2017-01-01\nAfrica\n3987.626\n4\nNA\n\n\n2018-01-01\nAfrica\n7690.713\n3\n92.86\n\n\n2019-01-01\nAfrica\n7368.820\n4\n-4.19\n\n\n2020-01-01\nAfrica\n11114.125\n3\n50.83\n\n\n2021-01-01\nAfrica\n44247.215\n2\n298.12\n\n\n2017-01-01\nAsia\n4002.939\n5\nNA\n\n\n2018-01-01\nAsia\n7829.999\n4\n95.61\n\n\n2019-01-01\nAsia\n7267.679\n1\n-7.18\n\n\n2020-01-01\nAsia\n10759.621\n2\n48.05\n\n\n2021-01-01\nAsia\n44570.901\n4\n314.24\n\n\n\n\n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "",
    "text": "The Bike Sales Database represents a bicycle manufacturer, including tables for products (bikes), customers (bike shops), and transactions (orders).\nIt consists of 3 tables:\n\nbikes table, which includes bicycle models, descriptions, and unit prices that are produced by the manufacturer.\nbikeshops table, which includes customers that the bicycle manufacturer has sold to.\norderlines table, which includes transactional data such as order ID, order line, date, customer, product, and quantity sold.\n\nbike_sales database is the local Postgres database stored on my machine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\nlibrary(lubridate)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database.\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"bike_sales\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> bike_sales@localhost:5432\n\n\n\n# list the database table names\ndbListTables(mycon)\n\n[1] \"bikeshops\"  \"bikes\"      \"orderlines\"\n\n\n\n# read the bikeshops table\ndbReadTable(mycon, \"bikeshops\") %>% head()\n\n  bikeshop.id                bikeshop.name       location\n1           1 Pittsburgh Mountain Machines Pittsburgh, PA\n2           2     Ithaca Mountain Climbers     Ithaca, NY\n3           3      Columbus Race Equipment   Columbus, OH\n4           4               Detroit Cycles    Detroit, MI\n5           5             Cincinnati Speed Cincinnati, OH\n6           6    Louisville Race Equipment Louisville, KY\n\n\n\n# read the bikes table\ndbReadTable(mycon, \"bikes\") %>% head()\n\n  bike.id                          model                description price\n1       1        Supersix Evo Black Inc. Road - Elite Road - Carbon 12790\n2       2       Supersix Evo Hi-Mod Team Road - Elite Road - Carbon 10660\n3       3 Supersix Evo Hi-Mod Dura Ace 1 Road - Elite Road - Carbon  7990\n4       4 Supersix Evo Hi-Mod Dura Ace 2 Road - Elite Road - Carbon  5330\n5       5     Supersix Evo Hi-Mod Utegra Road - Elite Road - Carbon  4260\n6       6               Supersix Evo Red Road - Elite Road - Carbon  3940\n\n\n\n# read the orderlines table\ndbReadTable(mycon, \"orderlines\") %>% head()\n\n  order.id order.line order.date customer.id product.id quantity\n1        1          1 2011-01-07           2         48        1\n2        1          2 2011-01-07           2         52        1\n3        2          1 2011-01-10          10         76        1\n4        2          2 2011-01-10          10         52        1\n5        3          1 2011-01-10           6          2        1\n6        3          2 2011-01-10           6         50        1\n\n\n\n# a simple query example\ndbGetQuery(mycon, \n          \"SELECT model, price \n           FROM bikes WHERE price > 10000 \n           ORDER BY price DESC\")\n\n                     model price\n1  Supersix Evo Black Inc. 12790\n2    Scalpel-Si Black Inc. 12790\n3  Habit Hi-Mod Black Inc. 12250\n4          F-Si Black Inc. 11190\n5 Supersix Evo Hi-Mod Team 10660\n\n\n\n\n\nIn all three tables there are dots in column names. This is not a good practice and I first had to figure out how to join the tables without an error! Here is the solution:\n\nbike_orderlines_joined <- dbGetQuery(mycon, \n\n'SELECT * \nFROM orderlines \nLEFT JOIN bikes\nON orderlines.\"product.id\" = bikes.\"bike.id\"\nLEFT JOIN bikeshops\nON orderlines.\"customer.id\" = bikeshops.\"bikeshop.id\"')\n\nhead(bike_orderlines_joined)\n\n  order.id order.line order.date customer.id product.id quantity bike.id\n1        1          1 2011-01-07           2         48        1      48\n2        1          2 2011-01-07           2         52        1      52\n3        2          1 2011-01-10          10         76        1      76\n4        2          2 2011-01-10          10         52        1      52\n5        3          1 2011-01-10           6          2        1       2\n6        3          2 2011-01-10           6         50        1      50\n                     model                       description price bikeshop.id\n1          Jekyll Carbon 2 Mountain - Over Mountain - Carbon  6070           2\n2         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970           2\n3      Beast of the East 1       Mountain - Trail - Aluminum  2770          10\n4         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970          10\n5 Supersix Evo Hi-Mod Team        Road - Elite Road - Carbon 10660           6\n6          Jekyll Carbon 4 Mountain - Over Mountain - Carbon  3200           6\n              bikeshop.name        location\n1  Ithaca Mountain Climbers      Ithaca, NY\n2  Ithaca Mountain Climbers      Ithaca, NY\n3         Kansas City 29ers Kansas City, KS\n4         Kansas City 29ers Kansas City, KS\n5 Louisville Race Equipment  Louisville, KY\n6 Louisville Race Equipment  Louisville, KY\n\n\n\nglimpse(bike_orderlines_joined)\n\nRows: 15,644\nColumns: 13\n$ order.id      <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,…\n$ order.line    <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1,…\n$ order.date    <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-01…\n$ customer.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ product.id    <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ quantity      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,…\n$ bike.id       <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ model         <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Eas…\n$ description   <chr> \"Mountain - Over Mountain - Carbon\", \"Mountain - Over Mo…\n$ price         <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570, …\n$ bikeshop.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ bikeshop.name <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\", …\n$ location      <chr> \"Ithaca, NY\", \"Ithaca, NY\", \"Kansas City, KS\", \"Kansas C…\n\n\nDisconnecting from the database.\n\ndbDisconnect(mycon)\n\n\n\n\n\nbike_orderlines <- bike_orderlines_joined %>% \n  # rename columns - replacing \".\" with \"_\"\n  set_names(names(.) %>% str_replace_all(\"\\\\.\", \"_\")) %>% \n  # remove the unnecessary columns \n  select(-c(customer_id, product_id, bike_id, bikeshop_id)) %>% \n  # separate description into category_1, category_2, and frame_material\n  separate(description, \n           c(\"category_1\", \"category_2\", \"frame_material\"),\n           sep = \" - \") %>% \n  # separate location into city and state\n  separate(location,\n           c(\"city\", \"state\"),\n           sep = \", \") %>%\n  # create a new column total_price\n  mutate(total_price = price * quantity) %>% \n  # reorder columns\n  select(contains(c(\"date\", \"id\", \"order\")),\n         quantity, price, total_price,\n         everything()) \n\nbike_orderlines %>% head()\n\n  order_date order_id order_line quantity price total_price\n1 2011-01-07        1          1        1  6070        6070\n2 2011-01-07        1          2        1  5970        5970\n3 2011-01-10        2          1        1  2770        2770\n4 2011-01-10        2          2        1  5970        5970\n5 2011-01-10        3          1        1 10660       10660\n6 2011-01-10        3          2        1  3200        3200\n                     model category_1    category_2 frame_material\n1          Jekyll Carbon 2   Mountain Over Mountain         Carbon\n2         Trigger Carbon 2   Mountain Over Mountain         Carbon\n3      Beast of the East 1   Mountain         Trail       Aluminum\n4         Trigger Carbon 2   Mountain Over Mountain         Carbon\n5 Supersix Evo Hi-Mod Team       Road    Elite Road         Carbon\n6          Jekyll Carbon 4   Mountain Over Mountain         Carbon\n              bikeshop_name        city state\n1  Ithaca Mountain Climbers      Ithaca    NY\n2  Ithaca Mountain Climbers      Ithaca    NY\n3         Kansas City 29ers Kansas City    KS\n4         Kansas City 29ers Kansas City    KS\n5 Louisville Race Equipment  Louisville    KY\n6 Louisville Race Equipment  Louisville    KY\n\n\n\nbike_orderlines %>% glimpse()\n\nRows: 15,644\nColumns: 13\n$ order_date     <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-0…\n$ order_id       <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7…\n$ order_line     <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1…\n$ quantity       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1…\n$ price          <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ total_price    <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ model          <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Ea…\n$ category_1     <chr> \"Mountain\", \"Mountain\", \"Mountain\", \"Mountain\", \"Road\",…\n$ category_2     <chr> \"Over Mountain\", \"Over Mountain\", \"Trail\", \"Over Mounta…\n$ frame_material <chr> \"Carbon\", \"Carbon\", \"Aluminum\", \"Carbon\", \"Carbon\", \"Ca…\n$ bikeshop_name  <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\",…\n$ city           <chr> \"Ithaca\", \"Ithaca\", \"Kansas City\", \"Kansas City\", \"Loui…\n$ state          <chr> \"NY\", \"NY\", \"KS\", \"KS\", \"KY\", \"KY\", \"KY\", \"KY\", \"KY\", \"…"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Lollipop Chart: Top N Customers",
    "text": "Lollipop Chart: Top N Customers\nQuestion: How much purchasing power is in top 10 customers?\nGoal is to visualize top N customers in terms of Revenue, including cumulative percentage.\n\nData manipulation\n\nn <- 10\n\ntop_customers <- bike_orderlines %>% \n  select(bikeshop_name, total_price) %>% \n  mutate(bikeshop_name = as_factor(bikeshop_name) %>% fct_lump_n(n = n, w = total_price)) %>% \n  group_by(bikeshop_name) %>% \n  summarise(revenue = sum(total_price)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_reorder(revenue)) %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_relevel(\"Other\", after = 0)) %>% \n  arrange(desc(bikeshop_name)) %>% \n  # revenue text\n  mutate(revenue_text = scales::dollar(revenue, scale = 1e-06, suffix = \"M\")) %>% \n  # cumulative percent\n  mutate(cum_pct = cumsum(revenue) / sum(revenue)) %>% \n  mutate(cum_pct_text = scales::percent(cum_pct)) %>% \n  # rank\n  mutate(rank = row_number()) %>% \n  mutate(rank = ifelse(rank == max(rank), NA_integer_, rank)) %>% \n  # label text\n  mutate(label_text = str_glue(\"Rank: {rank}\\nRev: {revenue_text}\\nCumPct: {cum_pct_text}\")) \n\ntop_customers\n\n# A tibble: 11 × 7\n   bikeshop_name                 revenue revenue…¹ cum_pct cum_p…²  rank label…³\n   <fct>                           <dbl> <chr>       <dbl> <chr>   <int> <glue> \n 1 Kansas City 29ers            11535455 $11.54M     0.162 16.2%       1 Rank: …\n 2 Denver Bike Shop              7697670 $7.70M      0.271 27.1%       2 Rank: …\n 3 Ithaca Mountain Climbers      6299335 $6.30M      0.359 35.9%       3 Rank: …\n 4 Phoenix Bi-peds               4168535 $4.17M      0.418 41.8%       4 Rank: …\n 5 Oklahoma City Race Equipment  3450040 $3.45M      0.467 46.7%       5 Rank: …\n 6 Las Vegas Cycles              3073615 $3.07M      0.510 51.0%       6 Rank: …\n 7 New Orleans Velocipedes       2761825 $2.76M      0.549 54.9%       7 Rank: …\n 8 Wichita Speed                 2380385 $2.38M      0.582 58.2%       8 Rank: …\n 9 Miami Race Equipment          2057130 $2.06M      0.611 61.1%       9 Rank: …\n10 Minneapolis Bike Shop         2023220 $2.02M      0.640 64.0%      10 Rank: …\n11 Other                        25585120 $25.59M     1     100.0%     NA Rank: …\n# … with abbreviated variable names ¹​revenue_text, ²​cum_pct_text, ³​label_text\n\n\n\n\nData visualization\n\ntop_customers %>% \n  ggplot(aes(revenue, bikeshop_name)) +\n  # geometries\n  geom_segment(aes(xend = 0, yend = bikeshop_name), \n               color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1],\n               size = 1) +\n  geom_point(color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1], \n             size = 3) +\n  geom_label(aes(label = label_text), \n             hjust = \"left\", \n             size = 3,\n             nudge_x = 0.8e+06) +\n  # formatting\n  scale_x_continuous(labels = scales::dollar_format(scale = 1e-06, suffix = \"M\")) +\n  labs(title = str_glue(\"Top {n} customers in terms of revenue, with cumulative percentage\"),\n       subtitle = str_glue(\"Top {n} customers contribute {top_customers$cum_pct_text[n]} of purchasing power.\"),\n       x = \"Revenue ($M)\",\n       y = \"Customer\",\n       caption = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\")) +\n  expand_limits(x = max(top_customers$revenue) + 6e+06) +\n  # theme\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Heatmap: Customers’ Purchasing Habits",
    "text": "Heatmap: Customers’ Purchasing Habits\nQuestion: Do specific customers have a purchasing preference?\nGoal is to visualize heatmap of proportion of sales by Secondary Product Category.\n\nData manipulation\n\npct_sales_by_customer <- bike_orderlines %>% \n  select(bikeshop_name, category_1, category_2, quantity) %>% \n  group_by(bikeshop_name, category_1, category_2) %>% \n  summarise(total_qty = sum(quantity)) %>% \n  ungroup() %>% \n  group_by(bikeshop_name) %>% \n  mutate(pct = total_qty / sum(total_qty)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = as.factor(bikeshop_name) %>% fct_rev()) %>%  \n  mutate(bikeshop_name_num = as.numeric(bikeshop_name))\n    \npct_sales_by_customer   \n\n# A tibble: 270 × 6\n   bikeshop_name      category_1 category_2         total_qty    pct bikeshop_…¹\n   <fct>              <chr>      <chr>                  <dbl>  <dbl>       <dbl>\n 1 Albuquerque Cycles Mountain   Cross Country Race        48 0.168           30\n 2 Albuquerque Cycles Mountain   Fat Bike                   9 0.0315          30\n 3 Albuquerque Cycles Mountain   Over Mountain             13 0.0455          30\n 4 Albuquerque Cycles Mountain   Sport                     35 0.122           30\n 5 Albuquerque Cycles Mountain   Trail                     38 0.133           30\n 6 Albuquerque Cycles Road       Cyclocross                 7 0.0245          30\n 7 Albuquerque Cycles Road       Elite Road                69 0.241           30\n 8 Albuquerque Cycles Road       Endurance Road            54 0.189           30\n 9 Albuquerque Cycles Road       Triathalon                13 0.0455          30\n10 Ann Arbor Speed    Mountain   Cross Country Race        32 0.0532          29\n# … with 260 more rows, and abbreviated variable name ¹​bikeshop_name_num\n\n\n\n\nData visualization\n\npct_sales_by_customer %>% \n  ggplot(aes(category_2, bikeshop_name)) + \n  # geometries\n  geom_tile(aes(fill = pct)) +\n  geom_text(aes(label = scales::percent(pct, accuracy = 0.1)),\n            size = 3,\n            color = ifelse(pct_sales_by_customer$pct >= 0.15, \"white\", \"black\")) +\n  facet_wrap(~ category_1, scales = \"free_x\") + \n  # formatting\n  scale_fill_gradient(low = \"white\", high = tidyquant::palette_light()[1]) + \n  labs(title = \"Heatmap of Purchasing Habits\", \n       subtitle = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\"),\n       x = \"Bike Type\",\n       y = \"Customer\") + \n  # theme\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = tidyquant::palette_light()[1], \n                                        color = \"white\"), \n        strip.text = element_text(color = \"white\", size = 11), \n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\nTop 3 customers that prefer mountain bikes:\n\nIthaca Mountain Climbers\nPittsburgh Mountain Machines\nTampa 29ers\n\nTop 3 customers that prefer road bikes:\n\nAnn Arbor Speed\nAustin Cruisers\nIndianapolis Velocipedes\n\n\nThat’s it! I hope you like it. For those wondering where I learned to make plots like this… in a fabulous course Data Science for Business Part 1 by Matt Dancho. This is probably the best course on R and I highly recommend it."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html",
    "href": "posts/shiny-mass-shootings/mass-shootings.html",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "",
    "text": "This post will be regularly updated with each new case.\nLast update on May 12, 2023."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data overview",
    "text": "Data overview\n\nlibrary(tidyverse)\nlibrary(tidygeocoder)\nlibrary(plotly)\ntheme_set(theme_classic())\n\nmass_shootings <- read_csv(\"data/mass_shootings_usa_1982-2023.csv\")\n\nmass_shootings %>% glimpse()\n\nRows: 143\nColumns: 24\n$ case                             <chr> \"Texas outlet mall shooting\", \"Louisv…\n$ location...2                     <chr> \"Allen, Texas\", \"Louisville, Kentucky…\n$ date                             <chr> \"5/6/23\", \"4/10/23\", \"3/27/23\", \"2/13…\n$ summary                          <chr> \"Mauricio Garcia, 33, wore tactical g…\n$ fatalities                       <dbl> 8, 5, 6, 3, 7, 11, 6, 5, 3, 5, 3, 7, …\n$ injured                          <chr> \"7\", \"8\", \"6\", \"5\", \"1\", \"10\", \"6\", \"…\n$ total_victims                    <chr> \"15\", \"13\", \"12\", \"8\", \"8\", \"21\", \"12…\n$ location...8                     <chr> \"Other\", \"workplace\", \"School\", \"Scho…\n$ age_of_shooter                   <chr> \"33\", \"25\", \"28\", \"43\", \"67\", \"72\", \"…\n$ prior_signs_mental_health_issues <chr> \"yes\", \"yes\", \"-\", \"-\", \"-\", \"yes\", \"…\n$ mental_health_details            <chr> \"Reportedly had a history of mental h…\n$ weapons_obtained_legally         <chr> \"yes\", \"yes\", \"yes\", \"yes\", \"-\", \"-\",…\n$ where_obtained                   <chr> \"-\", \"gun dealership in Louisville\", …\n$ weapon_type                      <chr> \"semiautomatic rifle\", \"semiautomatic…\n$ weapon_details                   <chr> \"AR-15-style rifle\", \"AR-15 rifle\", \"…\n$ race                             <chr> \"Latino\", \"White\", \"White\", \"Black\", …\n$ gender                           <chr> \"M\", \"M\", \"F (\\\"identifies as transge…\n$ sources                          <chr> \"https://www.nytimes.com/2023/05/07/u…\n$ mental_health_sources            <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"https://www…\n$ sources_additional_age           <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ latitude                         <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ longitude                        <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ type                             <chr> \"Mass\", \"Mass\", \"Mass\", \"Mass\", \"Spre…\n$ year                             <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2…\n\n\nWe have 143 cases, described with 24 variables. At first glance, this dataset clearly needs extensive cleaning."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n🧹 Step 1. Initial cleaning\nThe first cleaning step includes:\n\nselecting columns of interest,\nreplacing the character value \"-\" with NA in all columns with character data type,\nconverting date column from character to date data type,\nrenaming location columns,\nconverting character data type to numeric for specific columns.\n\n\nmass_shootings_cln <- mass_shootings %>% \n  select(1:6, 8:10, 12, 16, 17, 21:24) %>% \n  mutate(across(where(is.character), ~na_if(., \"-\"))) %>% \n  mutate(date = lubridate::mdy(date)) %>% \n  rename(location = location...2, location_2 = location...8) %>% \n  mutate_at(c(\"injured\", \"age_of_shooter\", \"latitude\", \"longitude\"), as.numeric)\n  \nmass_shootings_cln %>% glimpse()\n\nRows: 143\nColumns: 16\n$ case                             <chr> \"Texas outlet mall shooting\", \"Louisv…\n$ location                         <chr> \"Allen, Texas\", \"Louisville, Kentucky…\n$ date                             <date> 2023-05-06, 2023-04-10, 2023-03-27, …\n$ summary                          <chr> \"Mauricio Garcia, 33, wore tactical g…\n$ fatalities                       <dbl> 8, 5, 6, 3, 7, 11, 6, 5, 3, 5, 3, 7, …\n$ injured                          <dbl> 7, 8, 6, 5, 1, 10, 6, 25, 2, 2, 2, 46…\n$ location_2                       <chr> \"Other\", \"workplace\", \"School\", \"Scho…\n$ age_of_shooter                   <dbl> 33, 25, 28, 43, 67, 72, 31, 22, 22, 1…\n$ prior_signs_mental_health_issues <chr> \"yes\", \"yes\", NA, NA, NA, \"yes\", NA, …\n$ weapons_obtained_legally         <chr> \"yes\", \"yes\", \"yes\", \"yes\", NA, NA, N…\n$ race                             <chr> \"Latino\", \"White\", \"White\", \"Black\", …\n$ gender                           <chr> \"M\", \"M\", \"F (\\\"identifies as transge…\n$ latitude                         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ longitude                        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ type                             <chr> \"Mass\", \"Mass\", \"Mass\", \"Mass\", \"Spre…\n$ year                             <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2…\n\n\n🔎 Are there any duplicates? No.\n\nsum(duplicated(mass_shootings))\n\n[1] 0\n\n\n🔎 Number of missing values, NA, per column.\n\nmass_shootings_cln %>% \n  summarise_all(~sum(is.na(.))) %>% \n  # transposing for better visibility\n  pivot_longer(cols = everything(), names_to = \"column\", values_to = \"n_missing\")\n\n# A tibble: 16 × 2\n   column                           n_missing\n   <chr>                                <int>\n 1 case                                     0\n 2 location                                 0\n 3 date                                     0\n 4 summary                                  0\n 5 fatalities                               0\n 6 injured                                  1\n 7 location_2                               0\n 8 age_of_shooter                           2\n 9 prior_signs_mental_health_issues        28\n10 weapons_obtained_legally                16\n11 race                                    12\n12 gender                                   0\n13 latitude                                17\n14 longitude                               17\n15 type                                     0\n16 year                                     0\n\n\n17 of the most recent cases don’t have location coordinates at all. We’ll address this in the final cleanup step.\n\n\n🧹 Step 2. Fixing unique values of categorical variables\n🔎 Let’s take a look at the unique values of the gender column.\n\nmass_shootings_cln %>% \n  count(gender, sort = TRUE) \n\n# A tibble: 6 × 2\n  gender                                                                       n\n  <chr>                                                                    <int>\n1 \"Male\"                                                                      70\n2 \"M\"                                                                         67\n3 \"Female\"                                                                     2\n4 \"Male & Female\"                                                              2\n5 \"F\"                                                                          1\n6 \"F (\\\"identifies as transgender\\\" and \\\"Audrey Hale is a biological wom…     1\n\n\nAlmost all categorical variables need unique values correction.\nTo make a long story short, I’ll correct them all in one step using case_when function, and we’ll look at them later during the analysis.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  mutate(gender = case_when(gender == \"F\" ~ \"Female\",\n                            gender == \"M\" ~ \"Male\", \n                            gender %>% str_detect(\"transgender\")~\"Female (transgender)\",\n                            TRUE ~ gender),\n         race = case_when(race == \"white\" ~ \"White\",\n                          race == \"black\" ~ \"Black\",\n                          race == \"unclear\" ~ \"Unclear\",\n                          TRUE ~ race),\n         location_2 = \n           case_when(location_2 %in% c(\"workplace\", \"\\nWorkplace\") ~ \"Workplace\",\n                                location_2 == \"Other\\n\" ~ \"Other\",\n                                location_2 == \"religious\" ~ \"Religious\",\n                                TRUE ~ location_2),\n         prior_signs_mental_health_issues = \n           case_when(prior_signs_mental_health_issues == \"yes\" ~ \"Yes\",\n                     prior_signs_mental_health_issues == \"TBD\" ~ \"To be determined\",\n                     TRUE ~ prior_signs_mental_health_issues),\n         weapons_obtained_legally = \n           case_when(weapons_obtained_legally %in% c(\"yes\", \"\\nYes\") ~ \"Yes\",\n                     weapons_obtained_legally == \"TBD\" ~ \"To be determined\",\n                     weapons_obtained_legally %>% str_detect(\"Kelley\") ~ \"Unknown\",\n                     weapons_obtained_legally %>% str_detect(\"some\") ~ \"Partially\",\n                     TRUE ~ weapons_obtained_legally))\n\n\n\n🧹 Step 3. Geocoding locations with missing coordinates\nThere are 17 cases with missing location coordinates. In this step we’ll convert locations to coordinates with geocoding. and use them later to create a leaflet map for a shiny app.\nThe tidygeocoder package provides geocoding services. It’s designed to work easily with the tidyverse. It also provides access to several different geocoding services, including LocationIQ which I’m going to use here. LocationIQ is a freemium service that provides a free tier, which doesn’t require you to give them your billing details. When you sign up to LocationIQ, they’ll take you to the Manage Your API Access Tokens page, which is where we obtain our API token. Next, you need to provide the tidygeocoder package with your API key.\nYou can also use the Nominatim (“osm”) geocoding service (OpenStreetMap) which can be specified with the method argument (method = \"osm\"). I found LocationIQ to be faster.\nThe first step is to select only locations with missing coordinates and geocode them.\n\ngeocoded_locations <- mass_shootings_cln %>% \n  filter(is.na(latitude) | is.na(longitude)) %>% \n  select(location) %>% \n  geocode(location, method = \"iq\")\n\ngeocoded_locations %>% \n  mutate(across(where(is.numeric), ~ num(., digits = 6)))\n\n# A tibble: 17 × 3\n   location                         lat        long\n   <chr>                      <num:.6!>   <num:.6!>\n 1 Allen, Texas               33.103174  -96.670550\n 2 Louisville, Kentucky       38.254238  -85.759407\n 3 Nashville, Tennessee       36.162277  -86.774298\n 4 East Lansing, Michigan     42.732031  -84.472168\n 5 Half Moon Bay, California  37.463552 -122.428586\n 6 Monterey Park, California  34.051522 -118.129807\n 7 Chesapeake, Virginia       36.718371  -76.246680\n 8 Colorado Springs, Colorado 38.833958 -104.825348\n 9 Charlottesville, Virginia  38.029306  -78.476678\n10 Hedingham, North Carolina  35.808108  -78.541245\n11 Greenwood, Indiana         39.613658  -86.106653\n12 Highland Park, Illinois    42.181692  -87.800344\n13 Birmingham, Alabama        33.520682  -86.802433\n14 Smithsburg, Maryland       39.654819  -77.572768\n15 Tulsa, Oklahoma            36.156312  -95.992744\n16 Uvalde, Texas              29.300357  -99.773318\n17 Buffalo, New York          42.886717  -78.878392\n\n\nThe next step is to join mass shootings table with geocoded locations and replace missing latitudes and longitudes with geocoded.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  left_join(geocoded_locations, by = \"location\") %>% \n  mutate(latitude = ifelse(is.na(latitude), lat, latitude),\n         longitude = ifelse(is.na(longitude), long, longitude))\n\n🔎 Checking for null values.\n\nsum(is.na(mass_shootings_cln$latitude))\n\n[1] 0\n\nsum(is.na(mass_shootings_cln$longitude))\n\n[1] 0\n\n\nOK, this looks fine."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\n❕ Writing a function\nTo count unique values for all categorical variables separately, I’ll write a function, count_unique, to avoid copying and pasting a block of code several times.\nHere we have a special case where we have to pass a dataframe column name (variable) to a function argument. The solution is to embrace the argument by surrounding it in doubled braces, like group_by({{ var }}).\n\ncount_unique <- function(data, var) {\n  \n  data %>%\n    group_by({{ var }}) %>%    \n    summarise(count = n(), .groups = \"drop\") %>% \n    mutate(percent = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n    arrange(desc(count))\n\n}\n\n\n\n📄 Breakdown by categorical variables\n\nGender\n\ncount_unique(mass_shootings_cln, gender)\n\n# A tibble: 4 × 3\n  gender               count percent\n  <chr>                <int> <chr>  \n1 Male                   137 95.8%  \n2 Female                   3 2.1%   \n3 Male & Female            2 1.4%   \n4 Female (transgender)     1 0.7%   \n\n\n\n\nRace\n\ncount_unique(mass_shootings_cln, race) \n\n# A tibble: 8 × 3\n  race            count percent\n  <chr>           <int> <chr>  \n1 White              75 52.4%  \n2 Black              25 17.5%  \n3 Latino             12 8.4%   \n4 <NA>               12 8.4%   \n5 Asian              10 7.0%   \n6 Other               5 3.5%   \n7 Native American     3 2.1%   \n8 Unclear             1 0.7%   \n\n\n\n\nSpecific location\n\ncount_unique(mass_shootings_cln, location_2)\n\n# A tibble: 6 × 3\n  location_2 count percent\n  <chr>      <int> <chr>  \n1 Other         54 37.8%  \n2 Workplace     52 36.4%  \n3 School        22 15.4%  \n4 Religious      8 5.6%   \n5 Military       6 4.2%   \n6 Airport        1 0.7%   \n\n\n\n\nPrior signs of mental health issues\n\ncount_unique(mass_shootings_cln, prior_signs_mental_health_issues)\n\n# A tibble: 6 × 3\n  prior_signs_mental_health_issues count percent\n  <chr>                            <int> <chr>  \n1 Yes                                 68 47.6%  \n2 <NA>                                28 19.6%  \n3 Unclear                             24 16.8%  \n4 No                                  17 11.9%  \n5 To be determined                     5 3.5%   \n6 Unknown                              1 0.7%   \n\n\n\n\nWeapons obtained legally\n\ncount_unique(mass_shootings_cln, weapons_obtained_legally)\n\n# A tibble: 6 × 3\n  weapons_obtained_legally count percent\n  <chr>                    <int> <chr>  \n1 Yes                         96 67.1%  \n2 No                          16 11.2%  \n3 <NA>                        16 11.2%  \n4 To be determined             7 4.9%   \n5 Unknown                      7 4.9%   \n6 Partially                    1 0.7%   \n\n\n\n\nType\n\ncount_unique(mass_shootings_cln, type)\n\n# A tibble: 2 × 3\n  type  count percent\n  <chr> <int> <chr>  \n1 Mass    122 85.3%  \n2 Spree    21 14.7%  \n\n\nNote: Spree shootings here have three or more victims in a short time in multiple locations.\n\n\n\n📊 Age of shooter distribution\n\n\nCode for creating the age_group column\n# create \"age group\" column\nmass_shootings_cln <-  mass_shootings_cln %>% \n  mutate(age_group = case_when(\n    age_of_shooter >= 10 & age_of_shooter <= 14 ~ \"10-15\",\n    age_of_shooter <= 19 ~ \"15-20\",\n    age_of_shooter <= 24 ~ \"20-25\",\n    age_of_shooter <= 29 ~ \"25-30\",\n    age_of_shooter <= 34 ~ \"30-35\",\n    age_of_shooter <= 39 ~ \"35-40\",\n    age_of_shooter <= 44 ~ \"40-45\",\n    age_of_shooter <= 49 ~ \"45-50\",\n    age_of_shooter <= 54 ~ \"50-55\",\n    age_of_shooter <= 59 ~ \"55-60\",\n    age_of_shooter <= 64 ~ \"60-65\",\n    age_of_shooter <= 69 ~ \"65-70\",\n    age_of_shooter <= 74 ~ \"70-75\"))\n\n\n\np1 <- mass_shootings_cln %>% \n  filter(!is.na(age_group)) %>% \n  group_by(age_group) %>% \n  summarise(count = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n  mutate(label_text = str_glue(\"Age group: {age_group}\n                               Count: {count}\n                               Percent: {percent}\")) %>%\n  ggplot(aes(x = age_group, y = count, text = label_text)) +\n  geom_col(width = 0.7, fill = \"indianred\") +\n  labs(title = \"Age Distribution\", x = \"age group\") \n\nggplotly(p1, tooltip = \"text\")\n\n\n\n\n\n\n\nThe vast majority of shooters were between 15 and 50 years old.\nThe age distribution is bimodal, with one mode around 23 years of age and a second mode around 41 years of age.\nMost shooters were in the 20-25 age group (18.4 %), followed by 40-45 (16.3 %) and 25-30 (16.3 %).\n\n🔎 Who was the youngest shooter?\n\nindex <- which.min(mass_shootings_cln$age_of_shooter)\n\nmass_shootings_cln[index, ] %>% \n  select(case, date, summary, fatalities) %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\ncase\ndate\nsummary\nfatalities\n\n\n\n\nWestside Middle School killings\n1998-03-24\nMitchell Scott Johnson, 13, and Andrew Douglas Golden, 11, two juveniles, ambushed students and teachers as they left the school; they were apprehended by police at the scene.\n5\n\n\n\n\n\n\n\n📊 Number of cases per year\n\np2 <- mass_shootings_cln %>%\n  group_by(year) %>%\n  summarise(count = n()) %>% \n  ggplot(aes(year, count)) +\n  geom_col(fill = \"steelblue\") + \n  geom_smooth(method = \"loess\", se = FALSE, color = \"indianred\", size = 0.7) +\n  # geom_vline(xintercept = 2012, color = \"black\", linetype = \"dotted\") +\n  labs(title = \"Number of Cases per Year\") \n\nggplotly(p2)\n\n\n\n\n\n\n\nWe can see an increase in mass shootings in the last 12 years.\n2020 has a smaller number of cases probably due to Covid restrictions.\nThe data for 2023 is incomplete, but 6 cases in the first five months seems a lot.\n\n\n\n📊 Fatalities-Injured relationship\n\np3 <- mass_shootings_cln %>%\n  ggplot(aes(x = fatalities, y = injured)) +\n  geom_jitter() +\n  scale_y_sqrt() +\n  labs(title = \"Fatalities-Injured Relationship\")\n  \nggplotly(p3)\n\n\n\n\n\n\nPlease note that the Injured values are square root scaled for better visibility, but you can see the actual values by hovering over the points.\nSummary of fatalities\n\nsummary(mass_shootings_cln$fatalities)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3.00    4.00    6.00    7.79    8.00   58.00 \n\n\nSummary of injured people\n\nsummary(mass_shootings_cln$injured)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    1.00    3.00   11.27   10.00  546.00       1 \n\n\n\n\n📊 Total fatalities by state\n\n🛠️ Data manipulation\n\n# create us states with abbreviations tibble\nstates_with_abbr <- \n  tibble(state = state.name, abbr = state.abb) %>% \n  bind_rows(tibble(state = \"District of Columbia\", abbr = \"DC\"))\n\n# data manipulation\nby_state <- mass_shootings_cln %>% \n  # recode D.d. to District of Columbia\n  mutate(location = ifelse(location == \"Washington, D.C.\", \n                           \"Washington, District of Columbia\", \n                           location)) %>% \n  # separate location into city and state\n  separate(location, c(\"city\", \"state\"), sep = \", \") %>% \n  # group and summarize\n  group_by(state) %>% \n  summarise(total_cases = n(),\n            total_fatalities = sum(fatalities), .groups = \"drop\") %>% \n  # add us states abbreviations\n  left_join(states_with_abbr, by = \"state\") %>% \n  # rearrange columns\n  select(state, abbr, everything())\n\n\n\n📈 Top ten states regarding number of cases and fatalities\n\nby_state %>% \n  arrange(-total_cases, -total_fatalities) %>% \n  head(10)\n\n# A tibble: 10 × 4\n   state        abbr  total_cases total_fatalities\n   <chr>        <chr>       <int>            <dbl>\n 1 California   CA             25              175\n 2 Texas        TX             13              159\n 3 Florida      FL             12              126\n 4 Colorado     CO              8               53\n 5 Washington   WA              7               37\n 6 New York     NY              5               40\n 7 Wisconsin    WI              5               28\n 8 Pennsylvania PA              5               27\n 9 Illinois     IL              5               25\n10 Virginia     VA              4               53\n\n\n\n\n📊 Total fatalities by state visualization\n\nby_state %>% \n  plot_geo(locationmode = 'USA-states') %>% \n  add_trace(z = ~total_fatalities,\n            locations = ~abbr,\n            color = ~total_fatalities,\n            colors = ~\"Reds\") %>% \n  layout(\n    geo = list(\n      scope = \"usa\",\n      projection = list(type = \"albers usa\"),\n      lakecolor = toRGB(\"white\")\n    )\n  )"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Shiny app",
    "text": "Shiny app\nThe app you can see below is embedded in this quarto document since my website is static. It was originally published on shinyapps.io and you can also interact with it here.\nA quick note: With a free account I have 25 active hours (when my applications are not idle). If these 25 active hours are exceeded, my app will not be available again until the following month cycle. Hope you get lucky! 😊\n📢 By clicking on each circle, you can read a summary of the mass shooting case.\n\n\n\n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "title": "Titanic Survival Exercises",
    "section": "",
    "text": "After auditing the HarvardX’s Data Science: Visualization course I’ve found this assessment way too interesting and fun. So I decided to put all my new skills together to perform exploratory data analysis on a classic machine learning dataset: Titanic survival! My goal is to provide answers entirely through visualizations."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "title": "Titanic Survival Exercises",
    "section": "Background",
    "text": "Background\nThe Titanic was a British ocean liner that struck an iceberg and sunk on its maiden voyage in 1912 from the United Kingdom to New York. More than 1,500 of the estimated 2,224 passengers and crew died in the accident, making this one of the largest maritime disasters ever outside of war. The ship carried a wide range of passengers of all ages and both genders, from luxury travelers in first-class to immigrants in the lower classes. However, not all passengers were equally likely to survive the accident. We use real data about a selection of 891 passengers to learn who was on the Titanic and which passengers were more likely to survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "title": "Titanic Survival Exercises",
    "section": "Libraries, Customizations, and Data",
    "text": "Libraries, Customizations, and Data\n\nlibrary(tidyverse)\nlibrary(titanic)\n\noptions(digits = 3)  \ntheme_set(theme_classic())\ncolors_sex <- c(\"mediumorchid1\", \"dodgerblue\")\ncolors_survived <- c(\"gray65\", \"lightgreen\")\n\nDefining the titanic dataset.\n\ntitanic <- titanic_train %>%\n  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare) %>%\n  mutate(Survived = factor(Survived),\n         Pclass = factor(Pclass),\n         Sex = factor(Sex))\n\nhead(titanic)\n\n  Survived Pclass    Sex Age SibSp Parch  Fare\n1        0      3   male  22     1     0  7.25\n2        1      1 female  38     1     0 71.28\n3        1      3 female  26     0     0  7.92\n4        1      1 female  35     1     0 53.10\n5        0      3   male  35     0     0  8.05\n6        0      3   male  NA     0     0  8.46\n\nstr(titanic)\n\n'data.frame':   891 obs. of  7 variables:\n $ Survived: Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass  : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Sex     : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age     : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp   : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch   : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Fare    : num  7.25 71.28 7.92 53.1 8.05 ..."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "title": "Titanic Survival Exercises",
    "section": "Question 1: Variable Types",
    "text": "Question 1: Variable Types\nInstructions: Inspect the data and also use ?titanic_train to learn more about the variables in the dataset. Match these variables from the dataset to their variable type. There is at least one variable of each type (ordinal categorical, non-ordinal (nominal) categorical, continuous, discrete).\nChecking if Age variable is discrete or continuous…\n\nunique(titanic$Age)\n\n [1] 22.00 38.00 26.00 35.00    NA 54.00  2.00 27.00 14.00  4.00 58.00 20.00\n[13] 39.00 55.00 31.00 34.00 15.00 28.00  8.00 19.00 40.00 66.00 42.00 21.00\n[25] 18.00  3.00  7.00 49.00 29.00 65.00 28.50  5.00 11.00 45.00 17.00 32.00\n[37] 16.00 25.00  0.83 30.00 33.00 23.00 24.00 46.00 59.00 71.00 37.00 47.00\n[49] 14.50 70.50 32.50 12.00  9.00 36.50 51.00 55.50 40.50 44.00  1.00 61.00\n[61] 56.00 50.00 36.00 45.50 20.50 62.00 41.00 52.00 63.00 23.50  0.92 43.00\n[73] 60.00 10.00 64.00 13.00 48.00  0.75 53.00 57.00 80.00 70.00 24.50  6.00\n[85]  0.67 30.50  0.42 34.50 74.00\n\n\nAge is a continuous variable.\n\n\n\nVariable\nDescription\nVariable Type\n\n\n\n\nSurvived\nPassenger Survival Indicator\nnominal categorical\n\n\nPclass\nPassenger Class\nordinal categorical\n\n\nSex\nSex\nnominal categorical\n\n\nAge\nAge\ncontinuous\n\n\nSibSp\nNumber of Siblings/Spouses Aboard\ndiscrete\n\n\nParch\nNumber of Parents/Children Aboard\ndiscrete\n\n\nFare\nPassenger Fare\ncontinuous"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "title": "Titanic Survival Exercises",
    "section": "Question 2: Demographics of Titanic Passengers",
    "text": "Question 2: Demographics of Titanic Passengers\nInstructions: Make density plots of age grouped by sex. Try experimenting with combinations of faceting, alpha blending, stacking and using variable counts on the y-axis to answer the following questions. Some questions may be easier to answer with different versions of the density plot.\n\ntitanic %>% \n  ggplot(aes(Age)) +\n  geom_density(aes(color = Sex), size = 0.7) +\n  scale_color_manual(values = colors_sex) +\n  geom_vline(xintercept = c(18, 35), linetype = 2) +\n  geom_text(aes(x = 18, y = 0.031, label= \"18\", hjust = 1.5)) +\n  geom_text(aes(x = 35, y = 0.031, label= \"35\", hjust = -0.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, fill = Sex)) +\n  geom_density(alpha = 0.3) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 17, linetype = 2) +\n  geom_text(aes(x = 17, y = 0.031, label= \"17\", hjust = 1.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Sex)) +\n  geom_density(alpha = 0.7) +\n  facet_grid(Sex ~ .) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 40, linetype = 2) + \n  geom_text(aes(x = 40, y = 14, label= \"40\", hjust = -0.5)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers\n\n✅ Females and males had the same general shape of age distribution.\n✅ } The age distribution was bimodal, with one mode around 25 years of age and a second - - smaller mode around 5 years of age.\n❌ There were more females than males.\n✅ The count of males of age 40 was higher than the count of females of age 40.\n✅ The proportion of males age 18-35 was higher than the proportion of females age 18-35.\n✅ The proportion of females under age 17 was higher than the proportion of males under age 17.\n❌ The oldest passengers were female."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "title": "Titanic Survival Exercises",
    "section": "Question 3: QQ-plot of Age Distribution",
    "text": "Question 3: QQ-plot of Age Distribution\nInstructions: Use geom_qq() to make a QQ-plot of passenger age and add an identity line with geom_abline(). Filter out any individuals with an age of NA first.\n\nparams <- titanic %>%\n  filter(!is.na(Age)) %>%\n  summarize(mean = mean(Age), sd = sd(Age))\n\nparams\n\n  mean   sd\n1 29.7 14.5\n\ntitanic %>% ggplot(aes(sample = Age)) + \n  geom_qq(dparams = params) +\n  geom_abline()\n\n\n\n\nWhich of the following is the correct plot according to the instructions above?\n\n✅ The plot above."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "title": "Titanic Survival Exercises",
    "section": "Question 4: Survival by Sex",
    "text": "Question 4: Survival by Sex\nInstructions: To answer the following questions, make barplots of the Survived and Sex variables using geom_bar(). Try plotting one variable and filling by the other variable. You may want to try the default plot, then try adding position = position_dodge() to geom_bar() to make separate bars for each group.\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Sex)) +\n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_sex)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(width =  0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Less than half of passengers survived.\n✅ Most of the survivors were female.\n❌ Most of the males survived.\n✅ Most of the females survived."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "title": "Titanic Survival Exercises",
    "section": "Question 5: Survival by Age",
    "text": "Question 5: Survival by Age\nInstructions: Make a density plot of age filled by survival status. Change the y-axis to count and set alpha = 0.2.\nThe following answers were offered for all three questions:\n\n0-8\n10-18\n18-30\n30-50\n50-70\n70-80\n\n\nWhich age group is the only group more likely to survive than die?\n\ntitanic %>% \n  ggplot(aes(Age, y = ..count.., fill = Survived)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = colors_survived) +\n  geom_vline(xintercept = 8, linetype = 2) +\n  geom_text(aes(x = 8, y = 14, label= \"8\", hjust = -0.5))\n\n\n\n\n\n✅ Age group 0-8.\n\n\n\nWhich age group had the most deaths?\nIt’s hard to tell from the previews plot I’ll have to make a new column Age group based on the offered answers.\n\ntitanic2 <- titanic %>%\n  filter(!is.na(Age)) %>% \n  mutate(`Age group` = case_when(\n    Age > 0 & Age <= 8 ~ \"0-8\",\n    Age > 10 & Age <= 18 ~ \"10-18\",\n    Age > 18 & Age <= 30 ~ \"18-30\",\n    Age > 30 & Age <= 50 ~ \"30-50\",\n    Age > 50 & Age <= 70 ~ \"50-70\",\n    Age > 70 & Age <= 80 ~ \"70-80\"\n    )\n  )\n\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 18-30.\n\n\n\nWhich age group had the highest proportion of deaths?\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(position = \"fill\", width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 70-80"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "title": "Titanic Survival Exercises",
    "section": "Question 6: Survival by Fare",
    "text": "Question 6: Survival by Fare\nInstructions: Filter the data to remove individuals who paid a fare of 0. Make a boxplot of fare grouped by survival status. Try a log2 transformation of fares. Add the data points with jitter and alpha blending.\n\nset.seed(123)\n\ntitanic %>% filter(Fare != 0) %>% \n  ggplot(aes(Survived, Fare)) +\n  geom_boxplot(fill = colors_survived, width = 0.5, alpha = 0.5) + \n  geom_jitter(width = 0.1, alpha = 0.2) +\n  scale_y_continuous(trans = \"log2\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Passengers who survived generally payed higher fares than those who did not survive.\n❌ The interquartile range for fares was smaller for passengers who survived.\n✅ The median fare was lower for passengers who did not survive.\n❌ Only one individual paid a fare around $500. That individual survived. (3 individuals survived)\n✅ Most individuals who paid a fare around $8 did not survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 7: Survival by Passenger Class",
    "text": "Question 7: Survival by Passenger Class\nInstructions: The Pclass variable corresponds to the passenger class. Make three barplots. For the first, make a basic barplot of passenger class filled by survival. For the second, make the same barplot but use the argument position = position_fill() to show relative proportions in each group instead of counts. For the third, make a barplot of survival filled by passenger class using position = position_fill()\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Pclass)) + \n  geom_bar(width = 0.7) +\n  geom_text(aes(label = ..count..), stat = \"count\", vjust = -1) +\n  expand_limits(y = 530) +\n  ylab(\"count\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Pclass)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") \n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ There were more third class passengers than passengers in the first two classes combined.\n❌ There were the fewest passengers in first class, second-most passengers in second class, and most passengers in third class.\n✅ Survival proportion was highest for first class passengers, followed by second class. Third-class had the lowest survival proportion.\n✅ Most passengers in first class survived. Most passengers in other classes did not survive.\n❌ The majority of survivors were from first class.\n✅ The majority of those who did not survive were from third class."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 8: Survival by Age, Sex and Passenger Class",
    "text": "Question 8: Survival by Age, Sex and Passenger Class\nInstructions: Create a grid of density plots for age, filled by survival status, with count on the y-axis, faceted by sex and passenger class.\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Pclass)) +\n  geom_density(alpha=0.5) \n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Survived)) +\n  geom_density(alpha=0.5) +\n  facet_grid((Sex ~ Pclass)) +\n  scale_fill_manual(values = colors_survived) + \n  theme(panel.border = element_rect(colour = \"black\", fill = NA)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ The largest group of passengers was third-class males.\n❌ The age distribution is the same across passenger classes.\n❌ The gender distribution is the same across passenger classes.\n✅ Most first-class and second-class females survived.\n✅ Almost all second-class males did not survive, with the exception of children.\n\n\nThat’s all. Thanks for reading!"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "",
    "text": "The goal of this project is to scrape data on the 200 best movies of the last decade from the Rotten Tomatoes website with the R rvest package, and finally create a dashboard in Tableau. The idea is to show all the movies in one place. Hovering over the movie should reveal relevant data in the tooltip for quick overview. Clicking on the movie should open the movie’s website for more information.\nI’ve learned so much while working on this project (like web scraping, writing functions, iteration, …). The purrr package for functional programming is super-cool. It allows iteration with just one line of code (a very handy replacement for for loops).\nI hope you’ll enjoy the process as much as I did. At times it was quite challenging, but that’s how we learn!\n\n\n\n# loading packages\nlibrary(tidyverse)\nlibrary(rvest)\n\nAre we allowed to scrape data from the Rotten Tomatoes website?\n\nrobotstxt::paths_allowed(\"https://www.rottentomatoes.com/\")\n\n[1] TRUE"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "Plan",
    "text": "Plan\nThe data will be scraped from this page. Since it doesn’t contain all the data I am interested in, I have to visit every movie’s web page on the list and scrape data from there. Here is the plan:\n\nScrape data from the main page: the urls of movies, and the urls of images.\nScrape title, year_genre_runtime, critics_score, audiaece_score, and synopsis from the first movie to develop the code.\nWrite a function that scrapes data based on movie’s URL.\nIteration - use this function to scrape data from each individual movie and create a data frame with the columns title, year_genre_runtime, critics_score, audiaece_score, synopsis, and url.\nDownload images\nPrepare data for Tableau\nCreate a dashboard in Tableau"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "1. Scraping data from the main page",
    "text": "1. Scraping data from the main page\nReading the main page with read_html().\n\nmain_url <- \"https://editorial.rottentomatoes.com/guide/the-200-best-movies-of-the-2010s/\"\nmain_page <- read_html(main_url)\n\n\n\n\n\n\n\nFigure 1: The main page\n\n\n\n\nI make use of the SelectorGadget to identify the tags for the relevant nodes. Here is the link for Chrome (recommended).\n\nExtracting urls of movies\nThe same nodes that contain the text for the titles also contain information on the links to individual movie pages for each title. We can extract this information using the html_attr() function, which extracts attributes.\n\nmovie_urls <- main_page %>% \n  html_nodes(\".article_movie_title a\") %>% \n  html_attr(\"href\")\n\nmovie_urls %>% head()\n\n[1] \"https://www.rottentomatoes.com/m/12_years_a_slave\"    \n[2] \"https://www.rottentomatoes.com/m/20_feet_from_stardom\"\n[3] \"https://www.rottentomatoes.com/m/45_years\"            \n[4] \"https://www.rottentomatoes.com/m/all_is_lost_2013\"    \n[5] \"https://www.rottentomatoes.com/m/amazing_grace_2018\"  \n[6] \"https://www.rottentomatoes.com/m/american_hustle\"     \n\n\n\n\nExtracting urls of images\n\nimage_urls <- main_page %>% \n  html_nodes(\".article_poster\") %>% \n  html_attr(\"src\")\n\nLet’s check the image for the 6th title.\n\nknitr::include_graphics(image_urls[6])"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "2. Scraping data for the first movie on the list",
    "text": "2. Scraping data for the first movie on the list\nNow I’m going to scrape data for the movie 12 Years a Slave in order to develop the code.\nReading page for the first movie.\n\nurl <- \"https://www.rottentomatoes.com/m/12_years_a_slave\"\nmovie_page <- read_html(url)\n\n\n\n\n\n\n\nFigure 2: Title, year, genre, runtime, critics and audience score\n\n\n\n\nScroll down the page and you’ll find the movie synopsis.\n\n\n\n\n\nFigure 3: Synopsis\n\n\n\n\n\nExtracting title\n\ntitle <- movie_page %>% \n  html_node(\".scoreboard__title\") %>% \n  html_text()\n\ntitle\n\n[1] \"12 Years a Slave\"\n\n\n\n\nExtracting year, genre, and runtime\n\nyear_genre_runtime <- movie_page %>% \n  html_node(\".scoreboard__info\") %>% \n  html_text()\n\nyear_genre_runtime\n\n[1] \"2013, History/Drama, 2h 14m\"\n\n\n\n\nExtracting critics score\nThe next two are tricky. I had to look at the page source and find them manually.\n\ncritics_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"tomatometerscore\") %>% \n  str_c(.,\"%\")\n\ncritics_score\n\n[1] \"95%\"\n\n\n\n\nExtracting audience score\n\naudience_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"audiencescore\") %>% \n  str_c(.,\"%\")\n\naudience_score\n\n[1] \"90%\"\n\n\n\n\nExtracting movie synopsis\n\nsynopsis <- movie_page %>% \n  html_node(\"#movieSynopsis\") %>% \n  html_text2()\n\nsynopsis\n\n[1] \"In the years before the Civil War, Solomon Northup (Chiwetel Ejiofor), a free black man from upstate New York, is kidnapped and sold into slavery in the South. Subjected to the cruelty of one malevolent owner (Michael Fassbender), he also finds unexpected kindness from another, as he struggles continually to survive and maintain some of his dignity. Then in the 12th year of the disheartening ordeal, a chance meeting with an abolitionist from Canada changes Solomon's life forever.\"\n\n\n\n\nMakinging a data frame of extracted elements\n\nmovie  <- tibble(title = title, \n                 year_genre_runtime = year_genre_runtime,\n                 critics_score = critics_score,\n                 audience_score = audience_score,\n                 synopsis = synopsis,  \n                 url = url)\n\nmovie %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"12 Years a Slave\"\n$ year_genre_runtime <chr> \"2013, History/Drama, 2h 14m\"\n$ critics_score      <chr> \"95%\"\n$ audience_score     <chr> \"90%\"\n$ synopsis           <chr> \"In the years before the Civil War, Solomon Northup…\n$ url                <chr> \"https://www.rottentomatoes.com/m/12_years_a_slave\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "3. Writing a function",
    "text": "3. Writing a function\nInstead of manually scraping individual movies, I’ll write a function to do the same.\n\nscrape_movie <- function(x, ...){\n  \n  movie_page <- read_html(x)\n  \n  title <- movie_page %>% \n    html_node(\".scoreboard__title\") %>% \n    html_text()\n  \n  year_genre_runtime <- movie_page %>% \n    html_node(\".scoreboard__info\") %>% \n    html_text()\n  \n  critics_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"tomatometerscore\") %>% \n    str_c(.,\"%\")\n  \n  audience_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"audiencescore\") %>% \n    str_c(.,\"%\")\n  \n  synopsis <- movie_page %>% \n    html_node(\"#movieSynopsis\") %>% \n    html_text2()\n  \n  movie_df <- tibble(title = title, \n                     year_genre_runtime = year_genre_runtime,\n                     critics_score = critics_score,\n                     audience_score = audience_score,\n                     synopsis = synopsis,\n                     url = x)\n  \n  return(movie_df)\n  \n}\n\n\nFunction in action\nNow that we have the scrape_movie() function, let’s scrape data for the movie “American Hustle”.\n\nscrape_movie(movie_urls[6]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"American Hustle\"\n$ year_genre_runtime <chr> \"2013, Crime/Drama, 2h 18m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"74%\"\n$ synopsis           <chr> \"Irving Rosenfeld (Christian Bale) dabbles in forge…\n$ url                <chr> \"https://www.rottentomatoes.com/m/american_hustle\"\n\n\nOr “Ex Machina” (an interesting SF movie).\n\n scrape_movie(movie_urls[53]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"Ex Machina\"\n$ year_genre_runtime <chr> \"2014, Sci-fi/Mystery & thriller, 1h 47m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"86%\"\n$ synopsis           <chr> \"Caleb Smith (Domhnall Gleeson) a programmer at a h…\n$ url                <chr> \"https://www.rottentomatoes.com/m/ex_machina\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "4. Iteration",
    "text": "4. Iteration\nTo make my workflow a little more efficient, I make use of the map_dfr() function from the purrr package to iterate over all movie pages. map_dfr() will apply the scrape_movie()function to each element in the vector of links, and return a data frame created by row-binding. It’s as simple as that.\n\nmovies <- map_dfr(movie_urls, scrape_movie)\n\nmovies \n\n# A tibble: 200 × 6\n   title                year_genre_runtime         criti…¹ audie…² synop…³ url  \n   <chr>                <chr>                      <chr>   <chr>   <chr>   <chr>\n 1 12 Years a Slave     2013, History/Drama, 2h 1… 95%     90%     In the… http…\n 2 20 Feet From Stardom 2013, Documentary, 1h 30m  99%     82%     Filmma… http…\n 3 45 Years             2015, Drama, 1h 33m        97%     67%     As the… http…\n 4 All Is Lost          2013, Adventure/Mystery &… 94%     64%     During… http…\n 5 Amazing Grace        2018, Documentary/Music, … 99%     80%     Singer… http…\n 6 American Hustle      2013, Crime/Drama, 2h 18m  92%     74%     Irving… http…\n 7 Amy                  2015, Documentary/Biograp… 95%     87%     Archiv… http…\n 8 Anomalisa            2015, Comedy/Drama, 1h 30m 91%     71%     An ins… http…\n 9 Ant-Man and The Wasp 2018, Action/Adventure, 1… 87%     80%     Scott … http…\n10 Apollo 11            2019, Documentary/History… 99%     90%     Never-… http…\n# … with 190 more rows, and abbreviated variable names ¹​critics_score,\n#   ²​audience_score, ³​synopsis"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "5. Downloading images",
    "text": "5. Downloading images\nI’ve already extracted urls of images in the first step and saved them to image_urls. Now I’m going to create a directory and directory paths for the images.\n\nfs::dir_create(\"images/top_200_images/\")\n\npaths <- c(str_c(\"images/top_200_images/\", sprintf(\"%0.3d\", 1:200), \".jpg\"))\n\npaths %>% head()\n\n[1] \"images/top_200_images/001.jpg\" \"images/top_200_images/002.jpg\"\n[3] \"images/top_200_images/003.jpg\" \"images/top_200_images/004.jpg\"\n[5] \"images/top_200_images/005.jpg\" \"images/top_200_images/006.jpg\"\n\n\nSince Tableau sorts images alphabetically (1, 11, 111, 2, 22, …) by default, these leading zeros will help Tableau to correctly match the images with the data so I don’t have to do it manually.\n\nDownloading images\nThis time I’ll use map2() function from the purrr package, It will apply the download.file() function to pairs of elements from two vectors, image_urls and paths.\n\nmap2(image_urls, paths, function(.x, .y) download.file(.x, .y, mode=\"wb\")) \n\nAre the images properly saved? Let’s read in the image for the first movie.\n\nknitr::include_graphics(\"images/top_200_images/001.jpg\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "6. Data wrangling",
    "text": "6. Data wrangling\nPreparing the final dataset for Tableau.\n\nmovies <- movies %>% \n  \n  # separate year_genre_runtime column into year, genre, and runtime\n  separate(year_genre_runtime, sep = \", \", into = c(\"year\", \"genre\", \"runtime\")) %>% \n  mutate(year = as.factor(year)) %>% \n  \n  # separate genre column into primary and secondary genre\n  separate(genre, sep = \"/\", into = c(\"genre_1\", \"genre_2\"), remove = FALSE) %>% \n  \n  # create id column with leading zeroes so Tableau can automatically match the images\n  mutate(id = sprintf(\"%0.3d\", 1:200)) %>% \n  select(id, everything())\n\nmovies %>% head()\n\n# A tibble: 6 × 11\n  id    title  year  genre genre_1 genre_2 runtime criti…¹ audie…² synop…³ url  \n  <chr> <chr>  <fct> <chr> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>\n1 001   12 Ye… 2013  Hist… History Drama   2h 14m  95%     90%     In the… http…\n2 002   20 Fe… 2013  Docu… Docume… <NA>    1h 30m  99%     82%     Filmma… http…\n3 003   45 Ye… 2015  Drama Drama   <NA>    1h 33m  97%     67%     As the… http…\n4 004   All I… 2013  Adve… Advent… Myster… 1h 45m  94%     64%     During… http…\n5 005   Amazi… 2018  Docu… Docume… Music   1h 27m  99%     80%     Singer… http…\n6 006   Ameri… 2013  Crim… Crime   Drama   2h 18m  92%     74%     Irving… http…\n# … with abbreviated variable names ¹​critics_score, ²​audience_score, ³​synopsis\n\n\n\n# number of unique values in genre column\nmovies$genre %>% unique() %>% length()\n\n[1] 59\n\n\n\n# unique values in genre_1\nmovies$genre_1 %>% unique()\n\n [1] \"History\"            \"Documentary\"        \"Drama\"             \n [4] \"Adventure\"          \"Crime\"              \"Comedy\"            \n [7] \"Action\"             \"Sci-fi\"             \"Romance\"           \n[10] \"Horror\"             \"Biography\"          \"Mystery & thriller\"\n[13] \"Kids & family\"      \"War\"                \"Fantasy\"           \n[16] \"Musical\"            \"Western\"           \n\n\n\n# unique values in genre_2\nmovies$genre_2 %>% unique()\n\n [1] \"Drama\"              NA                   \"Mystery & thriller\"\n [4] \"Music\"              \"Biography\"          \"Adventure\"         \n [7] \"History\"            \"Romance\"            \"Comedy\"            \n[10] \"Lgbtq+\"             \"Action\"             \"War\"               \n[13] \"Fantasy\"            \"Sci-fi\"             \"Crime\"             \n[16] \"Musical\"            \"Western\"            \"Anime\"             \n[19] \"Horror\"            \n\n\nFinding values in genre_2, that are not in genre_1. This will help when creating a list parameter for filtering by primary or secondary genre.\n\nsetdiff(movies$genre_2, movies$genre_1)\n\n[1] NA       \"Music\"  \"Lgbtq+\" \"Anime\" \n\n\n\nDT table\nIf you prefer to search a table for data, then this one is for you!\n\nmovies %>% \n  select(1:9) %>% \n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\n\n\nWriting file\nI choose to save the data in an excel file only because the csv will remove the leading zeros in the id column.\n\nmovies %>% writexl::write_xlsx(\"datasets/top_200_movies_2010s_rotten_tomatoes.xlsx\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "7. Tableau dashboard",
    "text": "7. Tableau dashboard\nThe final dashboard is created in Tableau. It’s actually a jitter plot, which separates overlapping movies with the same critics’ score.\nTo avoid two filters, one for primary and one for secondary genre, a list parameter is created that filters movies by primary or secondary genre, or “All” values.\nFor the best viewing experience, please click on the full screen in the bottom right corner.\nYou can nteract with the embedded dashboard below or go to Tableau Public. Enjoy!"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#introduction",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#introduction",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Introduction",
    "text": "Introduction\nMass shootings have been a topic of intense discussion in the United States. A public “database” of mass shootings since 1982 has been made available by the Mother Jones, a non-profit organization. This “database” is stored in a Google spreadsheet. You can access it here and download as a CSV file.\nThere are many definitions of mass shooting. Here is what Britannica has to say:\n\nMass shooting, also called active shooter incident, as defined by the U.S. Federal Bureau of Investigation (FBI), an event in which one or more individuals are “actively engaged in killing or attempting to kill people in a populated area. Implicit in this definition is the shooter’s use of a firearm.” The FBI has not set a minimum number of casualties to qualify an event as a mass shooting, but U.S. statute (the Investigative Assistance for Violent Crimes Act of 2012) defines a “mass killing” as “3 or more killings in a single incident”."
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html",
    "href": "posts/customer-segmentation/cust-segm-report.html",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "",
    "text": "Marketing would like to increase email campaign engagement by segmenting the customer-base using their buying habbits."
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#problem-statement",
    "href": "posts/customer-segmentation/cust-segm-report.html#problem-statement",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Problem Statement",
    "text": "Problem Statement\nMarketing team would like to increase email campaign engagement by segmenting the customer-base using their buying habits."
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#solution-summary",
    "href": "posts/customer-segmentation/cust-segm-report.html#solution-summary",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Solution Summary",
    "text": "Solution Summary\nThe 4 customer segments were identified and given descriptions based on the customer’s top product purchases.\n\nSegment 1 Preferences: Road Bikes, Below $3200 (Economical Models) - 27%\nSegment 2 Preferences: Mountain Bikes, Above $3200 (Premium Models) - 10%\nSegment 3 Preferences: Road Bikes, Above $3200 (Premium Models) - 20%\nSegment 4 Preferences: Both Road and Mountain, Below $3200 (Economical Models) - 43%"
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#customer-preferences",
    "href": "posts/customer-segmentation/cust-segm-report.html#customer-preferences",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Customer Preferences",
    "text": "Customer Preferences\n\nHeat Map\nOur customer-base consists of 30 bike shops. Several customers have purchasing preferences for Road or Mountain Bikes based on the proportion of bikes purchased by category (mountain or road) and sub-category (Over Mountain, Trail, Elite Road, etc).\n\n\n\n\n\n\n\n\nCustomer Segmentation\nThis is a 2D Projection based on customer similarity that exposes 4 clusters, which are key segments in the customer base.\n\n\n\n\n\n\n\n\nCustomer Preferences By Segment\nThe 4 customer segments were given descriptions based on the customer’s top product purchases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: The table below is sortable. You can sort a column by clicking on its header."
  },
  {
    "objectID": "posts/customer-segmentation/tabs_badges.html",
    "href": "posts/customer-segmentation/tabs_badges.html",
    "title": "tabs_badges",
    "section": "",
    "text": "library(reactable)\nlibrary(htmltools)\n\norders <- data.frame(\n  Order = 2300:2304,\n  Created = seq(as.Date(\"2019-04-01\"), by = \"day\", length.out = 5),\n  Customer = sample(rownames(MASS::painters), 5),\n  Status = sample(c(\"Pending\", \"Paid\", \"Canceled\"), 5, replace = TRUE),\n  stringsAsFactors = FALSE\n)\n\nreactable(\n  orders,\n  columns = list(\n    Status = colDef(cell = function(value) {\n      class <- paste0(\"tag status-\", tolower(value))\n      div(class = class, value)\n    })\n  )\n)\n\n\n\n\n\n\n\n.tag {\n  display: inline-block;\n  padding: 0.125rem 0.75rem;\n  border-radius: 15px;\n  font-weight: 600;\n  font-size: 0.75rem;\n}\n\n.status-paid {\n  background: hsl(116, 60%, 90%);\n  color: hsl(116, 30%, 25%);\n}\n\n.status-pending {\n  background: hsl(230, 70%, 90%);\n  color: hsl(230, 45%, 30%);\n}\n\n.status-canceled {\n  background: hsl(350, 70%, 90%);\n  color: hsl(350, 45%, 30%);\n}"
  },
  {
    "objectID": "posts/airbnb-amsterdam/amsterdam-listings.html#setting-up-the-programming-environment",
    "href": "posts/airbnb-amsterdam/amsterdam-listings.html#setting-up-the-programming-environment",
    "title": "Streamlit app - Airbnb Amsterdam Listings",
    "section": "Setting up the programming environment",
    "text": "Setting up the programming environment\n\n# load libraries\nimport numpy as np\nimport pandas as pd\n\n# show all columns (instead of cascading columns in the middle)\npd.set_option(\"display.max_columns\", None)\n# don't show numbers in scientific notation\npd.set_option(\"display.float_format\", \"{:.2f}\".format)"
  },
  {
    "objectID": "posts/airbnb-amsterdam/amsterdam-listings.html#reading-the-data",
    "href": "posts/airbnb-amsterdam/amsterdam-listings.html#reading-the-data",
    "title": "Streamlit app - Airbnb Amsterdam Listings",
    "section": "Reading the data",
    "text": "Reading the data\n\n# read the Python Pickle and PyArrow Parquet files\ndf_list = pd.read_pickle(\"data/airbnb_amsterdam_listings.pkl\")\ndf_cal = pd.read_parquet(\"data/airbnb_amsterdam_calendar.parquet\", engine=\"pyarrow\")\n\n\ndf_list.head()\n\n\n\n\n\n  \n    \n      \n      id\n      host_acceptance_rate\n      host_is_superhost\n      host_listings_count\n      host_total_listings_count\n      neighbourhood_cleansed\n      latitude\n      longitude\n      room_type\n      accommodates\n      bedrooms\n      beds\n      amenities\n      price\n      minimum_nights\n      maximum_nights\n      has_availability\n      availability_30\n      availability_60\n      availability_90\n      availability_365\n      number_of_reviews\n      number_of_reviews_ltm\n      number_of_reviews_l30d\n      review_scores_rating\n      instant_bookable\n      reviews_per_month\n      price_in_euros\n      price_per_person\n      minimum_price\n      discount_per_5_days_booked\n      discount_per_10_days_booked\n      discount_per_30_and_more_days_booked\n      service_cost\n    \n  \n  \n    \n      0\n      23726706\n      0.95\n      f\n      1\n      1\n      IJburg - Zeeburgereiland\n      52.35\n      4.98\n      Private room\n      2\n      1.00\n      1.00\n      6\n      $88.00\n      2\n      14\n      t\n      0\n      6\n      20\n      66\n      78\n      11\n      3\n      4.99\n      t\n      1.53\n      None\n      $44\n      $176\n      5%\n      11%\n      16%\n      $4.99\n    \n    \n      1\n      35815036\n      1.00\n      t\n      1\n      1\n      Noord-Oost\n      52.42\n      4.96\n      Entire home/apt\n      2\n      NaN\n      1.00\n      5\n      $105.00\n      3\n      100\n      t\n      4\n      6\n      12\n      243\n      95\n      36\n      6\n      4.96\n      f\n      2.65\n      None\n      $52.5\n      $315\n      5%\n      12%\n      16%\n      $4.99\n    \n    \n      2\n      31553121\n      1.00\n      f\n      1\n      1\n      Noord-West\n      52.43\n      4.92\n      Entire home/apt\n      4\n      1.00\n      3.00\n      3\n      $152.00\n      2\n      60\n      t\n      0\n      3\n      3\n      3\n      82\n      26\n      1\n      4.74\n      f\n      2.02\n      None\n      $38\n      $304\n      7%\n      11%\n      22%\n      $4.99\n    \n    \n      3\n      34745823\n      0.94\n      f\n      3\n      3\n      Gaasperdam - Driemond\n      52.30\n      5.01\n      Entire home/apt\n      2\n      1.00\n      2.00\n      8\n      $87.00\n      2\n      1125\n      t\n      5\n      20\n      26\n      290\n      39\n      4\n      0\n      4.87\n      f\n      1.08\n      None\n      $43.5\n      $174\n      6%\n      10%\n      15%\n      $4.99\n    \n    \n      4\n      44586947\n      0.88\n      t\n      0\n      0\n      Gaasperdam - Driemond\n      52.31\n      5.03\n      Private room\n      4\n      2.00\n      3.00\n      4\n      $160.00\n      2\n      31\n      t\n      9\n      32\n      62\n      152\n      15\n      12\n      3\n      5.00\n      f\n      0.68\n      None\n      $40\n      $320\n      9%\n      22%\n      20%\n      $4.99\n    \n  \n\n\n\n\n\ndf_cal.head()\n\n\n\n\n\n  \n    \n      \n      listing_id\n      date\n      available\n      price_in_dollar\n      minimum_nights\n      maximum_nights\n    \n  \n  \n    \n      0\n      23726706\n      2022-06-05\n      False\n      90.00\n      2\n      1125\n    \n    \n      1\n      23726706\n      2022-06-06\n      False\n      90.00\n      2\n      1125\n    \n    \n      2\n      23726706\n      2022-06-07\n      False\n      90.00\n      2\n      1125\n    \n    \n      3\n      23726706\n      2022-06-08\n      False\n      90.00\n      2\n      1125\n    \n    \n      4\n      23726706\n      2022-06-09\n      False\n      85.00\n      2\n      1125"
  },
  {
    "objectID": "posts/airbnb-amsterdam/amsterdam-listings.html#cleaning-the-data",
    "href": "posts/airbnb-amsterdam/amsterdam-listings.html#cleaning-the-data",
    "title": "Streamlit app - Airbnb Amsterdam Listings",
    "section": "Cleaning the data",
    "text": "Cleaning the data"
  },
  {
    "objectID": "posts/airbnb-amsterdam/amsterdam-listings.html#streamlit-application",
    "href": "posts/airbnb-amsterdam/amsterdam-listings.html#streamlit-application",
    "title": "Streamlit app - Airbnb Amsterdam Listings",
    "section": "Streamlit application",
    "text": "Streamlit application"
  },
  {
    "objectID": "posts/airbnb-amsterdam/amsterdam-listings.html",
    "href": "posts/airbnb-amsterdam/amsterdam-listings.html",
    "title": "Streamlit app - Airbnb Amsterdam Listings",
    "section": "",
    "text": "{css, echo=FALSE} sourceCode {   border-left-color: #3fb618; }"
  },
  {
    "objectID": "posts/airbnb-amsterdam/amsterdam-listings.html#introduction",
    "href": "posts/airbnb-amsterdam/amsterdam-listings.html#introduction",
    "title": "Streamlit app - Airbnb Amsterdam Listings",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "posts/airbnb-amsterdam/amsterdam-listings.html#data-overview",
    "href": "posts/airbnb-amsterdam/amsterdam-listings.html#data-overview",
    "title": "Streamlit app - Airbnb Amsterdam Listings",
    "section": "Data overview",
    "text": "Data overview\nSetting up the programming environment\n\n# load libraries\nimport numpy as np\nimport pandas as pd\n\n# show all columns (instead of cascading columns in the middle)\npd.set_option(\"display.max_columns\", None)\n# don't show numbers in scientific notation\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\n\nReading the datasets\n\n# read the Python Pickle and PyArrow Parquet files\ndf_list = pd.read_pickle(\"data/airbnb_amsterdam_listings.pkl\")\ndf_cal = pd.read_parquet(\"data/airbnb_amsterdam_calendar.parquet\", engine=\"pyarrow\")\n\nDatasets overview\n\ndf_list.head()\n\n\n\n\n\n  \n    \n      \n      id\n      host_acceptance_rate\n      host_is_superhost\n      host_listings_count\n      host_total_listings_count\n      neighbourhood_cleansed\n      latitude\n      longitude\n      room_type\n      accommodates\n      bedrooms\n      beds\n      amenities\n      price\n      minimum_nights\n      maximum_nights\n      has_availability\n      availability_30\n      availability_60\n      availability_90\n      availability_365\n      number_of_reviews\n      number_of_reviews_ltm\n      number_of_reviews_l30d\n      review_scores_rating\n      instant_bookable\n      reviews_per_month\n      price_in_euros\n      price_per_person\n      minimum_price\n      discount_per_5_days_booked\n      discount_per_10_days_booked\n      discount_per_30_and_more_days_booked\n      service_cost\n    \n  \n  \n    \n      0\n      23726706\n      0.95\n      f\n      1\n      1\n      IJburg - Zeeburgereiland\n      52.35\n      4.98\n      Private room\n      2\n      1.00\n      1.00\n      6\n      $88.00\n      2\n      14\n      t\n      0\n      6\n      20\n      66\n      78\n      11\n      3\n      4.99\n      t\n      1.53\n      None\n      $44\n      $176\n      5%\n      11%\n      16%\n      $4.99\n    \n    \n      1\n      35815036\n      1.00\n      t\n      1\n      1\n      Noord-Oost\n      52.42\n      4.96\n      Entire home/apt\n      2\n      NaN\n      1.00\n      5\n      $105.00\n      3\n      100\n      t\n      4\n      6\n      12\n      243\n      95\n      36\n      6\n      4.96\n      f\n      2.65\n      None\n      $52.5\n      $315\n      5%\n      12%\n      16%\n      $4.99\n    \n    \n      2\n      31553121\n      1.00\n      f\n      1\n      1\n      Noord-West\n      52.43\n      4.92\n      Entire home/apt\n      4\n      1.00\n      3.00\n      3\n      $152.00\n      2\n      60\n      t\n      0\n      3\n      3\n      3\n      82\n      26\n      1\n      4.74\n      f\n      2.02\n      None\n      $38\n      $304\n      7%\n      11%\n      22%\n      $4.99\n    \n    \n      3\n      34745823\n      0.94\n      f\n      3\n      3\n      Gaasperdam - Driemond\n      52.30\n      5.01\n      Entire home/apt\n      2\n      1.00\n      2.00\n      8\n      $87.00\n      2\n      1125\n      t\n      5\n      20\n      26\n      290\n      39\n      4\n      0\n      4.87\n      f\n      1.08\n      None\n      $43.5\n      $174\n      6%\n      10%\n      15%\n      $4.99\n    \n    \n      4\n      44586947\n      0.88\n      t\n      0\n      0\n      Gaasperdam - Driemond\n      52.31\n      5.03\n      Private room\n      4\n      2.00\n      3.00\n      4\n      $160.00\n      2\n      31\n      t\n      9\n      32\n      62\n      152\n      15\n      12\n      3\n      5.00\n      f\n      0.68\n      None\n      $40\n      $320\n      9%\n      22%\n      20%\n      $4.99\n    \n  \n\n\n\n\n\n\nData dictionary for listings data\nHere is the definition of the listings data columns:\n\nid - Airbnb’s unique identifier for the listing\nhost_acceptance_rate - Rate at which a host accepts booking request\nhost_is_superhost - Some Airbnb hosts are considered better than others, which this feature displays\nhost_listings_count - The number of listings the host has (per Airbnb calculations)\nhost_total_listings_count - The number of listings the host has (per Airbnb calculations)\nneighborhood_cleansed - All neighborhoods present in Amsterdam\nlatitude - Geographical coordinate going from North to South\nlongitude - Geographical coordinate going from East to West\nroom_type - All room types available in Amsterdam on Airbnb\naccommodates - Maximum number of people that can fit in the Airbnb listing\nbedrooms - Number of bedrooms\nbeds - Number of beds (does not reveal if they are two-person or one-person beds)\namenities - Number of amenities present in the Airbnb listing\nprice - Price per day in US dollars\nminimum_nights - Minimum number of nights you need to book for this listing\nmaximum_nights - Maximum number of nights you are allowed to book this listing\nhas_availability - Displays if this listing can be booked\navailability_30 - Number of available spots in the next 30 days\nnumber_of_reviews_l30d - Number of reviews created in the last 30 days\nreview_scores_rating - Average rating given to the Airbnb listing\ninstant_bookable - Whether the guest can automatically book the listing without the host requiring to accept their booking request. An indicator of a commercial listing.\nprice_per_person - Price per person per night in US dollars\nminimum_price - Price in US dollars times minimum nights\ndiscount_per_… - Displays the discount when 5, 10, or 30+ days are booked\nservice_cost - Total service cost paid at booking\n\n\ndf_cal.head()\n\n\n\n\n\n  \n    \n      \n      listing_id\n      date\n      available\n      price_in_dollar\n      minimum_nights\n      maximum_nights\n    \n  \n  \n    \n      0\n      23726706\n      2022-06-05\n      False\n      90.00\n      2\n      1125\n    \n    \n      1\n      23726706\n      2022-06-06\n      False\n      90.00\n      2\n      1125\n    \n    \n      2\n      23726706\n      2022-06-07\n      False\n      90.00\n      2\n      1125\n    \n    \n      3\n      23726706\n      2022-06-08\n      False\n      90.00\n      2\n      1125\n    \n    \n      4\n      23726706\n      2022-06-09\n      False\n      85.00\n      2\n      1125\n    \n  \n\n\n\n\n\n\nData dictionary for calendar data\n\n\n\n\n\n\n\n\nColumn name\nData type\nDefinition\n\n\n\n\nlisting_id\ninteger\nAirbnb’s unique identifier for the listing\n\n\ndate\ndatetime\nThe date in the listing’s calendar\n\n\navailable\nboolean\nWhether the date is available for a booking\n\n\nprice_in_dollar\nfloat\nThe price listed for the day\n\n\nminimum_nights\ninteger\nMinimum nights for a booking made on this day\n\n\nmaximum_nights\ninteger\nMaximum nights for a booking made on this day"
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis.html",
    "href": "posts/vancouver-street-trees/trees-analysis.html",
    "title": "Vancouver Street Trees",
    "section": "",
    "text": "The City of Vancouver is one of the leading cities in Canada to advocate Open Data in the public sector. It has developed and maintained a robust public street trees database that is freely available to the public under the terms of the Open Government Licence – Vancouver.\nAs a Data Science practitioner and also a Data Liberation believer, it is my pleasure to explore a subset of the Vancouver Street Trees dataset that is introduced by the Data Visualization course at UBC. From this dataset, the intention of the analysis is to have an overall view of Vancouver street trees for their biological diversity, growth and distributions within neighbourhoods. The findings would provide reference and insights for the residents of Vancouver, botanists, urban greening project workers, and potential house buyers who are interested with the street trees in the city.\n\n\nThrough a completed Exploratory Data Analysis (EDA) on the above noted Vancouver Street Trees subset, the following questions have been defined according to the intention of the analysis.\n\nBased on the distribution of street trees planted in Vancouver by genus, which genus is the most popular one?\nIn the past 30 years, how many trees have been planted every single year? And what is the number of trees planted by genus each year?\nBased on the growth rates of each genus, which genus is the fastest one within the Vancouver street trees dataset?\nHow many street trees are growing in each neighbourhood and what are the average age and average diameter of trees in each neighbourhood?"
  },
  {
    "objectID": "posts/vancouver-street-trees/final_project_analysis_report.html",
    "href": "posts/vancouver-street-trees/final_project_analysis_report.html",
    "title": "Sandra Jurela",
    "section": "",
    "text": "November 7, 2021\n\n\nFinal Project Analysis Report by Peng Zhang\n\n\n\nThe City of Vancouver is one of the leading cities in Canada to advocate Open Data in the public sector. It has developed and maintained a robust public street trees database that is freely available to the public under the terms of the Open Government Licence – Vancouver.\nAs a Data Science practitioner and also a Data Liberation believer, it is my pleasure to explore a subset of the Vancouver Street Trees dataset that is introduced by the Data Visualization course at UBC. From this dataset, the intention of the analysis is to have an overall view of Vancouver street trees for their biological diversity, growth and distributions within neighbourhoods. The findings would provide reference and insights for the residents of Vancouver, botanists, urban greening project workers, and potential house buyers who are interested with the street trees in the city.\n\n\nThrough a completed Exploratory Data Analysis (EDA) on the above noted Vancouver Street Trees subset, the following questions have been defined according to the intention of the analysis.\n\nBased on the distribution of street trees planted in Vancouver by genus, which genus is the most popular one?\nIn the past 30 years, how many trees have been planted every single year? And what is the number of trees planted by genus each year?\nBased on the growth rates of each genus, which genus is the fastest one within the Vancouver street trees dataset?\nHow many street trees are growing in each neighbourhood and what are the average age and average diameter of trees in each neighbourhood?\n\n\n\n\n\n\n\n\n# Import libraries needed for this assignment\n\nimport altair as alt\nimport pandas as pd\nimport json\n\nalt.data_transformers.enable(\"data_server\")\n\n# Use data sever url method to read data\nURL = \"https://raw.githubusercontent.com/UBC-MDS/exploratory-data-viz/main/data/vancouver_trees.csv\"\ntrees_df_original = pd.read_csv(URL)\n\n# Glance at the original df\ntrees_df_original\n\n\n\n\n\n  \n    \n      \n      std_street\n      on_street\n      species_name\n      neighbourhood_name\n      date_planted\n      diameter\n      street_side_name\n      genus_name\n      assigned\n      civic_number\n      plant_area\n      curb\n      tree_id\n      common_name\n      height_range_id\n      on_street_block\n      cultivar_name\n      root_barrier\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      W 13TH AV\n      MAPLE ST\n      PSEUDOPLATANUS\n      Kitsilano\n      NaN\n      9.00\n      EVEN\n      ACER\n      N\n      1996\n      10\n      Y\n      13310\n      SYCAMORE MAPLE\n      4\n      2900\n      NaN\n      N\n      49.259856\n      -123.150586\n    \n    \n      1\n      WALES ST\n      WALES ST\n      PLATANOIDES\n      Renfrew-Collingwood\n      2018-11-28\n      3.00\n      ODD\n      ACER\n      N\n      5291\n      7\n      Y\n      259084\n      PRINCETON GOLD MAPLE\n      1\n      5200\n      PRINCETON GOLD\n      N\n      49.236650\n      -123.051831\n    \n    \n      2\n      W BROADWAY\n      W BROADWAY\n      RUBRUM\n      Kitsilano\n      1996-04-19\n      14.00\n      EVEN\n      ACER\n      N\n      3618\n      C\n      Y\n      167986\n      KARPICK RED MAPLE\n      3\n      3600\n      KARPICK\n      N\n      49.264250\n      -123.184020\n    \n    \n      3\n      PENTICTON ST\n      PENTICTON ST\n      CALLERYANA\n      Renfrew-Collingwood\n      2006-03-06\n      3.75\n      EVEN\n      PYRUS\n      N\n      2502\n      5\n      Y\n      213386\n      CHANTICLEER PEAR\n      1\n      2500\n      CHANTICLEER\n      Y\n      49.261036\n      -123.052921\n    \n    \n      4\n      RHODES ST\n      RHODES ST\n      GLYPTOSTROBOIDES\n      Renfrew-Collingwood\n      2001-11-01\n      3.00\n      ODD\n      METASEQUOIA\n      N\n      5639\n      N\n      Y\n      189223\n      DAWN REDWOOD\n      2\n      5600\n      NaN\n      N\n      49.233354\n      -123.050249\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      29995\n      ROBSON ST\n      ROBSON ST\n      CAMPESTRE\n      West End\n      NaN\n      7.00\n      ODD\n      ACER\n      N\n      1015\n      c\n      Y\n      122814\n      HEDGE MAPLE\n      2\n      1000\n      NaN\n      N\n      49.283666\n      -123.123231\n    \n    \n      29996\n      OSLER ST\n      CONNAUGHT DRIVE\n      PLATANOIDES\n      Shaughnessy\n      2007-04-16\n      8.00\n      ODD\n      ACER\n      N\n      4690\n      10\n      Y\n      132211\n      NORWAY MAPLE\n      1\n      1000\n      NaN\n      Y\n      49.243636\n      -123.129480\n    \n    \n      29997\n      BEATRICE ST\n      BEATRICE ST\n      CERASIFERA\n      Victoria-Fraserview\n      NaN\n      17.30\n      EVEN\n      PRUNUS\n      N\n      6218\n      9\n      Y\n      59355\n      PISSARD PLUM\n      3\n      6200\n      ATROPURPUREUM\n      N\n      49.227406\n      -123.066936\n    \n    \n      29998\n      ANGUS DRIVE\n      ANGUS DRIVE\n      BILOBA\n      Shaughnessy\n      2006-02-17\n      4.00\n      ODD\n      GINKGO\n      N\n      1551\n      9\n      Y\n      207753\n      GINKGO OR MAIDENHAIR TREE\n      1\n      1500\n      NaN\n      Y\n      49.254431\n      -123.140382\n    \n    \n      29999\n      MAIN ST\n      MAIN ST\n      EUCHLORA   X\n      Riley Park\n      NaN\n      12.00\n      ODD\n      TILIA\n      N\n      4323\n      C\n      Y\n      92997\n      CRIMEAN LINDEN\n      4\n      4300\n      NaN\n      N\n      49.246969\n      -123.101328\n    \n  \n\n30000 rows × 20 columns\n\n\n\n\n\n\n\n# Check columns of the original df\n\ntrees_df_original.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30000 entries, 0 to 29999\nData columns (total 20 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   std_street          30000 non-null  object \n 1   on_street           30000 non-null  object \n 2   species_name        30000 non-null  object \n 3   neighbourhood_name  30000 non-null  object \n 4   date_planted        14085 non-null  object \n 5   diameter            30000 non-null  float64\n 6   street_side_name    30000 non-null  object \n 7   genus_name          30000 non-null  object \n 8   assigned            30000 non-null  object \n 9   civic_number        30000 non-null  int64  \n 10  plant_area          29722 non-null  object \n 11  curb                30000 non-null  object \n 12  tree_id             30000 non-null  int64  \n 13  common_name         30000 non-null  object \n 14  height_range_id     30000 non-null  int64  \n 15  on_street_block     30000 non-null  int64  \n 16  cultivar_name       16178 non-null  object \n 17  root_barrier        30000 non-null  object \n 18  latitude            30000 non-null  float64\n 19  longitude           30000 non-null  float64\ndtypes: float64(3), int64(4), object(13)\nmemory usage: 4.6+ MB\n\n\nBased on the above data information and the dataset schema from City of Vancouver Open Data Portal - Street Trees, the columns can be preliminarily identified as four groups:\n\nTrees biological classifications and names, such as genus_name, species_name, common_name, cultivar_name\nTrees growth related characteristics, such as date_planted, diameter, height_range_id\nTrees coordinates and areas, such as latitude, longitude, neighbourhood_name\nOther specific location / orientation / identification information\n\nAs per the questions, the irrelevant columns under the fourth group and trees coordinates will be dropped. Also, to narrow down the focus to the highest level of tree classification, the columns of species_name, common_name, cultivar_name will also be dropped and only keep the column of genus_name.\n\n# Transform the column 'date_planted' to a datetime64 dtype and drop irrelevant columns\ntrees_df_dropped = pd.read_csv(URL,parse_dates = ['date_planted']\n                                    ).drop(columns=['std_street',\n                                                    'on_street',\n                                                    'street_side_name',\n                                                    'assigned',\n                                                    'civic_number',\n                                                    'plant_area',\n                                                    'curb',\n                                                    'on_street_block',\n                                                    'tree_id',\n                                                    'root_barrier',\n                                                    'latitude',\n                                                    'longitude',\n                                                    'species_name',\n                                                    'common_name',\n                                                    'cultivar_name'])\n\ntrees_df_dropped.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30000 entries, 0 to 29999\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   neighbourhood_name  30000 non-null  object        \n 1   date_planted        14085 non-null  datetime64[ns]\n 2   diameter            30000 non-null  float64       \n 3   genus_name          30000 non-null  object        \n 4   height_range_id     30000 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n\n\n\n\n\nBased on the above dateframe information, the column of date_planted has almost half of values missed. Since reviewing trees growth is one of the tasks in this analysis, any observations without the date of being planted will be considered as uninformative data and dropped from the original dataframe. In the EDA, an extra analysis has been performed and verified that dropping the observations with null values in date_planted will not cause the data representativeness issue.\n\n# Drop observations without value of date_planted\n\ntrees_df_dropped_dropna = trees_df_dropped.dropna(subset=['date_planted'])\n\ntrees_df_dropped_dropna.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 14085 entries, 1 to 29998\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   neighbourhood_name  14085 non-null  object        \n 1   date_planted        14085 non-null  datetime64[ns]\n 2   diameter            14085 non-null  float64       \n 3   genus_name          14085 non-null  object        \n 4   height_range_id     14085 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 660.2+ KB\n\n\n\n\n\n\n# Exam the remaining columns for more details\n\ntrees_df_dropped_dropna.describe(include='all',datetime_is_numeric=True)\n\n\n\n\n\n  \n    \n      \n      neighbourhood_name\n      date_planted\n      diameter\n      genus_name\n      height_range_id\n    \n  \n  \n    \n      count\n      14085\n      14085\n      14085.000000\n      14085\n      14085.000000\n    \n    \n      unique\n      22\n      NaN\n      NaN\n      68\n      NaN\n    \n    \n      top\n      Renfrew-Collingwood\n      NaN\n      NaN\n      ACER\n      NaN\n    \n    \n      freq\n      1323\n      NaN\n      NaN\n      3970\n      NaN\n    \n    \n      mean\n      NaN\n      2003-09-20 17:40:42.172523904\n      6.352586\n      NaN\n      1.822932\n    \n    \n      min\n      NaN\n      1989-10-27 00:00:00\n      0.000000\n      NaN\n      0.000000\n    \n    \n      25%\n      NaN\n      1997-12-15 00:00:00\n      3.000000\n      NaN\n      1.000000\n    \n    \n      50%\n      NaN\n      2003-04-01 00:00:00\n      5.000000\n      NaN\n      2.000000\n    \n    \n      75%\n      NaN\n      2009-11-13 00:00:00\n      8.000000\n      NaN\n      2.000000\n    \n    \n      max\n      NaN\n      2019-06-03 00:00:00\n      317.000000\n      NaN\n      9.000000\n    \n    \n      std\n      NaN\n      NaN\n      5.273737\n      NaN\n      0.983476\n    \n  \n\n\n\n\nIt has been noticed that the minimum of diameter and height_range_id are zero. For height_range_id, 0 represents the range of height is from 0 to 10 ft. However, since the diameter figure is the diameter of tree at breast height, it should not have the value of 0. So these observations will be considered as invalid data and removed from the df.\n\nindexs = trees_df_dropped_dropna[trees_df_dropped_dropna['diameter'] == 0].index\n\n# To simplify the object names afterwards, make a cope of df named as trees_df\ntrees_df = trees_df_dropped_dropna.copy()\n\ntrees_df.drop(indexs, inplace=True)\n\ntrees_df.describe(include='all',datetime_is_numeric=True)\n\n\n\n\n\n  \n    \n      \n      neighbourhood_name\n      date_planted\n      diameter\n      genus_name\n      height_range_id\n    \n  \n  \n    \n      count\n      14083\n      14083\n      14083.000000\n      14083\n      14083.000000\n    \n    \n      unique\n      22\n      NaN\n      NaN\n      68\n      NaN\n    \n    \n      top\n      Renfrew-Collingwood\n      NaN\n      NaN\n      ACER\n      NaN\n    \n    \n      freq\n      1323\n      NaN\n      NaN\n      3970\n      NaN\n    \n    \n      mean\n      NaN\n      2003-09-20 23:57:38.893701504\n      6.353489\n      NaN\n      1.822978\n    \n    \n      min\n      NaN\n      1989-10-27 00:00:00\n      0.500000\n      NaN\n      0.000000\n    \n    \n      25%\n      NaN\n      1997-12-15 12:00:00\n      3.000000\n      NaN\n      1.000000\n    \n    \n      50%\n      NaN\n      2003-04-01 00:00:00\n      5.000000\n      NaN\n      2.000000\n    \n    \n      75%\n      NaN\n      2009-11-13 00:00:00\n      8.000000\n      NaN\n      2.000000\n    \n    \n      max\n      NaN\n      2019-06-03 00:00:00\n      317.000000\n      NaN\n      9.000000\n    \n    \n      std\n      NaN\n      NaN\n      5.273568\n      NaN\n      0.983520\n    \n  \n\n\n\n\n\n\n\n\n\n\nThe cleaned target dataframe trees_df is composed of 5 columns. There are 68 distinct genera within the total of 14,083 trees (observations). According to City of Vancouver Open Data Portal - Street Trees where the dataset was originally obtained, the brief descriptions of columns are listed as below:\n\nCategorical columns\n\nneighbourhood_name: City’s defined local area in which the tree is located.\ngenus_name: Genus name of trees.\n\nQuantitative columns\n\ndiameter: DBH in inches (DBH stands for diameter of tree at breast height).\nheight_range_id: 0-10 for every 10 feet (e.g., 0 = 0-10 ft, 1 = 10-20 ft, 2 = 20-30 ft, and10 = 100+ ft).\n\nDatetime columns\n\ndate_planted: The date of planting.\n\n\nBiological diversity is one of the interests for the analysis. The quick guess for the answer would be Acer as Acer is a genus of trees commonly known as maples. A maple leaf is on the Canadian flag, and the maple has been chosen as a national symbol in Canada.\n\n# To visualize distrubition of genus for whole df\nplot_1_title = alt.TitleParams(\n    \"Figure 1 Number of street trees planted per genus\",\n     subtitle = \"Acer is the most popular genus of street trees\")\n\nplot_1_genus = alt.Chart(trees_df).mark_bar().encode(\n    alt.X('count():Q',title='Number of Trees'),\n    alt.Y('genus_name:N',title='Genus',sort='x')\n)\n\n# Add text annotation for the number of trees for each genus\ntext_1_genus = plot_1_genus.mark_text(align='left',dx=2).encode(text='count():Q')\n\nplot_1_genus = (plot_1_genus + text_1_genus).properties(title=plot_1_title,width=550)\n\nplot_1_genus\n\n\n\n\n\n\nFrom Figure 1, it has been confirmed that Acer is the most popular genus of trees based on the dataframe. The number of Acer trees is more than double of Prunus trees that have the second largest number in the Vancouver street trees. All the rest of the genera have less than 1,000 trees shown in the dataframe, and 47 out of the total 68 genera have less than 100 trees.\n\n\n\nThe dataframe has provide the specific date of being planted for each tree. From the available data, the age of current trees will be calculated first and then plot the number of trees planted by year. Since the accuracy is not a priority in this case, only the year of tree planted will be extracted for calculating the age till 2021.\n\n# Extract year of planted and calculate age of trees\ntrees_df_yr = trees_df.assign(year=trees_df['date_planted'].dt.year)\n\n# Calculate the age of trees till 2021.\ntrees_df_age = trees_df_yr.assign(age=(2021-trees_df_yr['year']))\n\n# Plot distribution of number of trees planted by year\n# It has been tested that if specifying the dtype of column 'year' as quantitative (Q), the x-axis label takes scientific\n# formatting, ie. 1,989. To avoid the \",\", the dtype of 'year' has been specified as nominal (N)\nplot_2_title = alt.TitleParams(\n    \"Figure 2 Number of street trees planted each year\",\n     subtitle = \"(Data available from 1989 to 2019)\")\n\nplot_2_year = alt.Chart(trees_df_age).mark_bar().encode(\n    alt.X('year:N',title=None),\n    alt.Y('count():Q',title='Number of trees planted')).properties(title=plot_2_title)\n\nplot_2_year\n\n\n\n\n\n\nFigure 2 indicates that there was a peak period between 1995 and 2013 to plant street trees by the City of Vancouver. During this period, the city had planted the highest number of trees in a single year in 1998 and in 2013. Before 1995 and after 2014, the number of trees planted was relatively lower, especially in 2016 when there were less than 50 new trees on public boulevards in Vancouver. Urban forestry is a systemic project. How many trees are planted is determined by a group of factors, such as public budget, tree replacement plan due to species distribution, insects, diseases, or environmental stress, etc. It is a clear message that the City of Vancouver has maintained a dynamic public trees planting program that benefits the wellbeings of residents in Vancouver.\nMoving further, to plot number of trees planted by year and add dropdown selection by genus.\n\n# Plot number of trees planted by year and add dropdown selection by genus\n\n# Specify the subtitle color and bold it to draw attention\nplot_3_title = alt.TitleParams(\n    \"Figure 3 Number of street trees planted each year by genus(from 1989 to 2019)\",\n    subtitle = \"Dropdown selection is available by genus\",\n    subtitleColor='steelblue', subtitleFontWeight='bold')\n\ngenus = sorted(trees_df_age['genus_name'].unique())\n\ndropdown_genus = alt.binding_select(name='Genus', options=genus)\n\nselect_genus = alt.selection_single(fields=['genus_name'], bind=dropdown_genus)\n\n# Since objective is to see number of trees for each genus, for y axis, specify stack=False\nplot_3_genus_year_bar = alt.Chart(trees_df_age).mark_bar().encode(\n    alt.X('year:N',title=None),\n    alt.Y('count():Q',stack=False,title='Number of trees planted per genus'),\n    alt.Color('genus_name:N',title='Genus name')\n).add_selection(select_genus).encode(\n    opacity=alt.condition(select_genus, alt.value(0.9), alt.value(0.0))).properties(title=plot_3_title)\n\nplot_3_genus_year_bar\n\n\n\n\n\n\nFigure 3 with dropdown selection has combined multiple information required in one plot and provide the audiences with convenience to efficiently explore number of trees planted from 1989 to 2019 for each genus.\nThe answer to the question 2 would be valuable reference for the public who are interested with tree planting history in Vancouver. In addition, for researchers, the answer will provide a first-hand insight into the urban forestry and street trees replacement planning.\n\n\n\nThe insights extracted from this question will not only confirm or verify some of the current scientific conclusions on trees growth patterns, but also provide accumulated evidences for improving the optimization of street trees in cities with similar ecological environment as Vancouver.\n\n# Calculate growth rate in diameter, growth rate in height and save them into df\n\ntrees_df_rate = trees_df_age.assign(rate_dia=trees_df_age['diameter']/trees_df_age['age'],\n                                    rate_height=trees_df_age['height_range_id']/trees_df_age['age']\n                                   ).round(2)\n\n# Plot two growth rates using scatter chart\n# Add zooming and panning to solve the issue that the plot is saturated\n# Add dropdown widget to select genus\nplot_4_title = alt.TitleParams(\n    \"Figure 4 Trees growth rate in diameter & height per genus\",\n    subtitle = [\"Dropdown selection is available by genus\", \"Zooming and Panning are available\"],\n    subtitleColor='steelblue', subtitleFontWeight='bold')\n\nplot_growth_rates = alt.Chart(trees_df_rate).mark_point(size=5).encode(\n    alt.X('rate_dia:Q',title=\"Growth rate in diameter (inch/yr)\"),\n    alt.Y('rate_height:Q',title=\"Growth rate in height (unit/yr)\")\n).interactive()\n\nplot_4_growth_rates_genus = plot_growth_rates.add_selection(select_genus).encode(\n    opacity=alt.condition(select_genus, alt.value(0.9), alt.value(0.0))).properties(title=plot_4_title,width=600)\n\nplot_4_growth_rates_genus\n\n\n\n\n\n\nNote that the growth rate in height is represented in unit/yr. Here the “unit” refers to height_range_id: 0-10 for every 10 feet (e.g., 0 = 0-10 ft, 1 = 10-20 ft, 2 = 20-30 ft, and10 = 100+ ft)\nBy adding zooming, panning and dropdown widget selection features, audiences can effectively check the two growth rates based on their interested genus.\nIt must be noted that in the original dataframe, the height of trees was collected as height_range_id. Audiences should be aware that the calculated average growth rate by height is not an absolute value of height but a processed relative variable. Typically, if the quantity of observations for a particular genus is sufficient (deep research is required to define the “sufficiency”), the result will have more accuracy from the statistical perspective. But if the genus has limited observations in the dataframe, the audiences should be more careful to assess the results. Again, deep research is required in future analysis.\nIn order to effectively compare the growth rate for all genera, except for a bar chart that just shows the mean value, a boxplot is also created to include a few key summary statistics and visualize data distribution. This combination will provide more information to audiences particularly for researchers who may care about the statistics more than a mean value.\n\n# Plot average growth rates in diameter per genus\n\nplot_5_title = alt.TitleParams(\n    \"Figure 5 Trees average growth rate in diameter per genus\",\n    subtitle = \"TSUGA, PSEUDOTSUGA & POPULUS are top three fastest growing genera\")\n\nplot_ave_growth_rate_dia_bar = alt.Chart(trees_df_rate).mark_bar().encode(\n    alt.X('mean(rate_dia)', title='Average growth rate in diameter (inch/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort='x'),\n    alt.Color('count()',scale=alt.Scale(scheme='blues'),title='Number of trees')\n).properties(width=130)\n\n# Sort boxplot by average growth rate in diameter to keep consistent with bar chart\nave_growth_rate_dia_order = trees_df_rate.groupby('genus_name')['rate_dia'].mean().sort_values().index.tolist()\n\n# To distinguish the boxplot from the bar chart, specify the color as green\nplot_ave_growth_rate_dia_box = alt.Chart(trees_df_rate).mark_boxplot(color='green').encode(\n    alt.X('rate_dia:Q', title='Growth rate in diameter (inch/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort=ave_growth_rate_dia_order)\n).properties(width=310)\n\n# Modify the domain of the axis’ scale for boxplot to make the plot more readable\n# At the same time, discard outliers with the value more than 4 inch/year\nplot_ave_growth_rate_dia_box_scale = plot_ave_growth_rate_dia_box.encode(alt.X('rate_dia:Q', \n                                                                               title='Growth rate in diameter (inch/yr)',\n                                                                               scale=alt.Scale(domain=[0, 4],clamp=True)))\n\n# Use top-level title configuration to locate title in the middle\nplot_5_ave_growth_rates_dia = (plot_ave_growth_rate_dia_bar | plot_ave_growth_rate_dia_box_scale\n                              ).properties(title=plot_5_title).configure_title(anchor='middle')\n\nplot_5_ave_growth_rates_dia\n\n\n\n\n\n\n\n# Plot average growth rates in height per genus\n\nplot_6_title = alt.TitleParams(\n    \"Figure 6 Trees average growth rate in height per genus\",\n    subtitle = \"TSUGA, POPULUS & PSEUDOTSUGA are top three fastest growing genera\")\n\n# To distinguish the plot from the above growth rates in diameter chart, change alternative color scheme\nplot_ave_growth_rate_height_bar = alt.Chart(trees_df_rate).mark_bar().encode(\n    alt.X('mean(rate_height)', title='Average growth rate in height (unit/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort='x'),\n    alt.Color('count()',scale=alt.Scale(scheme='teals'),title='Number of trees')\n).properties(width=130)\n\n# Sort boxplot by average growth rate in height to keep consistent with bar chart\nave_growth_rate_height_order = trees_df_rate.groupby('genus_name')['rate_height'].mean().sort_values().index.tolist()\n\n# To distinguish the boxplot from the bar chart, specify the color as grey\nplot_ave_growth_rate_height_box = alt.Chart(trees_df_rate).mark_boxplot(color='grey').encode(\n    alt.X('rate_height:Q', title='Growth rate in height (unit/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort=ave_growth_rate_height_order)\n).properties(width=310)\n\n# Modify the domain of the axis’ scale for boxplot to make the plot more readable\n# At the same time, discard outliers with the value more than 0.6 unit/year\nplot_ave_growth_rate_height_box_scale = plot_ave_growth_rate_height_box.encode(alt.X('rate_height:Q', \n                                                                               title='Growth rate in height (unit/yr)',\n                                                                               scale=alt.Scale(domain=[0, 0.6],clamp=True)))\n\n# Use top-level title configuration to locate title in the middle\nplot_6_ave_growth_rates_height = (plot_ave_growth_rate_height_bar | plot_ave_growth_rate_height_box_scale\n                                 ).properties(title=plot_6_title).configure_title(anchor='middle')\n\nplot_6_ave_growth_rates_height\n\n\n\n\n\n\nNote that the growth rate in height is represented in unit/yr. Here the “unit” refers to height_range_id: 0-10 for every 10 feet (e.g., 0 = 0-10 ft, 1 = 10-20 ft, 2 = 20-30 ft, and10 = 100+ ft)\nBased on Figure 5 and 6, TSUGA, POPULUS and PSEUDOTSUGA are identified as the top three fastest growing genera. But as indicated on the plots and Figure 1, the sample size of the three genera is very limited. The number of trees (observations) of TSUGA, POPULUS and PSEUDOTSUGA in the dataframe are 2, 3, and 8, respectively. Since the sample size may affect the confidence level when analyzing and comparing the growth rates among genera, a futher statistical analysis should be considered in the future to justify the conclusion in this case.\n\n\n\nThe last question might be interesting for residents in Vancouver to know or for potential home buyers to select their desired neighbourhood with considerations of trees distribution and some characters.\n\n# To make a base map of Vancouver by using the geojson url saved in url_geojson\n\nurl_geojson = 'https://raw.githubusercontent.com/UBC-MDS/exploratory-data-viz/main/data/local-area-boundary.geojson'\n\n# Format it in a Topo json format using alt.Data()\n\ndata_geojson_remote = alt.Data(url=url_geojson, format=alt.DataFormat(property='features',type='json'))\n\n# Then make base Vancouver Altair map using the data_geojson_remote object\n# Use an identity type and need to reflectY=True. Without this second argument the map of Vancouver is upside down\n\nbase_map = alt.Chart(data_geojson_remote).mark_geoshape(\n    color='white',stroke=\"grey\").encode(\n).project(type='identity', reflectY=True)\n\nPerform calculation needed and create a new dataframe.\n\n# Rename neighbourhood_name to name since that's what's it's called in the geojson url\ntrees_nbhd_num_df = pd.DataFrame(trees_df_age['neighbourhood_name'].value_counts()\n                            ).reset_index().rename(columns={'index':'name','neighbourhood_name':'trees_number'})\n\ntrees_nbhd_age_df = trees_df_age.groupby('neighbourhood_name').mean().round(1).reset_index(\n).rename(columns={'neighbourhood_name':'name','diameter':'ave_diameter','age':'ave_age'})[['name','ave_diameter','ave_age']]\n\ntrees_nbhd_df = trees_nbhd_num_df.merge(trees_nbhd_age_df, left_on='name', right_on='name')\n\ntrees_nbhd_df\n\n\n\n\n\n  \n    \n      \n      name\n      trees_number\n      ave_diameter\n      ave_age\n    \n  \n  \n    \n      0\n      Renfrew-Collingwood\n      1323\n      5.6\n      17.5\n    \n    \n      1\n      Hastings-Sunrise\n      1285\n      8.1\n      19.7\n    \n    \n      2\n      Kensington-Cedar Cottage\n      1169\n      6.5\n      18.7\n    \n    \n      3\n      Sunset\n      936\n      6.0\n      17.3\n    \n    \n      4\n      Victoria-Fraserview\n      908\n      5.5\n      17.7\n    \n    \n      5\n      Dunbar-Southlands\n      762\n      6.1\n      16.7\n    \n    \n      6\n      Marpole\n      689\n      5.6\n      17.2\n    \n    \n      7\n      Riley Park\n      684\n      5.8\n      17.2\n    \n    \n      8\n      Grandview-Woodland\n      632\n      6.6\n      18.0\n    \n    \n      9\n      Killarney\n      615\n      6.2\n      17.7\n    \n    \n      10\n      Kerrisdale\n      612\n      6.0\n      16.6\n    \n    \n      11\n      Oakridge\n      549\n      6.1\n      17.9\n    \n    \n      12\n      Arbutus-Ridge\n      535\n      6.8\n      17.5\n    \n    \n      13\n      Kitsilano\n      526\n      7.2\n      16.8\n    \n    \n      14\n      Mount Pleasant\n      513\n      7.9\n      18.7\n    \n    \n      15\n      Shaughnessy\n      483\n      5.1\n      16.6\n    \n    \n      16\n      West Point Grey\n      423\n      6.5\n      17.7\n    \n    \n      17\n      Downtown\n      393\n      5.8\n      16.4\n    \n    \n      18\n      South Cambie\n      362\n      6.5\n      18.3\n    \n    \n      19\n      Fairview\n      248\n      6.8\n      16.9\n    \n    \n      20\n      Strathcona\n      225\n      6.2\n      16.3\n    \n    \n      21\n      West End\n      211\n      7.2\n      18.0\n    \n  \n\n\n\n\n\n# Add tooltips and hover selection on map\n\nhover = alt.selection_single(fields=['name'], on='mouseover')\n\nplot_trees_nbhd = alt.Chart(data_geojson_remote).mark_geoshape().transform_lookup(\n    lookup='properties.name',\n    from_=alt.LookupData(trees_nbhd_df, 'name', ['trees_number','name'])).encode(\n    alt.Color('trees_number:Q',title='Number of trees'),\n    opacity=alt.condition(hover, alt.value(1), alt.value(0.1)),\n    tooltip=['name:N',alt.Tooltip('trees_number:Q', title='Number of trees')]\n).add_selection(hover).project(type='identity', reflectY=True)\n\nplot_trees_nbhd_map = base_map + plot_trees_nbhd\n\n# Plot ave_age per ave_diameter\n# Scale the x and y axis by specifying zero=False\n\nplot_age_dia = alt.Chart(trees_nbhd_df).mark_point().encode(\n    alt.X('ave_diameter:Q',scale=alt.Scale(zero=False),title='Average diameter (inch)'),\n    alt.Y('ave_age:Q',scale=alt.Scale(zero=False),title='Average age (yr)'),\n    stroke=alt.condition(hover, alt.value('black'), alt.value('#ffffff')))\n\nplot_7_title = alt.TitleParams(\n    \"Figure 7 Trees distribution per neighbourhood (number/age/diameter)\",\n    subtitle = [\"Hover selection on map is available by neighbourhood\", \"Tooltips are available on map\"],\n    subtitleColor='steelblue', subtitleFontWeight='bold')\n\nplot_7_nbhd_map = (plot_trees_nbhd_map & plot_age_dia\n                  ).properties(title=plot_7_title).configure_title(anchor='middle')\n\nplot_7_nbhd_map\n\n\n\n\n\n\nBased on the above trees_nbhd_df and Figure 7, the average age of trees in these neighbourhoods are between 16.3 years old and 19.7 years old. And the average diameter of trees are from 5.1 inch to 8.1 inch.\nThrough the link of map selection with the scatter plot of Figure 7, it is effective and efficient for audiences to hover over their target neighbourhood on map to quickly obtain the number of trees in the neighbourhood, know how old and how big the trees there on average.\n\n\n\n\n\nThe analysis intends to discover an overall view of Vancouver street trees for their biological diversity, growth and distributions within neighbourhoods. Through Figure 1 to Figure 7 and related tables, all four pre-defined questions have been answered. It has been confirmed that within 68 genera and a total of 14,083 trees, Acer is the most popular genus based on the dataset. It is not surprised as the maple trees are Canada’s official arboreal emblem. Historical data shows Vancouver has maintained a dynamic public trees planting program. Especially between 1995 and 2013, a large number of trees had been added to the streets by the government. The plots generated can effectively provide audiences with more details about the yearly number of trees planted by genus, the growth rates by genus, the trees distribution and certain characters by neighbourhood. In addition, by comparing the growth rates within 68 genera, it is noted that TSUGA, POPULUS and PSEUDOTSUGA are the top three fastest growing genera in Vancouver.\nExcept for the four questions that have been discussed, further questions could be considered in the future:\n\nWhat is the specific genus distribution as per neighbourhood. This could be demonstrated on the Vancouver map, too. The answer will be informative for residents to know the distribution of fast-growing trees or shade trees in their area.\nWhat is the average height range of trees in each neighbourhood. Some house buyers may prefer taller trees or shade trees in their neighbourhood.\nSince genus is the highest level of tree classification in the df, data analyst may extend the analysis into the next level, species, or common names, or cultivar_names to continuously explore the data patterns.\n\nIn the analysis report, the assumption and limitations of the analysis should be highlighted for audiences to pay attention, including:\n\nThe original data only includes the public trees on boulevards in the City of Vancouver. Park trees and private trees are not included.\nMore than half of the original observations from the original dataset have been dropped since lacking information on the date of the tree being planted. Therefore, the analysis is based on the remaining observations only. This may limit the conclusions or insights extracted from this analysis.\nIn order to simplify the analysis, the calculation of the age of trees has not included the specific month and day.\nThe information related to the height of trees was collected as height_range_id. So audiences should be aware that the height figures in this report are not the absolute measurement of height.\nDue to the limited sample size, more research is required to verify the conclusion of the fastest growing trees in this report.\n\nThe last important notes: 1. The top priority of this analysis is emphasizing on the demonstration of learning outcomes from the Data Vis perspectives. The analyst (author) recognized in this report the narrative should be more cohesive, the original dataset column analysis and data wrangling should be supported with more domain expertise if judged from a higher level data analysis standard. 2. More statistical tools could be applied in the analysis to make the report more convincing. This will be considered for future work.\n\n\n\n\n# Resize plots to accommodate the combined panel\n# Create new object to avoid overwriting previous plots\nplot_1 = plot_1_genus.properties(height=1100,width=250)\nplot_2 = plot_2_year.properties(height=300,width=350)\nplot_3 = plot_3_genus_year_bar.properties(height=300,width=350)\nplot_4 = plot_4_growth_rates_genus.properties(height=300,width=350)\n\nplot_1 | (plot_2 & plot_3 & plot_4)\n\n\n\n\n\n\n\n\n\n\n# Since objects with \"config\" attribute cannot be used within VConcatChart, remove the previous \"config\"\n# Create new object to avoid overwriting previous plots\nplot_5 = (plot_ave_growth_rate_dia_bar | plot_ave_growth_rate_dia_box_scale).properties(title=plot_5_title)\nplot_6 = (plot_ave_growth_rate_height_bar | plot_ave_growth_rate_height_box_scale).properties(title=plot_6_title)\nplot_7 = (plot_trees_nbhd_map & plot_age_dia).properties(title=plot_7_title)\n\n(plot_5 & plot_6) & plot_7\n\n\n\n\n\n\n\n\n\n\n\nNot all the work in this project is original. The following resources have been used as references.\n\nData Source\n\nThe Vancouver street trees dataset provided by UBC Data Visualization course. The dataset schema from City of Vancouver Open Data Portal - Street Trees.\n\nSample Report\n\nThe sample reports, Evolution of LEGO EDA report and analysis report provided by UBC Data Visualization course.\n\nReport Writing\n\nMarkdown formatting for the text cells in JupyterNotebook, Basic Syntax - The Markdown elements outlined in John Gruber’s design document, retrieved on November 1, 2021 from Markdown Guide.\n\nPython and Data Vis Techniques\n\nLearned from UBC Data Visualization course, Programming in Python for Data Science course, Piazza group discussions, Altair Documentation, and other self-learning resources from the internet."
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis_delete.html",
    "href": "posts/vancouver-street-trees/trees-analysis_delete.html",
    "title": "Vancouver Street Trees",
    "section": "",
    "text": "import pandas as pd\nimport altair as alt\n\ntrees = pd.read_csv('data/vancouver_trees.csv')\n\ntrees.head()\n\n\n\n\n\n  \n    \n      \n      std_street\n      on_street\n      species_name\n      neighbourhood_name\n      date_planted\n      diameter\n      street_side_name\n      genus_name\n      assigned\n      civic_number\n      plant_area\n      curb\n      tree_id\n      common_name\n      height_range_id\n      on_street_block\n      cultivar_name\n      root_barrier\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      W 13TH AV\n      MAPLE ST\n      PSEUDOPLATANUS\n      Kitsilano\n      NaN\n      9.00\n      EVEN\n      ACER\n      N\n      1996\n      10\n      Y\n      13310\n      SYCAMORE MAPLE\n      4\n      2900\n      NaN\n      N\n      49.259856\n      -123.150586\n    \n    \n      1\n      WALES ST\n      WALES ST\n      PLATANOIDES\n      Renfrew-Collingwood\n      2018-11-28\n      3.00\n      ODD\n      ACER\n      N\n      5291\n      7\n      Y\n      259084\n      PRINCETON GOLD MAPLE\n      1\n      5200\n      PRINCETON GOLD\n      N\n      49.236650\n      -123.051831\n    \n    \n      2\n      W BROADWAY\n      W BROADWAY\n      RUBRUM\n      Kitsilano\n      1996-04-19\n      14.00\n      EVEN\n      ACER\n      N\n      3618\n      C\n      Y\n      167986\n      KARPICK RED MAPLE\n      3\n      3600\n      KARPICK\n      N\n      49.264250\n      -123.184020\n    \n    \n      3\n      PENTICTON ST\n      PENTICTON ST\n      CALLERYANA\n      Renfrew-Collingwood\n      2006-03-06\n      3.75\n      EVEN\n      PYRUS\n      N\n      2502\n      5\n      Y\n      213386\n      CHANTICLEER PEAR\n      1\n      2500\n      CHANTICLEER\n      Y\n      49.261036\n      -123.052921\n    \n    \n      4\n      RHODES ST\n      RHODES ST\n      GLYPTOSTROBOIDES\n      Renfrew-Collingwood\n      2001-11-01\n      3.00\n      ODD\n      METASEQUOIA\n      N\n      5639\n      N\n      Y\n      189223\n      DAWN REDWOOD\n      2\n      5600\n      NaN\n      N\n      49.233354\n      -123.050249"
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis.html#data-imports-and-wrangling",
    "href": "posts/vancouver-street-trees/trees-analysis.html#data-imports-and-wrangling",
    "title": "Vancouver Street Trees",
    "section": "DATA IMPORTS AND WRANGLING",
    "text": "DATA IMPORTS AND WRANGLING\n\nData imports\n\n# Import libraries needed for this assignment\n\nimport altair as alt\nimport pandas as pd\nimport json\n\nalt.data_transformers.enable(\"data_server\")\nalt.renderers.enable('default')\n\n# Use data sever url method to read data\n# URL = \"https://raw.githubusercontent.com/UBC-MDS/exploratory-data-viz/main/data/vancouver_trees.csv\"\ntrees_df_original = pd.read_csv('data/vancouver_trees.csv')\n\n# Glance at the original df\ntrees_df_original\n\n\n\n\n\n  \n    \n      \n      std_street\n      on_street\n      species_name\n      neighbourhood_name\n      date_planted\n      diameter\n      street_side_name\n      genus_name\n      assigned\n      civic_number\n      plant_area\n      curb\n      tree_id\n      common_name\n      height_range_id\n      on_street_block\n      cultivar_name\n      root_barrier\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      W 13TH AV\n      MAPLE ST\n      PSEUDOPLATANUS\n      Kitsilano\n      NaN\n      9.00\n      EVEN\n      ACER\n      N\n      1996\n      10\n      Y\n      13310\n      SYCAMORE MAPLE\n      4\n      2900\n      NaN\n      N\n      49.259856\n      -123.150586\n    \n    \n      1\n      WALES ST\n      WALES ST\n      PLATANOIDES\n      Renfrew-Collingwood\n      2018-11-28\n      3.00\n      ODD\n      ACER\n      N\n      5291\n      7\n      Y\n      259084\n      PRINCETON GOLD MAPLE\n      1\n      5200\n      PRINCETON GOLD\n      N\n      49.236650\n      -123.051831\n    \n    \n      2\n      W BROADWAY\n      W BROADWAY\n      RUBRUM\n      Kitsilano\n      1996-04-19\n      14.00\n      EVEN\n      ACER\n      N\n      3618\n      C\n      Y\n      167986\n      KARPICK RED MAPLE\n      3\n      3600\n      KARPICK\n      N\n      49.264250\n      -123.184020\n    \n    \n      3\n      PENTICTON ST\n      PENTICTON ST\n      CALLERYANA\n      Renfrew-Collingwood\n      2006-03-06\n      3.75\n      EVEN\n      PYRUS\n      N\n      2502\n      5\n      Y\n      213386\n      CHANTICLEER PEAR\n      1\n      2500\n      CHANTICLEER\n      Y\n      49.261036\n      -123.052921\n    \n    \n      4\n      RHODES ST\n      RHODES ST\n      GLYPTOSTROBOIDES\n      Renfrew-Collingwood\n      2001-11-01\n      3.00\n      ODD\n      METASEQUOIA\n      N\n      5639\n      N\n      Y\n      189223\n      DAWN REDWOOD\n      2\n      5600\n      NaN\n      N\n      49.233354\n      -123.050249\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      29995\n      ROBSON ST\n      ROBSON ST\n      CAMPESTRE\n      West End\n      NaN\n      7.00\n      ODD\n      ACER\n      N\n      1015\n      c\n      Y\n      122814\n      HEDGE MAPLE\n      2\n      1000\n      NaN\n      N\n      49.283666\n      -123.123231\n    \n    \n      29996\n      OSLER ST\n      CONNAUGHT DRIVE\n      PLATANOIDES\n      Shaughnessy\n      2007-04-16\n      8.00\n      ODD\n      ACER\n      N\n      4690\n      10\n      Y\n      132211\n      NORWAY MAPLE\n      1\n      1000\n      NaN\n      Y\n      49.243636\n      -123.129480\n    \n    \n      29997\n      BEATRICE ST\n      BEATRICE ST\n      CERASIFERA\n      Victoria-Fraserview\n      NaN\n      17.30\n      EVEN\n      PRUNUS\n      N\n      6218\n      9\n      Y\n      59355\n      PISSARD PLUM\n      3\n      6200\n      ATROPURPUREUM\n      N\n      49.227406\n      -123.066936\n    \n    \n      29998\n      ANGUS DRIVE\n      ANGUS DRIVE\n      BILOBA\n      Shaughnessy\n      2006-02-17\n      4.00\n      ODD\n      GINKGO\n      N\n      1551\n      9\n      Y\n      207753\n      GINKGO OR MAIDENHAIR TREE\n      1\n      1500\n      NaN\n      Y\n      49.254431\n      -123.140382\n    \n    \n      29999\n      MAIN ST\n      MAIN ST\n      EUCHLORA   X\n      Riley Park\n      NaN\n      12.00\n      ODD\n      TILIA\n      N\n      4323\n      C\n      Y\n      92997\n      CRIMEAN LINDEN\n      4\n      4300\n      NaN\n      N\n      49.246969\n      -123.101328\n    \n  \n\n30000 rows × 20 columns\n\n\n\n\n\n\nIdentify and drop irrelevant columns\n\n# Check columns of the original df\n\ntrees_df_original.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30000 entries, 0 to 29999\nData columns (total 20 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   std_street          30000 non-null  object \n 1   on_street           30000 non-null  object \n 2   species_name        30000 non-null  object \n 3   neighbourhood_name  30000 non-null  object \n 4   date_planted        14085 non-null  object \n 5   diameter            30000 non-null  float64\n 6   street_side_name    30000 non-null  object \n 7   genus_name          30000 non-null  object \n 8   assigned            30000 non-null  object \n 9   civic_number        30000 non-null  int64  \n 10  plant_area          29722 non-null  object \n 11  curb                30000 non-null  object \n 12  tree_id             30000 non-null  int64  \n 13  common_name         30000 non-null  object \n 14  height_range_id     30000 non-null  int64  \n 15  on_street_block     30000 non-null  int64  \n 16  cultivar_name       16178 non-null  object \n 17  root_barrier        30000 non-null  object \n 18  latitude            30000 non-null  float64\n 19  longitude           30000 non-null  float64\ndtypes: float64(3), int64(4), object(13)\nmemory usage: 4.6+ MB\n\n\nBased on the above data information and the dataset schema from City of Vancouver Open Data Portal - Street Trees, the columns can be preliminarily identified as four groups:\n\nTrees biological classifications and names, such as genus_name, species_name, common_name, cultivar_name\nTrees growth related characteristics, such as date_planted, diameter, height_range_id\nTrees coordinates and areas, such as latitude, longitude, neighbourhood_name\nOther specific location / orientation / identification information\n\nAs per the questions, the irrelevant columns under the fourth group and trees coordinates will be dropped. Also, to narrow down the focus to the highest level of tree classification, the columns of species_name, common_name, cultivar_name will also be dropped and only keep the column of genus_name.\n\n# Transform the column 'date_planted' to a datetime64 dtype and drop irrelevant columns\ntrees_df_dropped = pd.read_csv('data/vancouver_trees.csv', parse_dates = ['date_planted']\n                                    ).drop(columns=['std_street',\n                                                    'on_street',\n                                                    'street_side_name',\n                                                    'assigned',\n                                                    'civic_number',\n                                                    'plant_area',\n                                                    'curb',\n                                                    'on_street_block',\n                                                    'tree_id',\n                                                    'root_barrier',\n                                                    'latitude',\n                                                    'longitude',\n                                                    'species_name',\n                                                    'common_name',\n                                                    'cultivar_name'])\n\ntrees_df_dropped.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30000 entries, 0 to 29999\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   neighbourhood_name  30000 non-null  object        \n 1   date_planted        14085 non-null  datetime64[ns]\n 2   diameter            30000 non-null  float64       \n 3   genus_name          30000 non-null  object        \n 4   height_range_id     30000 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n\n\n\n\nExam null values\nBased on the above dateframe information, the column of date_planted has almost half of values missed. Since reviewing trees growth is one of the tasks in this analysis, any observations without the date of being planted will be considered as uninformative data and dropped from the original dataframe. In the EDA, an extra analysis has been performed and verified that dropping the observations with null values in date_planted will not cause the data representativeness issue.\n\n# Drop observations without value of date_planted\n\ntrees_df_dropped_dropna = trees_df_dropped.dropna(subset=['date_planted'])\n\ntrees_df_dropped_dropna.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 14085 entries, 1 to 29998\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   neighbourhood_name  14085 non-null  object        \n 1   date_planted        14085 non-null  datetime64[ns]\n 2   diameter            14085 non-null  float64       \n 3   genus_name          14085 non-null  object        \n 4   height_range_id     14085 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 660.2+ KB\n\n\n\n\nFurther exam for error data\n\n# Exam the remaining columns for more details\n\ntrees_df_dropped_dropna.describe(include='all',datetime_is_numeric=True)\n\n\n\n\n\n  \n    \n      \n      neighbourhood_name\n      date_planted\n      diameter\n      genus_name\n      height_range_id\n    \n  \n  \n    \n      count\n      14085\n      14085\n      14085.000000\n      14085\n      14085.000000\n    \n    \n      unique\n      22\n      NaN\n      NaN\n      68\n      NaN\n    \n    \n      top\n      Renfrew-Collingwood\n      NaN\n      NaN\n      ACER\n      NaN\n    \n    \n      freq\n      1323\n      NaN\n      NaN\n      3970\n      NaN\n    \n    \n      mean\n      NaN\n      2003-09-20 17:40:42.172523904\n      6.352586\n      NaN\n      1.822932\n    \n    \n      min\n      NaN\n      1989-10-27 00:00:00\n      0.000000\n      NaN\n      0.000000\n    \n    \n      25%\n      NaN\n      1997-12-15 00:00:00\n      3.000000\n      NaN\n      1.000000\n    \n    \n      50%\n      NaN\n      2003-04-01 00:00:00\n      5.000000\n      NaN\n      2.000000\n    \n    \n      75%\n      NaN\n      2009-11-13 00:00:00\n      8.000000\n      NaN\n      2.000000\n    \n    \n      max\n      NaN\n      2019-06-03 00:00:00\n      317.000000\n      NaN\n      9.000000\n    \n    \n      std\n      NaN\n      NaN\n      5.273737\n      NaN\n      0.983476\n    \n  \n\n\n\n\n\nIt has been noticed that the minimum of diameter and height_range_id are zero. For height_range_id, 0 represents the range of height is from 0 to 10 ft. However, since the diameter figure is the diameter of tree at breast height, it should not have the value of 0. So these observations will be considered as invalid data and removed from the df.\n\nindexs = trees_df_dropped_dropna[trees_df_dropped_dropna['diameter'] == 0].index\n\n# To simplify the object names afterwards, make a cope of df named as trees_df\ntrees_df = trees_df_dropped_dropna.copy()\n\ntrees_df.drop(indexs, inplace=True)\n\ntrees_df.describe(include='all',datetime_is_numeric=True)\n\n\n\n\n\n  \n    \n      \n      neighbourhood_name\n      date_planted\n      diameter\n      genus_name\n      height_range_id\n    \n  \n  \n    \n      count\n      14083\n      14083\n      14083.000000\n      14083\n      14083.000000\n    \n    \n      unique\n      22\n      NaN\n      NaN\n      68\n      NaN\n    \n    \n      top\n      Renfrew-Collingwood\n      NaN\n      NaN\n      ACER\n      NaN\n    \n    \n      freq\n      1323\n      NaN\n      NaN\n      3970\n      NaN\n    \n    \n      mean\n      NaN\n      2003-09-20 23:57:38.893701504\n      6.353489\n      NaN\n      1.822978\n    \n    \n      min\n      NaN\n      1989-10-27 00:00:00\n      0.500000\n      NaN\n      0.000000\n    \n    \n      25%\n      NaN\n      1997-12-15 12:00:00\n      3.000000\n      NaN\n      1.000000\n    \n    \n      50%\n      NaN\n      2003-04-01 00:00:00\n      5.000000\n      NaN\n      2.000000\n    \n    \n      75%\n      NaN\n      2009-11-13 00:00:00\n      8.000000\n      NaN\n      2.000000\n    \n    \n      max\n      NaN\n      2019-06-03 00:00:00\n      317.000000\n      NaN\n      9.000000\n    \n    \n      std\n      NaN\n      NaN\n      5.273568\n      NaN\n      0.983520"
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis.html#analysis",
    "href": "posts/vancouver-street-trees/trees-analysis.html#analysis",
    "title": "Vancouver Street Trees",
    "section": "ANALYSIS",
    "text": "ANALYSIS\n\nDataset Description\nThe cleaned target dataframe trees_df is composed of 5 columns. There are 68 distinct genera within the total of 14,083 trees (observations). According to City of Vancouver Open Data Portal - Street Trees where the dataset was originally obtained, the brief descriptions of columns are listed as below:\n\nCategorical columns\n\nneighbourhood_name: City’s defined local area in which the tree is located.\ngenus_name: Genus name of trees.\n\nQuantitative columns\n\ndiameter: DBH in inches (DBH stands for diameter of tree at breast height).\nheight_range_id: 0-10 for every 10 feet (e.g., 0 = 0-10 ft, 1 = 10-20 ft, 2 = 20-30 ft, and10 = 100+ ft).\n\nDatetime columns\n\ndate_planted: The date of planting.\n\nQ1 - Based on distribution of street trees planted in Vancouver by genus, which genus is the most popular one?\nBiological diversity is one of the interests for the analysis. The quick guess for the answer would be Acer as Acer is a genus of trees commonly known as maples. A maple leaf is on the Canadian flag, and the maple has been chosen as a national symbol in Canada.\n\n# To visualize distrubition of genus for whole df\nplot_1_title = alt.TitleParams(\n    \"Figure 1 Number of street trees planted per genus\",\n     subtitle = \"Acer is the most popular genus of street trees\")\n\nplot_1_genus = alt.Chart(trees_df).mark_bar().encode(\n    alt.X('count():Q',title='Number of Trees'),\n    alt.Y('genus_name:N',title='Genus',sort='x')\n)\n\n# Add text annotation for the number of trees for each genus\ntext_1_genus = plot_1_genus.mark_text(align='left',dx=2).encode(text='count():Q')\n\nplot_1_genus = (plot_1_genus + text_1_genus).properties(title=plot_1_title,width=550)\n\nplot_1_genus\n\n\n\n\n\n\nFrom Figure 1, it has been confirmed that Acer is the most popular genus of trees based on the dataframe. The number of Acer trees is more than double of Prunus trees that have the second largest number in the Vancouver street trees. All the rest of the genera have less than 1,000 trees shown in the dataframe, and 47 out of the total 68 genera have less than 100 trees.\n\n\nQ2 - In the past 30 years, how many trees have been planted every single year? And what is the number of trees planted by genus each year?\nThe dataframe has provide the specific date of being planted for each tree. From the available data, the age of current trees will be calculated first and then plot the number of trees planted by year. Since the accuracy is not a priority in this case, only the year of tree planted will be extracted for calculating the age till 2021.\n\n# Extract year of planted and calculate age of trees\ntrees_df_yr = trees_df.assign(year=trees_df['date_planted'].dt.year)\n\n# Calculate the age of trees till 2021.\ntrees_df_age = trees_df_yr.assign(age=(2021-trees_df_yr['year']))\n\n# Plot distribution of number of trees planted by year\n# It has been tested that if specifying the dtype of column 'year' as quantitative (Q), the x-axis label takes scientific\n# formatting, ie. 1,989. To avoid the \",\", the dtype of 'year' has been specified as nominal (N)\nplot_2_title = alt.TitleParams(\n    \"Figure 2 Number of street trees planted each year\",\n     subtitle = \"(Data available from 1989 to 2019)\")\n\nplot_2_year = alt.Chart(trees_df_age).mark_bar().encode(\n    alt.X('year:N',title=None),\n    alt.Y('count():Q',title='Number of trees planted')).properties(title=plot_2_title)\n\nplot_2_year\n\n\n\n\n\n\nFigure 2 indicates that there was a peak period between 1995 and 2013 to plant street trees by the City of Vancouver. During this period, the city had planted the highest number of trees in a single year in 1998 and in 2013. Before 1995 and after 2014, the number of trees planted was relatively lower, especially in 2016 when there were less than 50 new trees on public boulevards in Vancouver. Urban forestry is a systemic project. How many trees are planted is determined by a group of factors, such as public budget, tree replacement plan due to species distribution, insects, diseases, or environmental stress, etc. It is a clear message that the City of Vancouver has maintained a dynamic public trees planting program that benefits the wellbeings of residents in Vancouver.\nMoving further, to plot number of trees planted by year and add dropdown selection by genus.\n\n# Plot number of trees planted by year and add dropdown selection by genus\n\n# Specify the subtitle color and bold it to draw attention\nplot_3_title = alt.TitleParams(\n    \"Figure 3 Number of street trees planted each year by genus(from 1989 to 2019)\",\n    subtitle = \"Dropdown selection is available by genus\",\n    subtitleColor='steelblue', subtitleFontWeight='bold')\n\ngenus = sorted(trees_df_age['genus_name'].unique())\n\ndropdown_genus = alt.binding_select(name='Genus', options=genus)\n\nselect_genus = alt.selection_single(fields=['genus_name'], bind=dropdown_genus)\n\n# Since objective is to see number of trees for each genus, for y axis, specify stack=False\nplot_3_genus_year_bar = alt.Chart(trees_df_age).mark_bar().encode(\n    alt.X('year:N',title=None),\n    alt.Y('count():Q',stack=False,title='Number of trees planted per genus'),\n    alt.Color('genus_name:N',title='Genus name')\n).add_selection(select_genus).encode(\n    opacity=alt.condition(select_genus, alt.value(0.9), alt.value(0.0))).properties(title=plot_3_title)\n\nplot_3_genus_year_bar\n\n\n\n\n\n\n\nFigure 3 with dropdown selection has combined multiple information required in one plot and provide the audiences with convenience to efficiently explore number of trees planted from 1989 to 2019 for each genus.\nThe answer to the question 2 would be valuable reference for the public who are interested with tree planting history in Vancouver. In addition, for researchers, the answer will provide a first-hand insight into the urban forestry and street trees replacement planning.\n\n\nQ3 - Based on the growth rates of each genus, which genus is the fastest one within the Vancouver street trees dataset?\nThe insights extracted from this question will not only confirm or verify some of the current scientific conclusions on trees growth patterns, but also provide accumulated evidences for improving the optimization of street trees in cities with similar ecological environment as Vancouver.\n\n# Calculate growth rate in diameter, growth rate in height and save them into df\n\ntrees_df_rate = trees_df_age.assign(rate_dia=trees_df_age['diameter']/trees_df_age['age'],\n                                    rate_height=trees_df_age['height_range_id']/trees_df_age['age']\n                                   ).round(2)\n\n# Plot two growth rates using scatter chart\n# Add zooming and panning to solve the issue that the plot is saturated\n# Add dropdown widget to select genus\nplot_4_title = alt.TitleParams(\n    \"Figure 4 Trees growth rate in diameter & height per genus\",\n    subtitle = [\"Dropdown selection is available by genus\", \"Zooming and Panning are available\"],\n    subtitleColor='steelblue', subtitleFontWeight='bold')\n\nplot_growth_rates = alt.Chart(trees_df_rate).mark_point(size=5).encode(\n    alt.X('rate_dia:Q',title=\"Growth rate in diameter (inch/yr)\"),\n    alt.Y('rate_height:Q',title=\"Growth rate in height (unit/yr)\")\n).interactive()\n\nplot_4_growth_rates_genus = plot_growth_rates.add_selection(select_genus).encode(\n    opacity=alt.condition(select_genus, alt.value(0.9), alt.value(0.0))).properties(title=plot_4_title,width=600)\n\nplot_4_growth_rates_genus\n\n\n\n\n\n\n\nNote that the growth rate in height is represented in unit/yr. Here the “unit” refers to height_range_id: 0-10 for every 10 feet (e.g., 0 = 0-10 ft, 1 = 10-20 ft, 2 = 20-30 ft, and10 = 100+ ft)\nBy adding zooming, panning and dropdown widget selection features, audiences can effectively check the two growth rates based on their interested genus.\nIt must be noted that in the original dataframe, the height of trees was collected as height_range_id. Audiences should be aware that the calculated average growth rate by height is not an absolute value of height but a processed relative variable. Typically, if the quantity of observations for a particular genus is sufficient (deep research is required to define the “sufficiency”), the result will have more accuracy from the statistical perspective. But if the genus has limited observations in the dataframe, the audiences should be more careful to assess the results. Again, deep research is required in future analysis.\nIn order to effectively compare the growth rate for all genera, except for a bar chart that just shows the mean value, a boxplot is also created to include a few key summary statistics and visualize data distribution. This combination will provide more information to audiences particularly for researchers who may care about the statistics more than a mean value.\n\n# Plot average growth rates in diameter per genus\n\nplot_5_title = alt.TitleParams(\n    \"Figure 5 Trees average growth rate in diameter per genus\",\n    subtitle = \"TSUGA, PSEUDOTSUGA & POPULUS are top three fastest growing genera\")\n\nplot_ave_growth_rate_dia_bar = alt.Chart(trees_df_rate).mark_bar().encode(\n    alt.X('mean(rate_dia)', title='Average growth rate in diameter (inch/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort='x'),\n    alt.Color('count()',scale=alt.Scale(scheme='blues'),title='Number of trees')\n).properties(width=130)\n\n# Sort boxplot by average growth rate in diameter to keep consistent with bar chart\nave_growth_rate_dia_order = trees_df_rate.groupby('genus_name')['rate_dia'].mean().sort_values().index.tolist()\n\n# To distinguish the boxplot from the bar chart, specify the color as green\nplot_ave_growth_rate_dia_box = alt.Chart(trees_df_rate).mark_boxplot(color='green').encode(\n    alt.X('rate_dia:Q', title='Growth rate in diameter (inch/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort=ave_growth_rate_dia_order)\n).properties(width=310)\n\n# Modify the domain of the axis’ scale for boxplot to make the plot more readable\n# At the same time, discard outliers with the value more than 4 inch/year\nplot_ave_growth_rate_dia_box_scale = plot_ave_growth_rate_dia_box.encode(alt.X('rate_dia:Q', \n                                                                               title='Growth rate in diameter (inch/yr)',\n                                                                               scale=alt.Scale(domain=[0, 4],clamp=True)))\n\n# Use top-level title configuration to locate title in the middle\nplot_5_ave_growth_rates_dia = (plot_ave_growth_rate_dia_bar | plot_ave_growth_rate_dia_box_scale\n                              ).properties(title=plot_5_title).configure_title(anchor='middle')\n\nplot_5_ave_growth_rates_dia\n\n\n\n\n\n\n\n# Plot average growth rates in height per genus\n\nplot_6_title = alt.TitleParams(\n    \"Figure 6 Trees average growth rate in height per genus\",\n    subtitle = \"TSUGA, POPULUS & PSEUDOTSUGA are top three fastest growing genera\")\n\n# To distinguish the plot from the above growth rates in diameter chart, change alternative color scheme\nplot_ave_growth_rate_height_bar = alt.Chart(trees_df_rate).mark_bar().encode(\n    alt.X('mean(rate_height)', title='Average growth rate in height (unit/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort='x'),\n    alt.Color('count()',scale=alt.Scale(scheme='teals'),title='Number of trees')\n).properties(width=130)\n\n# Sort boxplot by average growth rate in height to keep consistent with bar chart\nave_growth_rate_height_order = trees_df_rate.groupby('genus_name')['rate_height'].mean().sort_values().index.tolist()\n\n# To distinguish the boxplot from the bar chart, specify the color as grey\nplot_ave_growth_rate_height_box = alt.Chart(trees_df_rate).mark_boxplot(color='grey').encode(\n    alt.X('rate_height:Q', title='Growth rate in height (unit/yr)'),\n    alt.Y('genus_name:N',title='Genus',sort=ave_growth_rate_height_order)\n).properties(width=310)\n\n# Modify the domain of the axis’ scale for boxplot to make the plot more readable\n# At the same time, discard outliers with the value more than 0.6 unit/year\nplot_ave_growth_rate_height_box_scale = plot_ave_growth_rate_height_box.encode(alt.X('rate_height:Q', \n                                                                               title='Growth rate in height (unit/yr)',\n                                                                               scale=alt.Scale(domain=[0, 0.6],clamp=True)))\n\n# Use top-level title configuration to locate title in the middle\nplot_6_ave_growth_rates_height = (plot_ave_growth_rate_height_bar | plot_ave_growth_rate_height_box_scale\n                                 ).properties(title=plot_6_title).configure_title(anchor='middle')\n\nplot_6_ave_growth_rates_height\n\n\n\n\n\n\n\nNote that the growth rate in height is represented in unit/yr. Here the “unit” refers to height_range_id: 0-10 for every 10 feet (e.g., 0 = 0-10 ft, 1 = 10-20 ft, 2 = 20-30 ft, and10 = 100+ ft)\nBased on Figure 5 and 6, TSUGA, POPULUS and PSEUDOTSUGA are identified as the top three fastest growing genera. But as indicated on the plots and Figure 1, the sample size of the three genera is very limited. The number of trees (observations) of TSUGA, POPULUS and PSEUDOTSUGA in the dataframe are 2, 3, and 8, respectively. Since the sample size may affect the confidence level when analyzing and comparing the growth rates among genera, a futher statistical analysis should be considered in the future to justify the conclusion in this case.\n\n\nQ4 - How many street trees are growing in each neighbourhood and what are the average age and average diameter of trees in each neighbourhood?\nThe last question might be interesting for residents in Vancouver to know or for potential home buyers to select their desired neighbourhood with considerations of trees distribution and some characters.\n\n# To make a base map of Vancouver by using the geojson url saved in url_geojson\n\nurl_geojson = 'https://raw.githubusercontent.com/UBC-MDS/exploratory-data-viz/main/data/local-area-boundary.geojson'\n\n# Format it in a Topo json format using alt.Data()\n\ndata_geojson_remote = alt.Data(url=url_geojson, format=alt.DataFormat(property='features',type='json'))\n\n# Then make base Vancouver Altair map using the data_geojson_remote object\n# Use an identity type and need to reflectY=True. Without this second argument the map of Vancouver is upside down\n\nbase_map = alt.Chart(data_geojson_remote).mark_geoshape(\n    color='white',stroke=\"grey\").encode(\n).project(type='identity', reflectY=True)\n\nPerform calculation needed and create a new dataframe.\n\n# Rename neighbourhood_name to name since that's what's it's called in the geojson url\ntrees_nbhd_num_df = pd.DataFrame(trees_df_age['neighbourhood_name'].value_counts()\n                            ).reset_index().rename(columns={'index':'name','neighbourhood_name':'trees_number'})\n\ntrees_nbhd_age_df = trees_df_age.groupby('neighbourhood_name').mean().round(1).reset_index(\n).rename(columns={'neighbourhood_name':'name','diameter':'ave_diameter','age':'ave_age'})[['name','ave_diameter','ave_age']]\n\ntrees_nbhd_df = trees_nbhd_num_df.merge(trees_nbhd_age_df, left_on='name', right_on='name')\n\ntrees_nbhd_df\n\n\n\n\n\n  \n    \n      \n      name\n      trees_number\n      ave_diameter\n      ave_age\n    \n  \n  \n    \n      0\n      Renfrew-Collingwood\n      1323\n      5.6\n      17.5\n    \n    \n      1\n      Hastings-Sunrise\n      1285\n      8.1\n      19.7\n    \n    \n      2\n      Kensington-Cedar Cottage\n      1169\n      6.5\n      18.7\n    \n    \n      3\n      Sunset\n      936\n      6.0\n      17.3\n    \n    \n      4\n      Victoria-Fraserview\n      908\n      5.5\n      17.7\n    \n    \n      5\n      Dunbar-Southlands\n      762\n      6.1\n      16.7\n    \n    \n      6\n      Marpole\n      689\n      5.6\n      17.2\n    \n    \n      7\n      Riley Park\n      684\n      5.8\n      17.2\n    \n    \n      8\n      Grandview-Woodland\n      632\n      6.6\n      18.0\n    \n    \n      9\n      Killarney\n      615\n      6.2\n      17.7\n    \n    \n      10\n      Kerrisdale\n      612\n      6.0\n      16.6\n    \n    \n      11\n      Oakridge\n      549\n      6.1\n      17.9\n    \n    \n      12\n      Arbutus-Ridge\n      535\n      6.8\n      17.5\n    \n    \n      13\n      Kitsilano\n      526\n      7.2\n      16.8\n    \n    \n      14\n      Mount Pleasant\n      513\n      7.9\n      18.7\n    \n    \n      15\n      Shaughnessy\n      483\n      5.1\n      16.6\n    \n    \n      16\n      West Point Grey\n      423\n      6.5\n      17.7\n    \n    \n      17\n      Downtown\n      393\n      5.8\n      16.4\n    \n    \n      18\n      South Cambie\n      362\n      6.5\n      18.3\n    \n    \n      19\n      Fairview\n      248\n      6.8\n      16.9\n    \n    \n      20\n      Strathcona\n      225\n      6.2\n      16.3\n    \n    \n      21\n      West End\n      211\n      7.2\n      18.0\n    \n  \n\n\n\n\n\n# Add tooltips and hover selection on map\n\nhover = alt.selection_single(fields=['name'], on='mouseover')\n\nplot_trees_nbhd = alt.Chart(data_geojson_remote).mark_geoshape().transform_lookup(\n    lookup='properties.name',\n    from_=alt.LookupData(trees_nbhd_df, 'name', ['trees_number','name'])).encode(\n    alt.Color('trees_number:Q',title='Number of trees'),\n    opacity=alt.condition(hover, alt.value(1), alt.value(0.1)),\n    tooltip=['name:N',alt.Tooltip('trees_number:Q', title='Number of trees')]\n).add_selection(hover).project(type='identity', reflectY=True)\n\nplot_trees_nbhd_map = base_map + plot_trees_nbhd\n\n# Plot ave_age per ave_diameter\n# Scale the x and y axis by specifying zero=False\n\nplot_age_dia = alt.Chart(trees_nbhd_df).mark_point().encode(\n    alt.X('ave_diameter:Q',scale=alt.Scale(zero=False),title='Average diameter (inch)'),\n    alt.Y('ave_age:Q',scale=alt.Scale(zero=False),title='Average age (yr)'),\n    stroke=alt.condition(hover, alt.value('black'), alt.value('#ffffff')))\n\nplot_7_title = alt.TitleParams(\n    \"Figure 7 Trees distribution per neighbourhood (number/age/diameter)\",\n    subtitle = [\"Hover selection on map is available by neighbourhood\", \"Tooltips are available on map\"],\n    subtitleColor='steelblue', subtitleFontWeight='bold')\n\nplot_7_nbhd_map = (plot_trees_nbhd_map & plot_age_dia\n                  ).properties(title=plot_7_title).configure_title(anchor='middle')\n\nplot_7_nbhd_map\n\n\n\n\n\n\nBased on the above trees_nbhd_df and Figure 7, the average age of trees in these neighbourhoods are between 16.3 years old and 19.7 years old. And the average diameter of trees are from 5.1 inch to 8.1 inch.\nThrough the link of map selection with the scatter plot of Figure 7, it is effective and efficient for audiences to hover over their target neighbourhood on map to quickly obtain the number of trees in the neighbourhood, know how old and how big the trees there on average."
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis.html#discussions",
    "href": "posts/vancouver-street-trees/trees-analysis.html#discussions",
    "title": "Vancouver Street Trees",
    "section": "DISCUSSIONS",
    "text": "DISCUSSIONS\nThe analysis intends to discover an overall view of Vancouver street trees for their biological diversity, growth and distributions within neighbourhoods. Through Figure 1 to Figure 7 and related tables, all four pre-defined questions have been answered. It has been confirmed that within 68 genera and a total of 14,083 trees, Acer is the most popular genus based on the dataset. It is not surprised as the maple trees are Canada’s official arboreal emblem. Historical data shows Vancouver has maintained a dynamic public trees planting program. Especially between 1995 and 2013, a large number of trees had been added to the streets by the government. The plots generated can effectively provide audiences with more details about the yearly number of trees planted by genus, the growth rates by genus, the trees distribution and certain characters by neighbourhood. In addition, by comparing the growth rates within 68 genera, it is noted that TSUGA, POPULUS and PSEUDOTSUGA are the top three fastest growing genera in Vancouver.\nExcept for the four questions that have been discussed, further questions could be considered in the future:\n\nWhat is the specific genus distribution as per neighbourhood. This could be demonstrated on the Vancouver map, too. The answer will be informative for residents to know the distribution of fast-growing trees or shade trees in their area.\nWhat is the average height range of trees in each neighbourhood. Some house buyers may prefer taller trees or shade trees in their neighbourhood.\nSince genus is the highest level of tree classification in the df, data analyst may extend the analysis into the next level, species, or common names, or cultivar_names to continuously explore the data patterns.\n\nIn the analysis report, the assumption and limitations of the analysis should be highlighted for audiences to pay attention, including:\n\nThe original data only includes the public trees on boulevards in the City of Vancouver. Park trees and private trees are not included.\nMore than half of the original observations from the original dataset have been dropped since lacking information on the date of the tree being planted. Therefore, the analysis is based on the remaining observations only. This may limit the conclusions or insights extracted from this analysis.\nIn order to simplify the analysis, the calculation of the age of trees has not included the specific month and day.\nThe information related to the height of trees was collected as height_range_id. So audiences should be aware that the height figures in this report are not the absolute measurement of height.\nDue to the limited sample size, more research is required to verify the conclusion of the fastest growing trees in this report.\n\nThe last important notes: 1. The top priority of this analysis is emphasizing on the demonstration of learning outcomes from the Data Vis perspectives. The analyst (author) recognized in this report the narrative should be more cohesive, the original dataset column analysis and data wrangling should be supported with more domain expertise if judged from a higher level data analysis standard. 2. More statistical tools could be applied in the analysis to make the report more convincing. This will be considered for future work."
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis.html#dashboard-1",
    "href": "posts/vancouver-street-trees/trees-analysis.html#dashboard-1",
    "title": "Vancouver Street Trees",
    "section": "DASHBOARD 1",
    "text": "DASHBOARD 1\n\n# Resize plots to accommodate the combined panel\n# Create new object to avoid overwriting previous plots\nplot_1 = plot_1_genus.properties(height=1100,width=250)\nplot_2 = plot_2_year.properties(height=300,width=350)\nplot_3 = plot_3_genus_year_bar.properties(height=300,width=350)\nplot_4 = plot_4_growth_rates_genus.properties(height=300,width=350)\n\nplot_1 | (plot_2 & plot_3 & plot_4)"
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis.html#dashboard-2",
    "href": "posts/vancouver-street-trees/trees-analysis.html#dashboard-2",
    "title": "Vancouver Street Trees",
    "section": "DASHBOARD 2",
    "text": "DASHBOARD 2\n\n# Since objects with \"config\" attribute cannot be used within VConcatChart, remove the previous \"config\"\n# Create new object to avoid overwriting previous plots\nplot_5 = (plot_ave_growth_rate_dia_bar | plot_ave_growth_rate_dia_box_scale).properties(title=plot_5_title)\nplot_6 = (plot_ave_growth_rate_height_bar | plot_ave_growth_rate_height_box_scale).properties(title=plot_6_title)\nplot_7 = (plot_trees_nbhd_map & plot_age_dia).properties(title=plot_7_title)\n\n(plot_5 & plot_6) & plot_7"
  },
  {
    "objectID": "posts/vancouver-street-trees/trees-analysis.html#references",
    "href": "posts/vancouver-street-trees/trees-analysis.html#references",
    "title": "Vancouver Street Trees",
    "section": "REFERENCES",
    "text": "REFERENCES\n\nResources Used\nNot all the work in this project is original. The following resources have been used as references.\n\nData Source\n\nThe Vancouver street trees dataset provided by UBC Data Visualization course. The dataset schema from City of Vancouver Open Data Portal - Street Trees.\n\nSample Report\n\nThe sample reports, Evolution of LEGO EDA report and analysis report provided by UBC Data Visualization course.\n\nReport Writing\n\nMarkdown formatting for the text cells in JupyterNotebook, Basic Syntax - The Markdown elements outlined in John Gruber’s design document, retrieved on November 1, 2021 from Markdown Guide.\n\nPython and Data Vis Techniques\n\nLearned from UBC Data Visualization course, Programming in Python for Data Science course, Piazza group discussions, Altair Documentation, and other self-learning resources from the internet.\n\nalt.__version__\n\n'4.2.0'"
  },
  {
    "objectID": "posts/test/trees-analysis.html",
    "href": "posts/test/trees-analysis.html",
    "title": "Test - Altair Viz",
    "section": "",
    "text": "Data imports\n\n# Import libraries \nimport altair as alt\nimport pandas as pd\nimport json\n\nalt.data_transformers.enable(\"data_server\")\n\ntrees_df_original = pd.read_csv('data/vancouver_trees.csv')\n\n# Glance at the original df\ntrees_df_original\n\n\n\n\n\n  \n    \n      \n      std_street\n      on_street\n      species_name\n      neighbourhood_name\n      date_planted\n      diameter\n      street_side_name\n      genus_name\n      assigned\n      civic_number\n      plant_area\n      curb\n      tree_id\n      common_name\n      height_range_id\n      on_street_block\n      cultivar_name\n      root_barrier\n      latitude\n      longitude\n    \n  \n  \n    \n      0\n      W 13TH AV\n      MAPLE ST\n      PSEUDOPLATANUS\n      Kitsilano\n      NaN\n      9.00\n      EVEN\n      ACER\n      N\n      1996\n      10\n      Y\n      13310\n      SYCAMORE MAPLE\n      4\n      2900\n      NaN\n      N\n      49.259856\n      -123.150586\n    \n    \n      1\n      WALES ST\n      WALES ST\n      PLATANOIDES\n      Renfrew-Collingwood\n      2018-11-28\n      3.00\n      ODD\n      ACER\n      N\n      5291\n      7\n      Y\n      259084\n      PRINCETON GOLD MAPLE\n      1\n      5200\n      PRINCETON GOLD\n      N\n      49.236650\n      -123.051831\n    \n    \n      2\n      W BROADWAY\n      W BROADWAY\n      RUBRUM\n      Kitsilano\n      1996-04-19\n      14.00\n      EVEN\n      ACER\n      N\n      3618\n      C\n      Y\n      167986\n      KARPICK RED MAPLE\n      3\n      3600\n      KARPICK\n      N\n      49.264250\n      -123.184020\n    \n    \n      3\n      PENTICTON ST\n      PENTICTON ST\n      CALLERYANA\n      Renfrew-Collingwood\n      2006-03-06\n      3.75\n      EVEN\n      PYRUS\n      N\n      2502\n      5\n      Y\n      213386\n      CHANTICLEER PEAR\n      1\n      2500\n      CHANTICLEER\n      Y\n      49.261036\n      -123.052921\n    \n    \n      4\n      RHODES ST\n      RHODES ST\n      GLYPTOSTROBOIDES\n      Renfrew-Collingwood\n      2001-11-01\n      3.00\n      ODD\n      METASEQUOIA\n      N\n      5639\n      N\n      Y\n      189223\n      DAWN REDWOOD\n      2\n      5600\n      NaN\n      N\n      49.233354\n      -123.050249\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      29995\n      ROBSON ST\n      ROBSON ST\n      CAMPESTRE\n      West End\n      NaN\n      7.00\n      ODD\n      ACER\n      N\n      1015\n      c\n      Y\n      122814\n      HEDGE MAPLE\n      2\n      1000\n      NaN\n      N\n      49.283666\n      -123.123231\n    \n    \n      29996\n      OSLER ST\n      CONNAUGHT DRIVE\n      PLATANOIDES\n      Shaughnessy\n      2007-04-16\n      8.00\n      ODD\n      ACER\n      N\n      4690\n      10\n      Y\n      132211\n      NORWAY MAPLE\n      1\n      1000\n      NaN\n      Y\n      49.243636\n      -123.129480\n    \n    \n      29997\n      BEATRICE ST\n      BEATRICE ST\n      CERASIFERA\n      Victoria-Fraserview\n      NaN\n      17.30\n      EVEN\n      PRUNUS\n      N\n      6218\n      9\n      Y\n      59355\n      PISSARD PLUM\n      3\n      6200\n      ATROPURPUREUM\n      N\n      49.227406\n      -123.066936\n    \n    \n      29998\n      ANGUS DRIVE\n      ANGUS DRIVE\n      BILOBA\n      Shaughnessy\n      2006-02-17\n      4.00\n      ODD\n      GINKGO\n      N\n      1551\n      9\n      Y\n      207753\n      GINKGO OR MAIDENHAIR TREE\n      1\n      1500\n      NaN\n      Y\n      49.254431\n      -123.140382\n    \n    \n      29999\n      MAIN ST\n      MAIN ST\n      EUCHLORA   X\n      Riley Park\n      NaN\n      12.00\n      ODD\n      TILIA\n      N\n      4323\n      C\n      Y\n      92997\n      CRIMEAN LINDEN\n      4\n      4300\n      NaN\n      N\n      49.246969\n      -123.101328\n    \n  \n\n30000 rows × 20 columns\n\n\n\n\n\n\nIdentify and drop irrelevant columns\n\n# Check columns of the original df\ntrees_df_original.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30000 entries, 0 to 29999\nData columns (total 20 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   std_street          30000 non-null  object \n 1   on_street           30000 non-null  object \n 2   species_name        30000 non-null  object \n 3   neighbourhood_name  30000 non-null  object \n 4   date_planted        14085 non-null  object \n 5   diameter            30000 non-null  float64\n 6   street_side_name    30000 non-null  object \n 7   genus_name          30000 non-null  object \n 8   assigned            30000 non-null  object \n 9   civic_number        30000 non-null  int64  \n 10  plant_area          29722 non-null  object \n 11  curb                30000 non-null  object \n 12  tree_id             30000 non-null  int64  \n 13  common_name         30000 non-null  object \n 14  height_range_id     30000 non-null  int64  \n 15  on_street_block     30000 non-null  int64  \n 16  cultivar_name       16178 non-null  object \n 17  root_barrier        30000 non-null  object \n 18  latitude            30000 non-null  float64\n 19  longitude           30000 non-null  float64\ndtypes: float64(3), int64(4), object(13)\nmemory usage: 4.6+ MB\n\n\nBased on the above data information and the dataset schema from City of Vancouver Open Data Portal - Street Trees, the columns can be preliminarily identified as four groups:\n\nTrees biological classifications and names, such as genus_name, species_name, common_name, cultivar_name\nTrees growth related characteristics, such as date_planted, diameter, height_range_id\nTrees coordinates and areas, such as latitude, longitude, neighbourhood_name\nOther specific location / orientation / identification information\n\nAs per the questions, the irrelevant columns under the fourth group and trees coordinates will be dropped. Also, to narrow down the focus to the highest level of tree classification, the columns of species_name, common_name, cultivar_name will also be dropped and only keep the column of genus_name.\n\ntrees_df = pd.read_csv('data/vancouver_trees.csv',\n  usecols=['neighbourhood_name','date_planted','diameter','genus_name','height_range_id'],  \n  parse_dates = ['date_planted'])\n                                    \ntrees_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30000 entries, 0 to 29999\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   neighbourhood_name  30000 non-null  object        \n 1   date_planted        14085 non-null  datetime64[ns]\n 2   diameter            30000 non-null  float64       \n 3   genus_name          30000 non-null  object        \n 4   height_range_id     30000 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n\n\n\n\nExam null values\n\n# Drop observations without value of date_planted\n\ntrees_df = trees_df.dropna(subset=['date_planted'])\n\ntrees_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 14085 entries, 1 to 29998\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   neighbourhood_name  14085 non-null  object        \n 1   date_planted        14085 non-null  datetime64[ns]\n 2   diameter            14085 non-null  float64       \n 3   genus_name          14085 non-null  object        \n 4   height_range_id     14085 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(1), object(2)\nmemory usage: 660.2+ KB\n\n\n\n\nFurther exam for error data\n\n# Exam the remaining columns for more details\n\ntrees_df.describe(include='all', datetime_is_numeric=True)\n\n\n\n\n\n  \n    \n      \n      neighbourhood_name\n      date_planted\n      diameter\n      genus_name\n      height_range_id\n    \n  \n  \n    \n      count\n      14085\n      14085\n      14085.000000\n      14085\n      14085.000000\n    \n    \n      unique\n      22\n      NaN\n      NaN\n      68\n      NaN\n    \n    \n      top\n      Renfrew-Collingwood\n      NaN\n      NaN\n      ACER\n      NaN\n    \n    \n      freq\n      1323\n      NaN\n      NaN\n      3970\n      NaN\n    \n    \n      mean\n      NaN\n      2003-09-20 17:40:42.172523904\n      6.352586\n      NaN\n      1.822932\n    \n    \n      min\n      NaN\n      1989-10-27 00:00:00\n      0.000000\n      NaN\n      0.000000\n    \n    \n      25%\n      NaN\n      1997-12-15 00:00:00\n      3.000000\n      NaN\n      1.000000\n    \n    \n      50%\n      NaN\n      2003-04-01 00:00:00\n      5.000000\n      NaN\n      2.000000\n    \n    \n      75%\n      NaN\n      2009-11-13 00:00:00\n      8.000000\n      NaN\n      2.000000\n    \n    \n      max\n      NaN\n      2019-06-03 00:00:00\n      317.000000\n      NaN\n      9.000000\n    \n    \n      std\n      NaN\n      NaN\n      5.273737\n      NaN\n      0.983476\n    \n  \n\n\n\n\n\nIt has been noticed that the minimum of diameter and height_range_id are zero. For height_range_id, 0 represents the range of height is from 0 to 10 ft. However, since the diameter figure is the diameter of tree at breast height, it should not have the value of 0. So these observations will be considered as invalid data and removed from the df.\n\nindexs = trees_df[trees_df['diameter'] == 0].index\n\ntrees_df.drop(indexs, inplace=True)\n\ntrees_df.describe(include='all', datetime_is_numeric=True)\n\n\n\n\n\n  \n    \n      \n      neighbourhood_name\n      date_planted\n      diameter\n      genus_name\n      height_range_id\n    \n  \n  \n    \n      count\n      14083\n      14083\n      14083.000000\n      14083\n      14083.000000\n    \n    \n      unique\n      22\n      NaN\n      NaN\n      68\n      NaN\n    \n    \n      top\n      Renfrew-Collingwood\n      NaN\n      NaN\n      ACER\n      NaN\n    \n    \n      freq\n      1323\n      NaN\n      NaN\n      3970\n      NaN\n    \n    \n      mean\n      NaN\n      2003-09-20 23:57:38.893701504\n      6.353489\n      NaN\n      1.822978\n    \n    \n      min\n      NaN\n      1989-10-27 00:00:00\n      0.500000\n      NaN\n      0.000000\n    \n    \n      25%\n      NaN\n      1997-12-15 12:00:00\n      3.000000\n      NaN\n      1.000000\n    \n    \n      50%\n      NaN\n      2003-04-01 00:00:00\n      5.000000\n      NaN\n      2.000000\n    \n    \n      75%\n      NaN\n      2009-11-13 00:00:00\n      8.000000\n      NaN\n      2.000000\n    \n    \n      max\n      NaN\n      2019-06-03 00:00:00\n      317.000000\n      NaN\n      9.000000\n    \n    \n      std\n      NaN\n      NaN\n      5.273568\n      NaN\n      0.983520\n    \n  \n\n\n\n\n\n\n\nDataset Description\nThe cleaned target dataframe trees_df is composed of 5 columns. There are 68 distinct genera within the total of 14,083 trees (observations). According to City of Vancouver Open Data Portal - Street Trees where the dataset was originally obtained, the brief descriptions of columns are listed as below:\n\nCategorical columns\n\nneighbourhood_name: City’s defined local area in which the tree is located.\ngenus_name: Genus name of trees.\n\nQuantitative columns\n\ndiameter: DBH in inches (DBH stands for diameter of tree at breast height).\nheight_range_id: 0-10 for every 10 feet (e.g., 0 = 0-10 ft, 1 = 10-20 ft, 2 = 20-30 ft, and10 = 100+ ft).\n\nDatetime columns\n\ndate_planted: The date of planting.\n\n\nQ1\n\nBased on distribution of street trees planted in Vancouver by genus, which genus is the most popular one?\n\nBiological diversity is one of the interests for the analysis. The quick guess for the answer would be Acer as Acer is a genus of trees commonly known as maples. A maple leaf is on the Canadian flag, and the maple has been chosen as a national symbol in Canada.\n\n# To visualize distrubition of genus for whole df\nplot_1_title = alt.TitleParams(\n    \"Figure 1 Number of street trees planted per genus\",\n     subtitle = \"Acer is the most popular genus of street trees\")\n\nplot_1_genus = alt.Chart(trees_df).mark_bar().encode(\n    alt.X('count():Q',title='Number of Trees'),\n    alt.Y('genus_name:N',title='Genus',sort='-x')\n)\n\n# Add text annotation for the number of trees for each genus\ntext_1_genus = plot_1_genus.mark_text(align='left',dx=2).encode(text='count():Q')\n\nplot_1_genus = (plot_1_genus + text_1_genus).properties(title=plot_1_title,width=550)\n\nplot_1_genus\n\n\n\n\n\n\nFrom Figure 1, it has been confirmed that Acer is the most popular genus of trees based on the dataframe. The number of Acer trees is more than double of Prunus trees that have the second largest number in the Vancouver street trees. All the rest of the genera have less than 1,000 trees shown in the dataframe, and 47 out of the total 68 genera have less than 100 trees.\n\n\nQ2\n\nIn the past 30 years, how many trees have been planted every single year? And what is the number of trees planted by genus each year?\n\nThe dataframe has provide the specific date of being planted for each tree. From the available data, the age of current trees will be calculated first and then plot the number of trees planted by year. Since the accuracy is not a priority in this case, only the year of tree planted will be extracted for calculating the age till 2021.\n\n# Extract year of planted and calculate age of trees\ntrees_df_yr = trees_df.assign(year=trees_df['date_planted'].dt.year)\n\n# Calculate the age of trees till 2021.\ntrees_df_age = trees_df_yr.assign(age=(2021-trees_df_yr['year']))\n\n# Plot distribution of number of trees planted by year\n# It has been tested that if specifying the dtype of column 'year' as quantitative (Q), the x-axis label takes scientific\n# formatting, ie. 1,989. To avoid the \",\", the dtype of 'year' has been specified as nominal (N)\nplot_2_title = alt.TitleParams(\n    \"Figure 2 Number of street trees planted each year\",\n     subtitle = \"(Data available from 1989 to 2019)\")\n\nplot_2_year = alt.Chart(trees_df_age).mark_bar().encode(\n    alt.X('year:N',title=None),\n    alt.Y('count():Q',title='Number of trees planted')).properties(title=plot_2_title)\n\nplot_2_year\n\n\n\n\n\n\nFigure 2 indicates that there was a peak period between 1995 and 2013 to plant street trees by the City of Vancouver. During this period, the city had planted the highest number of trees in a single year in 1998 and in 2013. Before 1995 and after 2014, the number of trees planted was relatively lower, especially in 2016 when there were less than 50 new trees on public boulevards in Vancouver. Urban forestry is a systemic project. How many trees are planted is determined by a group of factors, such as public budget, tree replacement plan due to species distribution, insects, diseases, or environmental stress, etc. It is a clear message that the City of Vancouver has maintained a dynamic public trees planting program that benefits the wellbeings of residents in Vancouver.\nMoving further, to plot number of trees planted by year and add dropdown selection by genus.\n\n# Plot number of trees planted by year and add dropdown selection by genus\n\n# Specify the subtitle color and bold it to draw attention\nplot_3_title = alt.TitleParams(\n    \"Figure 3 Number of street trees planted each year by genus(from 1989 to 2019)\",\n    subtitle = \"Dropdown selection is available by genus\",\n    subtitleColor='steelblue', subtitleFontWeight='bold')\n\ngenus = sorted(trees_df_age['genus_name'].unique())\n\ndropdown_genus = alt.binding_select(name='Genus', options=genus)\n\nselect_genus = alt.selection_single(fields=['genus_name'], bind=dropdown_genus)\n\n# Since objective is to see number of trees for each genus, for y axis, \n# specify stack=False\nplot_3_genus_year_bar = alt.Chart(trees_df_age).mark_bar().encode(\n    alt.X('year:N',title=None),\n    alt.Y('count():Q',stack=False,title='Number of trees planted per genus'),\n    alt.Color('genus_name:N',title='Genus name')\n).add_selection(select_genus).encode(\n    opacity=alt.condition(select_genus, alt.value(0.9), alt.value(0.0))\n    ).properties(title=plot_3_title)\n\nplot_3_genus_year_bar\n\n\n\n\n\n\n\nFigure 3 with dropdown selection has combined multiple information required in one plot and provide the audiences with convenience to efficiently explore number of trees planted from 1989 to 2019 for each genus.\nThe answer to the question 2 would be valuable reference for the public who are interested with tree planting history in Vancouver. In addition, for researchers, the answer will provide a first-hand insight into the urban forestry and street trees replacement planning.\nTo be continued…"
  }
]