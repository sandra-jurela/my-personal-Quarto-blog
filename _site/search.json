[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Let’s be friends",
    "section": "",
    "text": "Hi and welcome to my personal blog. My name is Sandra Jurela and here I will be sharing my journey into Data Science. Recently I have developed a real passion for Data Science and this blog truly is a manifestation of that.\nI hold a Master’s degree in Civil Engineering (Hydrology and Water Resources Science). I started my career as a construction designer, and then for 10 years worked as a hydrologist at the Croatian Meteorological and Hydrological Service. I currently work for my family’s business.\nI live in Zagreb, Croatia, with my two boys (life partner and our sweet son).\nIf you’d like to connect with me, please do. I hope you enjoy your visit!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sandra Jurela",
    "section": "",
    "text": "Shiny App - Mass Shootings in the USA\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nEDA\n\n\ndata cleaning\n\n\n\n\nEDA with Shiny app on mass shootings between August 20th, 1982 and February 13th, 2023.\n\n\n\n\n\n\nMar 1, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCryptocurrency SQL Case Study\n\n\n\n\n\n\n\nSQL\n\n\nPostgreSQL\n\n\nEDA\n\n\n\n\nData with Danny - SQL masterclass - O’Reilly. With R solutions for Part 4: Window functions.\n\n\n\n\n\n\nNov 25, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nScraping 200 Best Movies of 2010s from Rotten Tomatoes\n\n\n\n\n\n\n\nweb scraping\n\n\ndata wrangling\n\n\nrvest\n\n\nR\n\n\nTableau\n\n\n\n\nScraping data on movies from Rotten Tomatoes and finally creating a dashboard in Tableau\n\n\n\n\n\n\nSep 1, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nTitanic Survival Exercises\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\ndata visualization\n\n\n\n\nAssessment of visualization skills acquired in the HarvardX’s Data Science: Visualization course.\n\n\n\n\n\n\nJun 9, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Analysis – Advanced Plots With {ggplot2}\n\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\nSQL\n\n\nPostgreSQL\n\n\nR\n\n\n\n\nQuerying database in R code chunk and making some useful plots with ggplot2.\n\n\n\n\n\n\nMay 20, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nAnswering Business Questions Using SQL\n\n\n\n\n\n\n\nSQL\n\n\nSQLite\n\n\nDataquest\n\n\ndata visualization\n\n\nR\n\n\n\n\nMy SQL project for the “Intermediate SQL for Data Analysis” course at Dataquest.\n\n\n\n\n\n\nFeb 15, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html",
    "href": "posts/answering-business-questions-using-sql/chinook.html",
    "title": "Answering Business Questions Using SQL",
    "section": "",
    "text": "The aim of this project is to explore a modified version of the Chinook database using SQL and answer some business questions. The Chinook database represents a fictional digital media shop, based on real data from an iTunes Library and manually generated data. The database is provided as a SQLite database file called chinook.db.\nHere’s a schema diagram for the Chinook database:"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "href": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "title": "Answering Business Questions Using SQL",
    "section": "Connecting to the Database and Data Overview",
    "text": "Connecting to the Database and Data Overview\n\nlibrary(DBI)\n\ndb <- dbConnect(RSQLite::SQLite(), dbname = \"data/chinook.db\")\n\nListing all tables in the Chinook database.\n\nSELECT\n  name,\n  type\nFROM sqlite_master\nWHERE type IN (\"table\", \"view\")\n\n\n11 records\n\n\nname\ntype\n\n\n\n\nalbum\ntable\n\n\nartist\ntable\n\n\ncustomer\ntable\n\n\nemployee\ntable\n\n\ngenre\ntable\n\n\ninvoice\ntable\n\n\ninvoice_line\ntable\n\n\nmedia_type\ntable\n\n\nplaylist\ntable\n\n\nplaylist_track\ntable\n\n\ntrack\ntable\n\n\n\n\n\nThe database consists of 11 tables containing information about artists, albums, media tracks, playlists, invoices, customers, and shop employees. Let’s start by getting familiar with our data from the main tables:\nemployee table\n\nSELECT *\nFROM employee\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ncustomer table\n\nSELECT *\nFROM customer\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice table\n\nSELECT *\nFROM invoice\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice_line table\n\nSELECT *\nFROM invoice_line\nLIMIT 30\n\n\n\n\n\n  \n\n\n\ntrack table\n\nSELECT *\nFROM track\nLIMIT 20"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "href": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "title": "Answering Business Questions Using SQL",
    "section": "1. Selecting Albums to Purchase",
    "text": "1. Selecting Albums to Purchase\nThe Chinook record store has just signed a deal with a new record label, and you’ve been tasked with selecting the first three albums that will be added to the store, from a list of four. All four albums are by artists that don’t have any tracks in the store right now - we have the artist names, and the genre of music they produce:\n\n\n\nArtist Name\nGenre\n\n\n\n\nRegal\nHip-Hop\n\n\nRed Tone\nPunk\n\n\nMeteor and the Girls\nPop\n\n\nSlim Jim Bites\nBlues\n\n\n\nThe record label specializes in artists from the USA, and they have given Chinook some money to advertise the new albums in the USA, so we’re interested in finding out which genres sell the best in the USA.\nYou’ll need to write a query to find out which genres sell the most tracks in the USA, write up a summary of your findings, and make a recommendation for the three artists whose albums we should purchase for the store.\nInstructions\n\nWrite a query that returns each genre, with the number of tracks sold in the USA:\n\nin absolute numbers\nin percentages.\n\nWrite a paragraph that interprets the data and makes a recommendation for the three artists whose albums we should purchase for the store, based on sales of tracks from their genres.\n\n\nSELECT \n  g.name AS genre,\n  SUM(il.quantity) AS tracks_sold,\n  ROUND(CAST(SUM(il.quantity) AS FLOAT)/\n  (\n    SELECT SUM(il.quantity) \n    FROM invoice i\n    INNER JOIN invoice_line il\n    ON i.invoice_id = il.invoice_id\n    WHERE i.billing_country = 'USA'\n  ) \n  , 4) AS percentage_sold\nFROM invoice i\nINNER JOIN invoice_line il\nON i.invoice_id = il.invoice_id\nINNER JOIN track t \nON il.track_id = t.track_id\nINNER JOIN genre g\nON t.genre_id = g.genre_id\nWHERE i.billing_country = 'USA'\nGROUP BY genre\nORDER BY tracks_sold DESC\n\n\n\n\n\n\ngenre\ntracks_sold\npercentage_sold\n\n\n\n\nRock\n561\n0.5338\n\n\nAlternative & Punk\n130\n0.1237\n\n\nMetal\n124\n0.1180\n\n\nR&B/Soul\n53\n0.0504\n\n\nBlues\n36\n0.0343\n\n\nAlternative\n35\n0.0333\n\n\nPop\n22\n0.0209\n\n\nLatin\n22\n0.0209\n\n\nHip Hop/Rap\n20\n0.0190\n\n\nJazz\n14\n0.0133\n\n\nEasy Listening\n13\n0.0124\n\n\nReggae\n6\n0.0057\n\n\nElectronica/Dance\n5\n0.0048\n\n\nClassical\n4\n0.0038\n\n\nHeavy Metal\n3\n0.0029\n\n\nSoundtrack\n2\n0.0019\n\n\nTV Shows\n1\n0.0010\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_classic())\n\ngenre_of_interest <- c(\"Alternative & Punk\", \"Hip Hop/Rap\", \"Pop\", \"Blues\")\n\ngenre_perc %>% \n  mutate(of_interest = ifelse(genre %in% genre_of_interest, \"yes\", \"no\"),\n         perc_text = scales::percent(percentage_sold, accuracy = 0.1)) %>% \n  ggplot(aes(x=tracks_sold, y=reorder(genre, tracks_sold, sum), fill=of_interest)) + \n  geom_bar(stat = 'identity', width = 0.7) +\n  geom_text(aes(label = perc_text), hjust = -0.2) +\n  labs(title = \"Sold Tracks by Genre, USA\", x = \"Tracks Sold\", y = \"Genre\", \n       fill = \"Genre of Interest\") + \n  scale_fill_manual(values = c(\"gray74\", \"orange\")) +\n  scale_x_continuous(limits = c(0, 600)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe most popular genres in the USA are Rock, Alternative & Punk, and Metal, followed with a big gap by all the others. Since our choice is limited by Hip-Hop, Punk, Pop, and Blues genres, and since we have to choose 3 out of 4 albums, we should purchase the new albums by the following artists:\n\nRed Tone (Punk)\nSlim Jim Bites (Blues)\nMeteor and the Girls (Pop)"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "title": "Answering Business Questions Using SQL",
    "section": "2. Analyzing Employee Sales Performance",
    "text": "2. Analyzing Employee Sales Performance\nEach customer for the Chinook store gets assigned to a sales support agent within the company when they first make a purchase. You have been asked to analyze the purchases of customers belonging to each employee to see if any sales support agent is performing either better or worse than the others.\nYou might like to consider whether any extra columns from the employee table explain any variance you see, or whether the variance might instead be indicative of employee performance.\nInstructions\n\nWrite a query that finds the total dollar amount of sales assigned to each sales support agent within the company. Add any extra attributes for that employee that you find are relevant to the analysis.\nWrite a short statement describing your results, and providing a possible interpretation.\n\n\nSELECT \n  e.first_name || ' ' || e.last_name AS sales_support_agent,\n  e.hire_date,\n  COUNT(DISTINCT c.customer_id) AS customers,\n  SUM(i.total) AS total_sales\nFROM employee e\nINNER JOIN customer c\nON e.employee_id = c.support_rep_id\nINNER JOIN invoice i\nON c.customer_id = i.customer_id\nGROUP BY sales_support_agent\n\n\n3 records\n\n\nsales_support_agent\nhire_date\ncustomers\ntotal_sales\n\n\n\n\nJane Peacock\n2017-04-01 00:00:00\n21\n1731.51\n\n\nMargaret Park\n2017-05-03 00:00:00\n20\n1584.00\n\n\nSteve Johnson\n2017-10-17 00:00:00\n18\n1393.92\n\n\n\n\n\nWhile there is a 20% difference in sales between Jane (the top employee) and Steve (the bottom employee), the difference roughly corresponds with the differences in their hiring dates."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "title": "Answering Business Questions Using SQL",
    "section": "3. Analyzing Sales by Country",
    "text": "3. Analyzing Sales by Country\nYour next task is to analyze the sales data for customers from each different country. You have been given guidance to use the country value from the customers table, and ignore the country from the billing address in the invoice table.\nInstructions\n\nWrite a query that collates data on purchases from different countries.\nWhere a country has only one customer, collect them into an “Other” group.\nThe results should be sorted by the total sales from highest to lowest, with the “Other” group at the very bottom.\nFor each country, include:\n\ntotal number of customers\ntotal value of sales\naverage value of sales per customer\naverage order value\n\n\n\nWITH t1 AS (\n  SELECT\n    CASE\n      WHEN COUNT(DISTINCT c.customer_id) = 1 THEN 'Other'\n      ELSE c.country\n      END AS country,\n    COUNT(DISTINCT c.customer_id) AS customers,\n    SUM(i.total) AS total_sales,\n    SUM(i.total)/COUNT(DISTINCT c.customer_id) AS avg_sales_per_cust,\n    AVG(i.total) AS avg_order\n  FROM customer c\n  INNER JOIN invoice i\n  ON c.customer_id = i.customer_id\n  GROUP BY country\n)\n\nSELECT \n  country,\n  SUM(customers) AS customers,\n  SUM(total_sales) AS total_sales,\n  AVG(avg_sales_per_cust) AS avg_sales_per_cust,\n  AVG(avg_order) AS avg_order\nFROM \n  (\n    SELECT\n      t1.*,\n      CASE \n        WHEN country = 'Other' THEN 1\n        ELSE 0\n        END AS sorted\n    FROM t1\n  )\nGROUP BY country\nORDER BY sorted, total_sales DESC\n\n\n\n\n\n\ncountry\ncustomers\ntotal_sales\navg_sales_per_cust\navg_order\n\n\n\n\nUSA\n13\n1040.49\n80.03769\n7.942672\n\n\nCanada\n8\n535.59\n66.94875\n7.047237\n\n\nBrazil\n5\n427.68\n85.53600\n7.011147\n\n\nFrance\n5\n389.07\n77.81400\n7.781400\n\n\nGermany\n4\n334.62\n83.65500\n8.161463\n\n\nCzech Republic\n2\n273.24\n136.62000\n9.108000\n\n\nUnited Kingdom\n3\n245.52\n81.84000\n8.768571\n\n\nPortugal\n2\n185.13\n92.56500\n6.383793\n\n\nIndia\n2\n183.15\n91.57500\n8.721429\n\n\nOther\n15\n1094.94\n72.99600\n7.445071\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, customers, total_sales) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(customers = customers/sum(customers),\n         total_sales = total_sales/sum(total_sales)) %>%\n  pivot_longer(-country, names_to = \"variable\", values_to = \"value\") %>% \n  ggplot(aes(x=value, y=country, fill=variable)) +\n  geom_bar(stat = \"identity\", width = 0.65, position = position_dodge(0.8)) +\n  labs(title=\"Share of Customers and Sales by Country\", x=\"share\", fill=\"\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"gray77\", \"seagreen3\")) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, avg_order, avg_sales_per_cust) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(avg_order = (avg_order - mean(avg_order)) / mean(avg_order),\n         avg_sales_per_cust = (avg_sales_per_cust - mean(avg_sales_per_cust)) / \n                                 mean(avg_sales_per_cust)) %>%\n  rename(`Average Order` = avg_order,\n         `Average Sales per Customer` = avg_sales_per_cust) %>% \n  pivot_longer(-country, names_to = \"variable\", values_to = \"pct_diff_from_mean\") %>% \n  ggplot(aes(x=pct_diff_from_mean, y=country, fill=variable)) +\n  geom_col(width = 0.65, position = position_dodge(0.8)) +\n  facet_wrap(~ variable, scales = \"free_x\") + \n  labs(title=\"Average Order & Average Sales per Customer\", \n       subtitle = \"(Percent Difference from Mean)\", x=\"pct diff from mean\", fill=\"\") +\n  scale_fill_manual(values = c(\"steelblue\", \"lightskyblue2\")) +\n  scale_x_continuous(labels = scales::percent) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe USA has the largest customer base and, consequently, the highest total sales.\nBased on the data, there may be opportunity in the following countries:\n\nCzech Republic\nUnited Kingdom\nIndia\n\nIt’s worth keeping in mind that the amount of data from each of these countries is relatively low. Because of this, we should be cautious spending too much money on new marketing campaigns, as the sample size is not large enough to give us high confidence. A better approach would be to run small campaigns in these countries, collecting and analyzing the new customers to make sure that these trends hold with new customers."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "href": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "title": "Answering Business Questions Using SQL",
    "section": "4. Albums vs Individual Tracks",
    "text": "4. Albums vs Individual Tracks\nThe Chinook store is setup in a way that allows customer to make purchases in one of the two ways:\n\npurchase a whole album\npurchase a collection of one or more individual tracks.\n\nThe store does not let customers purchase a whole album, and then add individual tracks to that same purchase (unless they do that by choosing each track manually). When customers purchase albums they are charged the same price as if they had purchased each of those tracks separately.\nManagement are currently considering changing their purchasing strategy to save money. The strategy they are considering is to purchase only the most popular tracks from each album from record companies, instead of purchasing every track from an album.\nWe have been asked to find out what percentage of purchases are individual tracks vs whole albums, so that management can use this data to understand the effect this decision might have on overall revenue.\nInstructions\n\nWrite a query that categorizes each invoice as either an album purchase or not, and calculates the following summary statistics:\n\nNumber of invoices\nPercentage of invoices\n\nWrite one to two sentences explaining your findings, and making a prospective recommendation on whether the Chinook store should continue to buy full albums from record companies\n\n\nWITH cat_purchase AS (\n  SELECT\n    il.invoice_id,\n    CASE\n      WHEN\n      COUNT(DISTINCT t.album_id) = 1\n      AND \n      COUNT(DISTINCT t.track_id) = c.count_album_tracks\n      THEN 'album'\n      ELSE 'individual track(s)'\n      END AS purchase_type,\n      c.count_album_tracks\n    FROM track t\n    JOIN invoice_line il\n    ON il.track_id = t.track_id\n    JOIN (SELECT COUNT(*) AS count_album_tracks, album_id\n          FROM track\n          GROUP BY album_id) c\n    ON c.album_id = t.album_id\n    GROUP BY invoice_id\n)\n\nSELECT\n  purchase_type,\n  COUNT(*) AS number_of_invoices,\n  ROUND(CAST(COUNT(*) AS float) / CAST(\n    (SELECT COUNT(*)\n    FROM invoice) AS FLOAT), 2) AS percentage_of_invoices\nFROM cat_purchase\nGROUP BY purchase_type\n\n\n2 records\n\n\npurchase_type\nnumber_of_invoices\npercentage_of_invoices\n\n\n\n\nalbum\n114\n0.19\n\n\nindividual track(s)\n500\n0.81\n\n\n\n\n\nAlbum purchases account for 19% of all purchases. Based on this data, I would recommend against purchasing only the most popular tracks from each album from record companies, since there is a potential of losing a significant portion of revenue."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "href": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "title": "Answering Business Questions Using SQL",
    "section": "5. Which artist is used in the most playlists?",
    "text": "5. Which artist is used in the most playlists?\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY number_of_playlists DESC, unique_tracks DESC\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nEugene Ormandy\nClassical\n7\n3\n\n\nBerliner Philharmoniker & Herbert Von Karajan\nClassical\n6\n3\n\n\nThe King’s Singers\nClassical\n6\n2\n\n\nEnglish Concert & Trevor Pinnock\nClassical\n6\n2\n\n\nAcademy of St. Martin in the Fields & Sir Neville Marriner\nClassical\n6\n2\n\n\nMichael Tilson Thomas & San Francisco Symphony\nClassical\n5\n2\n\n\nYo-Yo Ma\nClassical\n5\n1\n\n\nWilhelm Kempff\nClassical\n5\n1\n\n\nTon Koopman\nClassical\n5\n1\n\n\nSir Georg Solti, Sumi Jo & Wiener Philharmoniker\nOpera\n5\n1\n\n\n\n\n\nEugene Ormandy takes the first place with only 3 unique tracks in 7 different playlists. His music belongs to the classical genre, which we have previously seen is one of the least popular genres in the USA.\nIf we order this table by number of unique tracks, we get a completely different list.\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY unique_tracks DESC, number_of_playlists DESC\n\n\nDisplaying records 1 - 10\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nLed Zeppelin\nRock\n3\n114\n\n\nMetallica\nMetal\n4\n112\n\n\nU2\nRock\n3\n112\n\n\nIron Maiden\nMetal\n4\n95\n\n\nDeep Purple\nRock\n3\n92\n\n\nIron Maiden\nRock\n3\n81\n\n\nPearl Jam\nRock\n4\n54\n\n\nVan Halen\nRock\n3\n52\n\n\nOs Paralamas Do Sucesso\nLatin\n3\n49\n\n\nLost\nTV Shows\n2\n48"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "href": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "title": "Answering Business Questions Using SQL",
    "section": "6. How many tracks have been purchased vs not purchased?",
    "text": "6. How many tracks have been purchased vs not purchased?\n\nWITH all_and_purchased_tracks AS (\n  SELECT \n    t.track_id AS all_tracks,\n    il.track_id AS purch_tracks\n  FROM track t\n  LEFT JOIN invoice_line il\n  ON t.track_id = il.track_id\n)\n  \nSELECT\n  COUNT(DISTINCT all_tracks) AS total_tracks,\n  COUNT(DISTINCT purch_tracks) AS pirchased,\n  COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) AS not_purchased,\n  ROUND(CAST(COUNT(DISTINCT purch_tracks) AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_purchased,\n  ROUND(CAST(COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) \n    AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_not_purchased\nFROM all_and_purchased_tracks\n\n\n1 records\n\n\n\n\n\n\n\n\n\ntotal_tracks\npirchased\nnot_purchased\nperc_purchased\nperc_not_purchased\n\n\n\n\n3503\n1806\n1697\n0.516\n0.484\n\n\n\n\n\n\n\nCode\npie(c(51.6, 48.4), labels = c(\"purchased\", \"not purchased\"), \n    main = \"Purchased vs not purchased tracks\")\n\n\n\n\n\nAlmost half of all the unique tracks available in the Chinook store were never bought, probably being of unpopular genre or unpopular artists."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "href": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "title": "Answering Business Questions Using SQL",
    "section": "7. Do protected vs non-protected media types have an effect on popularity?",
    "text": "7. Do protected vs non-protected media types have an effect on popularity?\nLet’s take a look at the media_type table.\n\nSELECT *\nFROM media_type\n\n\n5 records\n\n\nmedia_type_id\nname\n\n\n\n\n1\nMPEG audio file\n\n\n2\nProtected AAC audio file\n\n\n3\nProtected MPEG-4 video file\n\n\n4\nPurchased AAC audio file\n\n\n5\nAAC audio file\n\n\n\n\n\nThere are 2 out of 5 media types that are protected.\n\nWITH t AS (\n  SELECT \n    t.track_id,\n    CASE\n      WHEN mt.name LIKE \"%protected%\" THEN \"yes\" ELSE \"no\"\n      END AS protected\n  FROM track t\n  JOIN media_type mt ON mt.media_type_id = t.media_type_id\n)\n\nSELECT \n  protected,\n  COUNT(DISTINCT t.track_id) AS unique_tracks,\n  ROUND(CAST(COUNT(DISTINCT t.track_id) AS FLOAT) / (SELECT COUNT(*) FROM track), 2)\n    AS 'unique_tracks_%',\n  COUNT(DISTINCT il.track_id) AS sold_unique_tracks,\n  COUNT(il.track_id) AS sold_tracks,\n  ROUND(CAST(COUNT(il.track_id) AS FLOAT) / (SELECT COUNT(*) FROM invoice_line), 2)\n    AS 'sold_tracks_%'\nFROM t\nLEFT JOIN invoice_line il ON t.track_id = il.track_id\nGROUP BY protected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprotected\nunique_tracks\nunique_tracks_%\nsold_unique_tracks\nsold_tracks\nsold_tracks_%\n\n\n\n\nno\n3052\n0.87\n1652\n4315\n0.91\n\n\nyes\n451\n0.13\n154\n442\n0.09\n\n\n\n\n\n\n\nCode\nby_media_type %>% \n  select(protected, unique_tracks, sold_tracks) %>% \n  rename(unique = unique_tracks, \n         sold = sold_tracks) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  mutate(tracks = as.factor(tracks) %>% fct_rev()) %>% \n  group_by(tracks) %>% \n  mutate(pct = count/sum(count) %>% round(2)) %>% \n  ungroup() %>% \n  ggplot(aes(x=tracks, y=count, fill=protected)) +\n  geom_col(width = 0.5, position = \"stack\", color = \"white\") +\n  geom_text(aes(label = count), position = position_stack(vjust = .5),\n            color = \"white\", fontface = \"bold\")+\n  scale_fill_manual(values = c(\"#fbc02d\", \"#03a9f4\")) +\n  theme(legend.position = \"top\")\nby_media_type %>% \n  select(protected, unique_tracks, sold_unique_tracks) %>% \n  rename(unique = unique_tracks, \n         unique_sold = sold_unique_tracks) %>% \n  mutate(unique_unsold = unique - unique_sold) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  filter(tracks != \"unique\") %>% \n  group_by(protected) %>% \n  mutate(percentage = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n  ggplot(aes(x=protected, y=count, fill=tracks)) +\n  geom_col(width = 0.5, position = \"fill\", color = \"white\") +\n  scale_fill_manual(values = c(\"seagreen3\", \"tomato\")) +\n  geom_text(aes(label = percentage), position = position_fill(vjust = .5), \n            color = \"white\", fontface = \"bold\") +\n  labs(y=\"proportion\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can make the following observations:\n\nOnly 13% of all the unique tracks available in the Chinook store are of protected media types.\nAmong all the tracks that were sold, those of protected media types amounts only to 9%.\nFrom all the unique tracks of protected media types, only 34,1% were sold, while from those of non-protected ones 54,1%.\n\nWe can conclude that the tracks of protected media types are much less popular than those of non-protected."
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "",
    "text": "The Bike Sales Database represents a bicycle manufacturer, including tables for products (bikes), customers (bike shops), and transactions (orders).\nIt consists of 3 tables:\n\nbikes table, which includes bicycle models, descriptions, and unit prices that are produced by the manufacturer.\nbikeshops table, which includes customers that the bicycle manufacturer has sold to.\norderlines table, which includes transactional data such as order ID, order line, date, customer, product, and quantity sold.\n\nbike_sales database is the local Postgres database stored on my machine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\nlibrary(lubridate)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database.\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"bike_sales\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> bike_sales@localhost:5432\n\n\n\n# list the database table names\ndbListTables(mycon)\n\n[1] \"bikeshops\"  \"bikes\"      \"orderlines\"\n\n\n\n# read the bikeshops table\ndbReadTable(mycon, \"bikeshops\") %>% head()\n\n  bikeshop.id                bikeshop.name       location\n1           1 Pittsburgh Mountain Machines Pittsburgh, PA\n2           2     Ithaca Mountain Climbers     Ithaca, NY\n3           3      Columbus Race Equipment   Columbus, OH\n4           4               Detroit Cycles    Detroit, MI\n5           5             Cincinnati Speed Cincinnati, OH\n6           6    Louisville Race Equipment Louisville, KY\n\n\n\n# read the bikes table\ndbReadTable(mycon, \"bikes\") %>% head()\n\n  bike.id                          model                description price\n1       1        Supersix Evo Black Inc. Road - Elite Road - Carbon 12790\n2       2       Supersix Evo Hi-Mod Team Road - Elite Road - Carbon 10660\n3       3 Supersix Evo Hi-Mod Dura Ace 1 Road - Elite Road - Carbon  7990\n4       4 Supersix Evo Hi-Mod Dura Ace 2 Road - Elite Road - Carbon  5330\n5       5     Supersix Evo Hi-Mod Utegra Road - Elite Road - Carbon  4260\n6       6               Supersix Evo Red Road - Elite Road - Carbon  3940\n\n\n\n# read the orderlines table\ndbReadTable(mycon, \"orderlines\") %>% head()\n\n  order.id order.line order.date customer.id product.id quantity\n1        1          1 2011-01-07           2         48        1\n2        1          2 2011-01-07           2         52        1\n3        2          1 2011-01-10          10         76        1\n4        2          2 2011-01-10          10         52        1\n5        3          1 2011-01-10           6          2        1\n6        3          2 2011-01-10           6         50        1\n\n\n\n# a simple query example\ndbGetQuery(mycon, \n          \"SELECT model, price \n           FROM bikes WHERE price > 10000 \n           ORDER BY price DESC\")\n\n                     model price\n1  Supersix Evo Black Inc. 12790\n2    Scalpel-Si Black Inc. 12790\n3  Habit Hi-Mod Black Inc. 12250\n4          F-Si Black Inc. 11190\n5 Supersix Evo Hi-Mod Team 10660\n\n\n\n\n\nIn all three tables there are dots in column names. This is not a good practice and I first had to figure out how to join the tables without an error! Here is the solution:\n\nbike_orderlines_joined <- dbGetQuery(mycon, \n\n'SELECT * \nFROM orderlines \nLEFT JOIN bikes\nON orderlines.\"product.id\" = bikes.\"bike.id\"\nLEFT JOIN bikeshops\nON orderlines.\"customer.id\" = bikeshops.\"bikeshop.id\"')\n\nhead(bike_orderlines_joined)\n\n  order.id order.line order.date customer.id product.id quantity bike.id\n1        1          1 2011-01-07           2         48        1      48\n2        1          2 2011-01-07           2         52        1      52\n3        2          1 2011-01-10          10         76        1      76\n4        2          2 2011-01-10          10         52        1      52\n5        3          1 2011-01-10           6          2        1       2\n6        3          2 2011-01-10           6         50        1      50\n                     model                       description price bikeshop.id\n1          Jekyll Carbon 2 Mountain - Over Mountain - Carbon  6070           2\n2         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970           2\n3      Beast of the East 1       Mountain - Trail - Aluminum  2770          10\n4         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970          10\n5 Supersix Evo Hi-Mod Team        Road - Elite Road - Carbon 10660           6\n6          Jekyll Carbon 4 Mountain - Over Mountain - Carbon  3200           6\n              bikeshop.name        location\n1  Ithaca Mountain Climbers      Ithaca, NY\n2  Ithaca Mountain Climbers      Ithaca, NY\n3         Kansas City 29ers Kansas City, KS\n4         Kansas City 29ers Kansas City, KS\n5 Louisville Race Equipment  Louisville, KY\n6 Louisville Race Equipment  Louisville, KY\n\n\n\nglimpse(bike_orderlines_joined)\n\nRows: 15,644\nColumns: 13\n$ order.id      <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,…\n$ order.line    <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1,…\n$ order.date    <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-01…\n$ customer.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ product.id    <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ quantity      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,…\n$ bike.id       <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ model         <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Eas…\n$ description   <chr> \"Mountain - Over Mountain - Carbon\", \"Mountain - Over Mo…\n$ price         <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570, …\n$ bikeshop.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ bikeshop.name <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\", …\n$ location      <chr> \"Ithaca, NY\", \"Ithaca, NY\", \"Kansas City, KS\", \"Kansas C…\n\n\nDisconnecting from the database.\n\ndbDisconnect(mycon)\n\n\n\n\n\nbike_orderlines <- bike_orderlines_joined %>% \n  # rename columns - replacing \".\" with \"_\"\n  set_names(names(.) %>% str_replace_all(\"\\\\.\", \"_\")) %>% \n  # remove the unnecessary columns \n  select(-c(customer_id, product_id, bike_id, bikeshop_id)) %>% \n  # separate description into category_1, category_2, and frame_material\n  separate(description, \n           c(\"category_1\", \"category_2\", \"frame_material\"),\n           sep = \" - \") %>% \n  # separate location into city and state\n  separate(location,\n           c(\"city\", \"state\"),\n           sep = \", \") %>%\n  # create a new column total_price\n  mutate(total_price = price * quantity) %>% \n  # reorder columns\n  select(contains(c(\"date\", \"id\", \"order\")),\n         quantity, price, total_price,\n         everything()) \n\nbike_orderlines %>% head()\n\n  order_date order_id order_line quantity price total_price\n1 2011-01-07        1          1        1  6070        6070\n2 2011-01-07        1          2        1  5970        5970\n3 2011-01-10        2          1        1  2770        2770\n4 2011-01-10        2          2        1  5970        5970\n5 2011-01-10        3          1        1 10660       10660\n6 2011-01-10        3          2        1  3200        3200\n                     model category_1    category_2 frame_material\n1          Jekyll Carbon 2   Mountain Over Mountain         Carbon\n2         Trigger Carbon 2   Mountain Over Mountain         Carbon\n3      Beast of the East 1   Mountain         Trail       Aluminum\n4         Trigger Carbon 2   Mountain Over Mountain         Carbon\n5 Supersix Evo Hi-Mod Team       Road    Elite Road         Carbon\n6          Jekyll Carbon 4   Mountain Over Mountain         Carbon\n              bikeshop_name        city state\n1  Ithaca Mountain Climbers      Ithaca    NY\n2  Ithaca Mountain Climbers      Ithaca    NY\n3         Kansas City 29ers Kansas City    KS\n4         Kansas City 29ers Kansas City    KS\n5 Louisville Race Equipment  Louisville    KY\n6 Louisville Race Equipment  Louisville    KY\n\n\n\nbike_orderlines %>% glimpse()\n\nRows: 15,644\nColumns: 13\n$ order_date     <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-0…\n$ order_id       <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7…\n$ order_line     <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1…\n$ quantity       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1…\n$ price          <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ total_price    <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ model          <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Ea…\n$ category_1     <chr> \"Mountain\", \"Mountain\", \"Mountain\", \"Mountain\", \"Road\",…\n$ category_2     <chr> \"Over Mountain\", \"Over Mountain\", \"Trail\", \"Over Mounta…\n$ frame_material <chr> \"Carbon\", \"Carbon\", \"Aluminum\", \"Carbon\", \"Carbon\", \"Ca…\n$ bikeshop_name  <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\",…\n$ city           <chr> \"Ithaca\", \"Ithaca\", \"Kansas City\", \"Kansas City\", \"Loui…\n$ state          <chr> \"NY\", \"NY\", \"KS\", \"KS\", \"KY\", \"KY\", \"KY\", \"KY\", \"KY\", \"…"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Lollipop Chart: Top N Customers",
    "text": "Lollipop Chart: Top N Customers\nQuestion: How much purchasing power is in top 10 customers?\nGoal is to visualize top N customers in terms of Revenue, including cumulative percentage.\n\nData manipulation\n\nn <- 10\n\ntop_customers <- bike_orderlines %>% \n  select(bikeshop_name, total_price) %>% \n  mutate(bikeshop_name = as_factor(bikeshop_name) %>% fct_lump_n(n = n, w = total_price)) %>% \n  group_by(bikeshop_name) %>% \n  summarise(revenue = sum(total_price)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_reorder(revenue)) %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_relevel(\"Other\", after = 0)) %>% \n  arrange(desc(bikeshop_name)) %>% \n  # revenue text\n  mutate(revenue_text = scales::dollar(revenue, scale = 1e-06, suffix = \"M\")) %>% \n  # cumulative percent\n  mutate(cum_pct = cumsum(revenue) / sum(revenue)) %>% \n  mutate(cum_pct_text = scales::percent(cum_pct)) %>% \n  # rank\n  mutate(rank = row_number()) %>% \n  mutate(rank = ifelse(rank == max(rank), NA_integer_, rank)) %>% \n  # label text\n  mutate(label_text = str_glue(\"Rank: {rank}\\nRev: {revenue_text}\\nCumPct: {cum_pct_text}\")) \n\ntop_customers\n\n# A tibble: 11 × 7\n   bikeshop_name                 revenue revenue…¹ cum_pct cum_p…²  rank label…³\n   <fct>                           <dbl> <chr>       <dbl> <chr>   <int> <glue> \n 1 Kansas City 29ers            11535455 $11.54M     0.162 16.2%       1 Rank: …\n 2 Denver Bike Shop              7697670 $7.70M      0.271 27.1%       2 Rank: …\n 3 Ithaca Mountain Climbers      6299335 $6.30M      0.359 35.9%       3 Rank: …\n 4 Phoenix Bi-peds               4168535 $4.17M      0.418 41.8%       4 Rank: …\n 5 Oklahoma City Race Equipment  3450040 $3.45M      0.467 46.7%       5 Rank: …\n 6 Las Vegas Cycles              3073615 $3.07M      0.510 51.0%       6 Rank: …\n 7 New Orleans Velocipedes       2761825 $2.76M      0.549 54.9%       7 Rank: …\n 8 Wichita Speed                 2380385 $2.38M      0.582 58.2%       8 Rank: …\n 9 Miami Race Equipment          2057130 $2.06M      0.611 61.1%       9 Rank: …\n10 Minneapolis Bike Shop         2023220 $2.02M      0.640 64.0%      10 Rank: …\n11 Other                        25585120 $25.59M     1     100.0%     NA Rank: …\n# … with abbreviated variable names ¹​revenue_text, ²​cum_pct_text, ³​label_text\n\n\n\n\nData visualization\n\ntop_customers %>% \n  ggplot(aes(revenue, bikeshop_name)) +\n  # geometries\n  geom_segment(aes(xend = 0, yend = bikeshop_name), \n               color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1],\n               size = 1) +\n  geom_point(color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1], \n             size = 3) +\n  geom_label(aes(label = label_text), \n             hjust = \"left\", \n             size = 3,\n             nudge_x = 0.8e+06) +\n  # formatting\n  scale_x_continuous(labels = scales::dollar_format(scale = 1e-06, suffix = \"M\")) +\n  labs(title = str_glue(\"Top {n} customers in terms of revenue, with cumulative percentage\"),\n       subtitle = str_glue(\"Top {n} customers contribute {top_customers$cum_pct_text[n]} of purchasing power.\"),\n       x = \"Revenue ($M)\",\n       y = \"Customer\",\n       caption = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\")) +\n  expand_limits(x = max(top_customers$revenue) + 6e+06) +\n  # theme\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Heatmap: Customers’ Purchasing Habits",
    "text": "Heatmap: Customers’ Purchasing Habits\nQuestion: Do specific customers have a purchasing preference?\nGoal is to visualize heatmap of proportion of sales by Secondary Product Category.\n\nData manipulation\n\npct_sales_by_customer <- bike_orderlines %>% \n  select(bikeshop_name, category_1, category_2, quantity) %>% \n  group_by(bikeshop_name, category_1, category_2) %>% \n  summarise(total_qty = sum(quantity)) %>% \n  ungroup() %>% \n  group_by(bikeshop_name) %>% \n  mutate(pct = total_qty / sum(total_qty)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = as.factor(bikeshop_name) %>% fct_rev()) %>%  \n  mutate(bikeshop_name_num = as.numeric(bikeshop_name))\n    \npct_sales_by_customer   \n\n# A tibble: 270 × 6\n   bikeshop_name      category_1 category_2         total_qty    pct bikeshop_…¹\n   <fct>              <chr>      <chr>                  <dbl>  <dbl>       <dbl>\n 1 Albuquerque Cycles Mountain   Cross Country Race        48 0.168           30\n 2 Albuquerque Cycles Mountain   Fat Bike                   9 0.0315          30\n 3 Albuquerque Cycles Mountain   Over Mountain             13 0.0455          30\n 4 Albuquerque Cycles Mountain   Sport                     35 0.122           30\n 5 Albuquerque Cycles Mountain   Trail                     38 0.133           30\n 6 Albuquerque Cycles Road       Cyclocross                 7 0.0245          30\n 7 Albuquerque Cycles Road       Elite Road                69 0.241           30\n 8 Albuquerque Cycles Road       Endurance Road            54 0.189           30\n 9 Albuquerque Cycles Road       Triathalon                13 0.0455          30\n10 Ann Arbor Speed    Mountain   Cross Country Race        32 0.0532          29\n# … with 260 more rows, and abbreviated variable name ¹​bikeshop_name_num\n\n\n\n\nData visualization\n\npct_sales_by_customer %>% \n  ggplot(aes(category_2, bikeshop_name)) + \n  # geometries\n  geom_tile(aes(fill = pct)) +\n  geom_text(aes(label = scales::percent(pct, accuracy = 0.1)),\n            size = 3,\n            color = ifelse(pct_sales_by_customer$pct >= 0.15, \"white\", \"black\")) +\n  facet_wrap(~ category_1, scales = \"free_x\") + \n  # formatting\n  scale_fill_gradient(low = \"white\", high = tidyquant::palette_light()[1]) + \n  labs(title = \"Heatmap of Purchasing Habits\", \n       subtitle = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\"),\n       x = \"Bike Type\",\n       y = \"Customer\") + \n  # theme\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = tidyquant::palette_light()[1], \n                                        color = \"white\"), \n        strip.text = element_text(color = \"white\", size = 11), \n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\nTop 3 customers that prefer mountain bikes:\n\nIthaca Mountain Climbers\nPittsburgh Mountain Machines\nTampa 29ers\n\nTop 3 customers that prefer road bikes:\n\nAnn Arbor Speed\nAustin Cruisers\nIndianapolis Velocipedes\n\n\nThat’s it! I hope you like it. For those wondering where I learned to make plots like this… in a fabulous course Data Science for Business Part 1 by Matt Dancho. This is probably the best course on R and I highly recommend it."
  },
  {
    "objectID": "posts/shiny-test/shiny-test.html",
    "href": "posts/shiny-test/shiny-test.html",
    "title": "Old Faithful",
    "section": "",
    "text": "Number of bins:"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "title": "Titanic Survival Exercises",
    "section": "",
    "text": "After auditing the HarvardX’s Data Science: Visualization course I’ve found this assessment way too interesting and fun. So I decided to put all my new skills together to perform exploratory data analysis on a classic machine learning dataset: Titanic survival! My goal is to provide answers entirely through visualizations."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "title": "Titanic Survival Exercises",
    "section": "Background",
    "text": "Background\nThe Titanic was a British ocean liner that struck an iceberg and sunk on its maiden voyage in 1912 from the United Kingdom to New York. More than 1,500 of the estimated 2,224 passengers and crew died in the accident, making this one of the largest maritime disasters ever outside of war. The ship carried a wide range of passengers of all ages and both genders, from luxury travelers in first-class to immigrants in the lower classes. However, not all passengers were equally likely to survive the accident. We use real data about a selection of 891 passengers to learn who was on the Titanic and which passengers were more likely to survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "title": "Titanic Survival Exercises",
    "section": "Libraries, Customizations, and Data",
    "text": "Libraries, Customizations, and Data\n\nlibrary(tidyverse)\nlibrary(titanic)\n\noptions(digits = 3)  \ntheme_set(theme_classic())\ncolors_sex <- c(\"mediumorchid1\", \"dodgerblue\")\ncolors_survived <- c(\"gray65\", \"lightgreen\")\n\nDefining the titanic dataset.\n\ntitanic <- titanic_train %>%\n  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare) %>%\n  mutate(Survived = factor(Survived),\n         Pclass = factor(Pclass),\n         Sex = factor(Sex))\n\nhead(titanic)\n\n  Survived Pclass    Sex Age SibSp Parch  Fare\n1        0      3   male  22     1     0  7.25\n2        1      1 female  38     1     0 71.28\n3        1      3 female  26     0     0  7.92\n4        1      1 female  35     1     0 53.10\n5        0      3   male  35     0     0  8.05\n6        0      3   male  NA     0     0  8.46\n\nstr(titanic)\n\n'data.frame':   891 obs. of  7 variables:\n $ Survived: Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass  : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Sex     : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age     : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp   : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch   : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Fare    : num  7.25 71.28 7.92 53.1 8.05 ..."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "title": "Titanic Survival Exercises",
    "section": "Question 1: Variable Types",
    "text": "Question 1: Variable Types\nInstructions: Inspect the data and also use ?titanic_train to learn more about the variables in the dataset. Match these variables from the dataset to their variable type. There is at least one variable of each type (ordinal categorical, non-ordinal (nominal) categorical, continuous, discrete).\nChecking if Age variable is discrete or continuous…\n\nunique(titanic$Age)\n\n [1] 22.00 38.00 26.00 35.00    NA 54.00  2.00 27.00 14.00  4.00 58.00 20.00\n[13] 39.00 55.00 31.00 34.00 15.00 28.00  8.00 19.00 40.00 66.00 42.00 21.00\n[25] 18.00  3.00  7.00 49.00 29.00 65.00 28.50  5.00 11.00 45.00 17.00 32.00\n[37] 16.00 25.00  0.83 30.00 33.00 23.00 24.00 46.00 59.00 71.00 37.00 47.00\n[49] 14.50 70.50 32.50 12.00  9.00 36.50 51.00 55.50 40.50 44.00  1.00 61.00\n[61] 56.00 50.00 36.00 45.50 20.50 62.00 41.00 52.00 63.00 23.50  0.92 43.00\n[73] 60.00 10.00 64.00 13.00 48.00  0.75 53.00 57.00 80.00 70.00 24.50  6.00\n[85]  0.67 30.50  0.42 34.50 74.00\n\n\nAge is a continuous variable.\n\n\n\nVariable\nDescription\nVariable Type\n\n\n\n\nSurvived\nPassenger Survival Indicator\nnominal categorical\n\n\nPclass\nPassenger Class\nordinal categorical\n\n\nSex\nSex\nnominal categorical\n\n\nAge\nAge\ncontinuous\n\n\nSibSp\nNumber of Siblings/Spouses Aboard\ndiscrete\n\n\nParch\nNumber of Parents/Children Aboard\ndiscrete\n\n\nFare\nPassenger Fare\ncontinuous"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "title": "Titanic Survival Exercises",
    "section": "Question 2: Demographics of Titanic Passengers",
    "text": "Question 2: Demographics of Titanic Passengers\nInstructions: Make density plots of age grouped by sex. Try experimenting with combinations of faceting, alpha blending, stacking and using variable counts on the y-axis to answer the following questions. Some questions may be easier to answer with different versions of the density plot.\n\ntitanic %>% \n  ggplot(aes(Age)) +\n  geom_density(aes(color = Sex), size = 0.7) +\n  scale_color_manual(values = colors_sex) +\n  geom_vline(xintercept = c(18, 35), linetype = 2) +\n  geom_text(aes(x = 18, y = 0.031, label= \"18\", hjust = 1.5)) +\n  geom_text(aes(x = 35, y = 0.031, label= \"35\", hjust = -0.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, fill = Sex)) +\n  geom_density(alpha = 0.3) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 17, linetype = 2) +\n  geom_text(aes(x = 17, y = 0.031, label= \"17\", hjust = 1.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Sex)) +\n  geom_density(alpha = 0.7) +\n  facet_grid(Sex ~ .) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 40, linetype = 2) + \n  geom_text(aes(x = 40, y = 14, label= \"40\", hjust = -0.5)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers\n\n✅ Females and males had the same general shape of age distribution.\n✅ } The age distribution was bimodal, with one mode around 25 years of age and a second - - smaller mode around 5 years of age.\n❌ There were more females than males.\n✅ The count of males of age 40 was higher than the count of females of age 40.\n✅ The proportion of males age 18-35 was higher than the proportion of females age 18-35.\n✅ The proportion of females under age 17 was higher than the proportion of males under age 17.\n❌ The oldest passengers were female."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "title": "Titanic Survival Exercises",
    "section": "Question 3: QQ-plot of Age Distribution",
    "text": "Question 3: QQ-plot of Age Distribution\nInstructions: Use geom_qq() to make a QQ-plot of passenger age and add an identity line with geom_abline(). Filter out any individuals with an age of NA first.\n\nparams <- titanic %>%\n  filter(!is.na(Age)) %>%\n  summarize(mean = mean(Age), sd = sd(Age))\n\nparams\n\n  mean   sd\n1 29.7 14.5\n\ntitanic %>% ggplot(aes(sample = Age)) + \n  geom_qq(dparams = params) +\n  geom_abline()\n\n\n\n\nWhich of the following is the correct plot according to the instructions above?\n\n✅ The plot above."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "title": "Titanic Survival Exercises",
    "section": "Question 4: Survival by Sex",
    "text": "Question 4: Survival by Sex\nInstructions: To answer the following questions, make barplots of the Survived and Sex variables using geom_bar(). Try plotting one variable and filling by the other variable. You may want to try the default plot, then try adding position = position_dodge() to geom_bar() to make separate bars for each group.\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Sex)) +\n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_sex)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(width =  0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Less than half of passengers survived.\n✅ Most of the survivors were female.\n❌ Most of the males survived.\n✅ Most of the females survived."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "title": "Titanic Survival Exercises",
    "section": "Question 5: Survival by Age",
    "text": "Question 5: Survival by Age\nInstructions: Make a density plot of age filled by survival status. Change the y-axis to count and set alpha = 0.2.\nThe following answers were offered for all three questions:\n\n0-8\n10-18\n18-30\n30-50\n50-70\n70-80\n\n\nWhich age group is the only group more likely to survive than die?\n\ntitanic %>% \n  ggplot(aes(Age, y = ..count.., fill = Survived)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = colors_survived) +\n  geom_vline(xintercept = 8, linetype = 2) +\n  geom_text(aes(x = 8, y = 14, label= \"8\", hjust = -0.5))\n\n\n\n\n\n✅ Age group 0-8.\n\n\n\nWhich age group had the most deaths?\nIt’s hard to tell from the previews plot I’ll have to make a new column Age group based on the offered answers.\n\ntitanic2 <- titanic %>%\n  filter(!is.na(Age)) %>% \n  mutate(`Age group` = case_when(\n    Age >0 & Age <=8 ~ \"0-8\",\n    Age >=10 & Age <=18 ~ \"10-18\",\n    Age >=18 & Age <=30 ~ \"18-30\",\n    Age >=30 & Age <=50 ~ \"30-50\",\n    Age >=50 & Age <=70 ~ \"50-70\",\n    Age >=70 & Age <=80 ~ \"70-80\"))\n\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 18-30.\n\n\n\nWhich age group had the highest proportion of deaths?\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(position = \"fill\", width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 70-80"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "title": "Titanic Survival Exercises",
    "section": "Question 6: Survival by Fare",
    "text": "Question 6: Survival by Fare\nInstructions: Filter the data to remove individuals who paid a fare of 0. Make a boxplot of fare grouped by survival status. Try a log2 transformation of fares. Add the data points with jitter and alpha blending.\n\nset.seed(123)\n\ntitanic %>% filter(Fare != 0) %>% \n  ggplot(aes(Survived, Fare)) +\n  geom_boxplot(fill = colors_survived, width = 0.5, alpha = 0.5) + \n  geom_jitter(width = 0.1, alpha = 0.2) +\n  scale_y_continuous(trans = \"log2\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Passengers who survived generally payed higher fares than those who did not survive.\n❌ The interquartile range for fares was smaller for passengers who survived.\n✅ The median fare was lower for passengers who did not survive.\n❌ Only one individual paid a fare around $500. That individual survived. (3 individuals survived)\n✅ Most individuals who paid a fare around $8 did not survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 7: Survival by Passenger Class",
    "text": "Question 7: Survival by Passenger Class\nInstructions: The Pclass variable corresponds to the passenger class. Make three barplots. For the first, make a basic barplot of passenger class filled by survival. For the second, make the same barplot but use the argument position = position_fill() to show relative proportions in each group instead of counts. For the third, make a barplot of survival filled by passenger class using position = position_fill()\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Pclass)) + \n  geom_bar(width = 0.7) +\n  geom_text(aes(label = ..count..), stat = \"count\", vjust = -1) +\n  expand_limits(y = 530) +\n  ylab(\"count\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Pclass)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") \n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ There were more third class passengers than passengers in the first two classes combined.\n❌ There were the fewest passengers in first class, second-most passengers in second class, and most passengers in third class.\n✅ Survival proportion was highest for first class passengers, followed by second class. Third-class had the lowest survival proportion.\n✅ Most passengers in first class survived. Most passengers in other classes did not survive.\n❌ The majority of survivors were from first class.\n✅ The majority of those who did not survive were from third class."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 8: Survival by Age, Sex and Passenger Class",
    "text": "Question 8: Survival by Age, Sex and Passenger Class\nInstructions: Create a grid of density plots for age, filled by survival status, with count on the y-axis, faceted by sex and passenger class.\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Pclass)) +\n  geom_density(alpha=0.5) \n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Survived)) +\n  geom_density(alpha=0.5) +\n  facet_grid((Sex ~ Pclass)) +\n  scale_fill_manual(values = colors_survived) + \n  theme(panel.border = element_rect(colour = \"black\", fill = NA)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ The largest group of passengers was third-class males.\n❌ The age distribution is the same across passenger classes.\n❌ The gender distribution is the same across passenger classes.\n✅ Most first-class and second-class females survived.\n✅ Almost all second-class males did not survive, with the exception of children.\n\n\nThat’s all. Thanks for reading!"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "",
    "text": "The goal of this project is to scrape data on the 200 best movies of the last decade from the Rotten Tomatoes website with the R rvest package, and finally create a dashboard in Tableau. The idea is to show all the movies in one place. Hovering over the movie should reveal relevant data in the tooltip for quick overview. Clicking on the movie should open the movie’s website for more information.\nI’ve learned so much while working on this project (like web scraping, writing functions, iteration, …). The purrr package for functional programming is super-cool. It allows iteration with just one line of code (a very handy replacement for for loops).\nI hope you’ll enjoy the process as much as I did. At times it was quite challenging, but that’s how we learn!\n\n\n\n# loading packages\nlibrary(tidyverse)\nlibrary(rvest)\n\nAre we allowed to scrape data from the Rotten Tomatoes website?\n\nrobotstxt::paths_allowed(\"https://www.rottentomatoes.com/\")\n\n[1] TRUE"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "Plan",
    "text": "Plan\nThe data will be scraped from this page. Since it doesn’t contain all the data I am interested in, I have to visit every movie’s web page on the list and scrape data from there. Here is the plan:\n\nScrape data from the main page: the urls of movies, and the urls of images.\nScrape title, year_genre_runtime, critics_score, audiaece_score, and synopsis from the first movie to develop the code.\nWrite a function that scrapes data based on movie’s URL.\nIteration - use this function to scrape data from each individual movie and create a data frame with the columns title, year_genre_runtime, critics_score, audiaece_score, synopsis, and url.\nDownload images\nPrepare data for Tableau\nCreate a dashboard in Tableau"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "1. Scraping data from the main page",
    "text": "1. Scraping data from the main page\nReading the main page with read_html().\n\nmain_url <- \"https://editorial.rottentomatoes.com/guide/the-200-best-movies-of-the-2010s/\"\nmain_page <- read_html(main_url)\n\n\n\n\n\n\n\nFigure 1: The main page\n\n\n\n\nI make use of the SelectorGadget to identify the tags for the relevant nodes. Here is the link for Chrome (recommended).\n\nExtracting urls of movies\nThe same nodes that contain the text for the titles also contain information on the links to individual movie pages for each title. We can extract this information using the html_attr() function, which extracts attributes.\n\nmovie_urls <- main_page %>% \n  html_nodes(\".article_movie_title a\") %>% \n  html_attr(\"href\")\n\nmovie_urls %>% head()\n\n[1] \"https://www.rottentomatoes.com/m/12_years_a_slave\"    \n[2] \"https://www.rottentomatoes.com/m/20_feet_from_stardom\"\n[3] \"https://www.rottentomatoes.com/m/45_years\"            \n[4] \"https://www.rottentomatoes.com/m/all_is_lost_2013\"    \n[5] \"https://www.rottentomatoes.com/m/amazing_grace_2018\"  \n[6] \"https://www.rottentomatoes.com/m/american_hustle\"     \n\n\n\n\nExtracting urls of images\n\nimage_urls <- main_page %>% \n  html_nodes(\".article_poster\") %>% \n  html_attr(\"src\")\n\nLet’s check the image for the 6th title.\n\nknitr::include_graphics(image_urls[6])"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "2. Scraping data for the first movie on the list",
    "text": "2. Scraping data for the first movie on the list\nNow I’m going to scrape data for the movie 12 Years a Slave in order to develop the code.\nReading page for the first movie.\n\nurl <- \"https://www.rottentomatoes.com/m/12_years_a_slave\"\nmovie_page <- read_html(url)\n\n\n\n\n\n\n\nFigure 2: Title, year, genre, runtime, critics and audience score\n\n\n\n\nScroll down the page and you’ll find the movie synopsis.\n\n\n\n\n\nFigure 3: Synopsis\n\n\n\n\n\nExtracting title\n\ntitle <- movie_page %>% \n  html_node(\".scoreboard__title\") %>% \n  html_text()\n\ntitle\n\n[1] \"12 Years a Slave\"\n\n\n\n\nExtracting year, genre, and runtime\n\nyear_genre_runtime <- movie_page %>% \n  html_node(\".scoreboard__info\") %>% \n  html_text()\n\nyear_genre_runtime\n\n[1] \"2013, History/Drama, 2h 14m\"\n\n\n\n\nExtracting critics score\nThe next two are tricky. I had to look at the page source and find them manually.\n\ncritics_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"tomatometerscore\") %>% \n  str_c(.,\"%\")\n\ncritics_score\n\n[1] \"95%\"\n\n\n\n\nExtracting audience score\n\naudience_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"audiencescore\") %>% \n  str_c(.,\"%\")\n\naudience_score\n\n[1] \"90%\"\n\n\n\n\nExtracting movie synopsis\n\nsynopsis <- movie_page %>% \n  html_node(\"#movieSynopsis\") %>% \n  html_text2()\n\nsynopsis\n\n[1] \"In the years before the Civil War, Solomon Northup (Chiwetel Ejiofor), a free black man from upstate New York, is kidnapped and sold into slavery in the South. Subjected to the cruelty of one malevolent owner (Michael Fassbender), he also finds unexpected kindness from another, as he struggles continually to survive and maintain some of his dignity. Then in the 12th year of the disheartening ordeal, a chance meeting with an abolitionist from Canada changes Solomon's life forever.\"\n\n\n\n\nMakinging a data frame of extracted elements\n\nmovie  <- tibble(title = title, \n                 year_genre_runtime = year_genre_runtime,\n                 critics_score = critics_score,\n                 audience_score = audience_score,\n                 synopsis = synopsis,  \n                 url = url)\n\nmovie %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"12 Years a Slave\"\n$ year_genre_runtime <chr> \"2013, History/Drama, 2h 14m\"\n$ critics_score      <chr> \"95%\"\n$ audience_score     <chr> \"90%\"\n$ synopsis           <chr> \"In the years before the Civil War, Solomon Northup…\n$ url                <chr> \"https://www.rottentomatoes.com/m/12_years_a_slave\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "3. Writing a function",
    "text": "3. Writing a function\nInstead of manually scraping individual movies, I’ll write a function to do the same.\n\nscrape_movie <- function(x, ...){\n  \n  movie_page <- read_html(x)\n  \n  title <- movie_page %>% \n    html_node(\".scoreboard__title\") %>% \n    html_text()\n  \n  year_genre_runtime <- movie_page %>% \n    html_node(\".scoreboard__info\") %>% \n    html_text()\n  \n  critics_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"tomatometerscore\") %>% \n    str_c(.,\"%\")\n  \n  audience_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"audiencescore\") %>% \n    str_c(.,\"%\")\n  \n  synopsis <- movie_page %>% \n    html_node(\"#movieSynopsis\") %>% \n    html_text2()\n  \n  movie_df <- tibble(title = title, \n                     year_genre_runtime = year_genre_runtime,\n                     critics_score = critics_score,\n                     audience_score = audience_score,\n                     synopsis = synopsis,\n                     url = x)\n  \n  return(movie_df)\n  \n}\n\n\nFunction in action\nNow that we have the scrape_movie() function, let’s scrape data for the movie “American Hustle”.\n\nscrape_movie(movie_urls[6]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"American Hustle\"\n$ year_genre_runtime <chr> \"2013, Crime/Drama, 2h 18m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"74%\"\n$ synopsis           <chr> \"Irving Rosenfeld (Christian Bale) dabbles in forge…\n$ url                <chr> \"https://www.rottentomatoes.com/m/american_hustle\"\n\n\nOr “Ex Machina” (an interesting SF movie).\n\n scrape_movie(movie_urls[53]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"Ex Machina\"\n$ year_genre_runtime <chr> \"2014, Sci-fi/Mystery & thriller, 1h 47m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"86%\"\n$ synopsis           <chr> \"Caleb Smith (Domhnall Gleeson) a programmer at a h…\n$ url                <chr> \"https://www.rottentomatoes.com/m/ex_machina\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "4. Iteration",
    "text": "4. Iteration\nTo make my workflow a little more efficient, I make use of the map_dfr() function from the purrr package to iterate over all movie pages. map_dfr() will apply the scrape_movie()function to each element in the vector of links, and return a data frame created by row-binding. It’s as simple as that.\n\nmovies <- map_dfr(movie_urls, scrape_movie)\n\nmovies \n\n# A tibble: 200 × 6\n   title                year_genre_runtime         criti…¹ audie…² synop…³ url  \n   <chr>                <chr>                      <chr>   <chr>   <chr>   <chr>\n 1 12 Years a Slave     2013, History/Drama, 2h 1… 95%     90%     In the… http…\n 2 20 Feet From Stardom 2013, Documentary, 1h 30m  99%     82%     Filmma… http…\n 3 45 Years             2015, Drama, 1h 33m        97%     67%     As the… http…\n 4 All Is Lost          2013, Adventure/Mystery &… 94%     64%     During… http…\n 5 Amazing Grace        2018, Documentary/Music, … 99%     80%     Singer… http…\n 6 American Hustle      2013, Crime/Drama, 2h 18m  92%     74%     Irving… http…\n 7 Amy                  2015, Documentary/Biograp… 95%     87%     Archiv… http…\n 8 Anomalisa            2015, Comedy/Drama, 1h 30m 91%     71%     An ins… http…\n 9 Ant-Man and The Wasp 2018, Action/Adventure, 1… 87%     80%     Scott … http…\n10 Apollo 11            2019, Documentary/History… 99%     90%     Never-… http…\n# … with 190 more rows, and abbreviated variable names ¹​critics_score,\n#   ²​audience_score, ³​synopsis"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "5. Downloading images",
    "text": "5. Downloading images\nI’ve already extracted urls of images in the first step and saved them to image_urls. Now I’m going to create a directory and directory paths for the images.\n\nfs::dir_create(\"images/top_200_images/\")\n\npaths <- c(str_c(\"images/top_200_images/\", sprintf(\"%0.3d\", 1:200), \".jpg\"))\n\npaths %>% head()\n\n[1] \"images/top_200_images/001.jpg\" \"images/top_200_images/002.jpg\"\n[3] \"images/top_200_images/003.jpg\" \"images/top_200_images/004.jpg\"\n[5] \"images/top_200_images/005.jpg\" \"images/top_200_images/006.jpg\"\n\n\nSince Tableau sorts images alphabetically (1, 11, 111, 2, 22, …) by default, these leading zeros will help Tableau to correctly match the images with the data so I don’t have to do it manually.\n\nDownloading images\nThis time I’ll use map2() function from the purrr package, It will apply the download.file() function to pairs of elements from two vectors, image_urls and paths.\n\nmap2(image_urls, paths, function(.x, .y) download.file(.x, .y, mode=\"wb\")) \n\nAre the images properly saved? Let’s read in the image for the first movie.\n\nknitr::include_graphics(\"images/top_200_images/001.jpg\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "6. Data wrangling",
    "text": "6. Data wrangling\nPreparing the final dataset for Tableau.\n\nmovies <- movies %>% \n  \n  # separate year_genre_runtime column into year, genre, and runtime\n  separate(year_genre_runtime, sep = \", \", into = c(\"year\", \"genre\", \"runtime\")) %>% \n  mutate(year = as.factor(year)) %>% \n  \n  # separate genre column into primary and secondary genre\n  separate(genre, sep = \"/\", into = c(\"genre_1\", \"genre_2\"), remove = FALSE) %>% \n  \n  # create id column with leading zeroes so Tableau can automatically match the images\n  mutate(id = sprintf(\"%0.3d\", 1:200)) %>% \n  select(id, everything())\n\nmovies %>% head()\n\n# A tibble: 6 × 11\n  id    title  year  genre genre_1 genre_2 runtime criti…¹ audie…² synop…³ url  \n  <chr> <chr>  <fct> <chr> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>\n1 001   12 Ye… 2013  Hist… History Drama   2h 14m  95%     90%     In the… http…\n2 002   20 Fe… 2013  Docu… Docume… <NA>    1h 30m  99%     82%     Filmma… http…\n3 003   45 Ye… 2015  Drama Drama   <NA>    1h 33m  97%     67%     As the… http…\n4 004   All I… 2013  Adve… Advent… Myster… 1h 45m  94%     64%     During… http…\n5 005   Amazi… 2018  Docu… Docume… Music   1h 27m  99%     80%     Singer… http…\n6 006   Ameri… 2013  Crim… Crime   Drama   2h 18m  92%     74%     Irving… http…\n# … with abbreviated variable names ¹​critics_score, ²​audience_score, ³​synopsis\n\n\n\n# number of unique values in genre column\nmovies$genre %>% unique() %>% length()\n\n[1] 59\n\n\n\n# unique values in genre_1\nmovies$genre_1 %>% unique()\n\n [1] \"History\"            \"Documentary\"        \"Drama\"             \n [4] \"Adventure\"          \"Crime\"              \"Comedy\"            \n [7] \"Action\"             \"Sci-fi\"             \"Romance\"           \n[10] \"Horror\"             \"Biography\"          \"Mystery & thriller\"\n[13] \"Kids & family\"      \"War\"                \"Fantasy\"           \n[16] \"Musical\"            \"Western\"           \n\n\n\n# unique values in genre_2\nmovies$genre_2 %>% unique()\n\n [1] \"Drama\"              NA                   \"Mystery & thriller\"\n [4] \"Music\"              \"Biography\"          \"Adventure\"         \n [7] \"History\"            \"Romance\"            \"Comedy\"            \n[10] \"Lgbtq+\"             \"Action\"             \"War\"               \n[13] \"Fantasy\"            \"Sci-fi\"             \"Crime\"             \n[16] \"Musical\"            \"Western\"            \"Anime\"             \n[19] \"Horror\"            \n\n\nFinding values in genre_2, that are not in genre_1. This will help when creating a list parameter for filtering by primary or secondary genre.\n\nsetdiff(movies$genre_2, movies$genre_1)\n\n[1] NA       \"Music\"  \"Lgbtq+\" \"Anime\" \n\n\n\nDT table\nIf you prefer to search a table for data, then this one is for you!\n\nmovies %>% \n  select(1:9) %>% \n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\n\n\nWriting file\nI choose to save the data in an excel file only because the csv will remove the leading zeros in the id column.\n\nmovies %>% writexl::write_xlsx(\"datasets/top_200_movies_2010s_rotten_tomatoes.xlsx\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "7. Tableau dashboard",
    "text": "7. Tableau dashboard\nThe final dashboard is created in Tableau. It’s actually a jitter plot, which separates overlapping movies with the same critics’ score.\nTo avoid two filters, one for primary and one for secondary genre, a list parameter is created that filters movies by primary or secondary genre, or “All” values.\nFor the best viewing experience, please click on the full screen in the bottom right corner.\nYou can nteract with the embedded dashboard below or go to Tableau Public. Enjoy!"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html",
    "href": "posts/shiny-mass-shootings/mass-shootings.html",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "",
    "text": "Mass Shootings have been a topic of intense discussion in the United States. A public “database” of mass shootings since 1982 has been made available by the Mother Jones, a non-profit organization. This “database” is stored in a Google spreadsheet and you can view it here and download as a CSV file."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data overview",
    "text": "Data overview\n\nlibrary(tidyverse)\nlibrary(tidygeocoder)\nlibrary(plotly)\ntheme_set(theme_classic())\n\nmass_shootings <- read_csv(\"data/mass_shootings_usa_1982-2023.csv\")\n\nmass_shootings %>% glimpse()\n\nRows: 140\nColumns: 24\n$ case                             <chr> \"Michigan State University shooting\",…\n$ location...2                     <chr> \"East Lansing, Michigan\", \"Half Moon …\n$ date                             <date> 2023-02-13, 2023-01-23, 2023-01-21, …\n$ summary                          <chr> \"Anthony D. McRae, 43, opened fire at…\n$ fatalities                       <dbl> 3, 7, 11, 6, 5, 3, 5, 3, 7, 3, 3, 4, …\n$ injured                          <chr> \"5\", \"1\", \"10\", \"6\", \"25\", \"2\", \"2\", …\n$ total_victims                    <chr> \"8\", \"8\", \"21\", \"12\", \"30\", \"5\", \"7\",…\n$ location...8                     <chr> \"School\", \"workplace\", \"Other\", \"Work…\n$ age_of_shooter                   <chr> \"43\", \"67\", \"72\", \"31\", \"22\", \"22\", \"…\n$ prior_signs_mental_health_issues <chr> \"-\", \"-\", \"yes\", \"-\", \"yes\", \"-\", \"-\"…\n$ mental_health_details            <chr> \"-\", \"-\", \"According to the LA Times,…\n$ weapons_obtained_legally         <chr> \"yes\", \"-\", \"-\", \"-\", \"-\", \"yes\", \"-\"…\n$ where_obtained                   <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"Dance's Spo…\n$ weapon_type                      <chr> \"semiautomatic handguns\", \"semiautoma…\n$ weapon_details                   <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"Glock 45 9m…\n$ race                             <chr> \"Black\", \"Asian\", \"Asian\", \"Black\", \"…\n$ gender                           <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M…\n$ sources                          <chr> \"https://www.cnn.com/us/live-news/mic…\n$ mental_health_sources            <chr> \"-\", \"-\", \"https://www.latimes.com/ca…\n$ sources_additional_age           <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ latitude                         <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ longitude                        <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ type                             <chr> \"Mass\", \"Spree\", \"Mass\", \"Mass\", \"Mas…\n$ year                             <dbl> 2023, 2023, 2023, 2022, 2022, 2022, 2…\n\n\nWe have 140 cases, described with 24 variables. At first glance, this dataset clearly needs extensive cleaning."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#cleaning",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#cleaning",
    "title": "Shiny App: Mass Shootings in the USA",
    "section": "Cleaning",
    "text": "Cleaning\n🧹 Are there any duplicates?\n\nsum(duplicated(mass_shootings))\n\n[1] 0\n\n\n🧹 Selecting columns of interest, replacing character - with NA in all columns, renaming location columns, converting data types.\n\nmass_shootings_cln <- mass_shootings %>% \n  select(1:5, 8:10, 12, 16, 17, 21:24) %>% \n  mutate(across(where(is.character), ~na_if(., \"-\"))) %>% \n  rename(location = location...2, location_2 = location...8) %>% \n  mutate(age_of_shooter = as.numeric(age_of_shooter),\n         latitude = as.numeric(latitude),\n         longitude = as.numeric(longitude))\n\nmass_shootings_cln %>% glimpse()\n\nRows: 140\nColumns: 15\n$ case                             <chr> \"Michigan State University shooting\",…\n$ location                         <chr> \"East Lansing, Michigan\", \"Half Moon …\n$ date                             <date> 2023-02-13, 2023-01-23, 2023-01-21, …\n$ summary                          <chr> \"Anthony D. McRae, 43, opened fire at…\n$ fatalities                       <dbl> 3, 7, 11, 6, 5, 3, 5, 3, 7, 3, 3, 4, …\n$ location_2                       <chr> \"School\", \"workplace\", \"Other\", \"Work…\n$ age_of_shooter                   <dbl> 43, 67, 72, 31, 22, 22, 15, 20, 21, 7…\n$ prior_signs_mental_health_issues <chr> NA, NA, \"yes\", NA, \"yes\", NA, NA, NA,…\n$ weapons_obtained_legally         <chr> \"yes\", NA, NA, NA, NA, \"yes\", NA, \"ye…\n$ race                             <chr> \"Black\", \"Asian\", \"Asian\", \"Black\", \"…\n$ gender                           <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M…\n$ latitude                         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ longitude                        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ type                             <chr> \"Mass\", \"Spree\", \"Mass\", \"Mass\", \"Mas…\n$ year                             <dbl> 2023, 2023, 2023, 2022, 2022, 2022, 2…\n\n\nChecking unique values for certain variables.\n\nmass_shootings_cln %>% \n  group_by(gender) %>% \n  count(sort = TRUE)\n\n# A tibble: 5 × 2\n# Groups:   gender [5]\n  gender            n\n  <chr>         <int>\n1 Male             70\n2 M                65\n3 Female            2\n4 Male & Female     2\n5 F                 1\n\n\n\nmass_shootings_cln %>% \n  group_by(race) %>% \n  count(sort = TRUE)\n\n# A tibble: 10 × 2\n# Groups:   race [10]\n   race                n\n   <chr>           <int>\n 1 white              41\n 2 White              32\n 3 Black              16\n 4 <NA>               12\n 5 Latino             11\n 6 Asian              10\n 7 black               9\n 8 Other               5\n 9 Native American     3\n10 unclear             1\n\n\n\nmass_shootings_cln %>% \n  group_by(location_2) %>% \n  count(sort = TRUE)\n\n# A tibble: 10 × 2\n# Groups:   location_2 [10]\n   location_2        n\n   <chr>         <int>\n 1 \"Other\"          50\n 2 \"Workplace\"      45\n 3 \"School\"         21\n 4 \"Religious\"       7\n 5 \"Military\"        6\n 6 \"workplace\"       5\n 7 \"Other\\n\"         3\n 8 \"\\nWorkplace\"     1\n 9 \"Airport\"         1\n10 \"religious\"       1\n\n\n\nmass_shootings_cln %>% \n  group_by(prior_signs_mental_health_issues) %>% \n  count(sort = TRUE)\n\n# A tibble: 7 × 2\n# Groups:   prior_signs_mental_health_issues [7]\n  prior_signs_mental_health_issues     n\n  <chr>                            <int>\n1 Yes                                 58\n2 <NA>                                27\n3 Unclear                             24\n4 No                                  17\n5 yes                                  8\n6 TBD                                  5\n7 Unknown                              1\n\n\n\nmass_shootings_cln %>% \n  group_by(weapons_obtained_legally) %>% \n  count(sort = TRUE)\n\n# A tibble: 9 × 2\n# Groups:   weapons_obtained_legally [9]\n  weapons_obtained_legally                                                     n\n  <chr>                                                                    <int>\n1 \"Yes\"                                                                       82\n2 \"No\"                                                                        16\n3  <NA>                                                                       16\n4 \"yes\"                                                                        9\n5 \"TBD\"                                                                        7\n6 \"Unknown\"                                                                    6\n7 \"\\nYes\"                                                                      2\n8 \"Kelley passed federal criminal background checks; the US Air Force fai…     1\n9 \"Yes (\\\"some of the weapons were purchased legally and some of them may…     1\n\n\nWhat a mess. This dataset needs some additional cleaning.\n🧹 Changing values in certain columns with case_when function"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n🧹 Step 1. Innitial cleaning\nThe first cleaning step includes:\n\nselecting columns of interest,\nreplacing the character value\"-\" with NA in all columns with character data type,\nrenaming location columns,\nconverting character data type to numeric for specific columns.\n\n\nmass_shootings_cln <- mass_shootings %>% \n  select(1:6, 8:10, 12, 16, 17, 21:24) %>% \n  mutate(across(where(is.character), ~na_if(., \"-\"))) %>% \n  rename(location = location...2, location_2 = location...8) %>% \n  mutate_at(c(\"injured\", \"age_of_shooter\", \"latitude\", \"longitude\"), as.numeric)\n  \nmass_shootings_cln %>% glimpse()\n\nRows: 140\nColumns: 16\n$ case                             <chr> \"Michigan State University shooting\",…\n$ location                         <chr> \"East Lansing, Michigan\", \"Half Moon …\n$ date                             <date> 2023-02-13, 2023-01-23, 2023-01-21, …\n$ summary                          <chr> \"Anthony D. McRae, 43, opened fire at…\n$ fatalities                       <dbl> 3, 7, 11, 6, 5, 3, 5, 3, 7, 3, 3, 4, …\n$ injured                          <dbl> 5, 1, 10, 6, 25, 2, 2, 2, 46, 0, 1, N…\n$ location_2                       <chr> \"School\", \"workplace\", \"Other\", \"Work…\n$ age_of_shooter                   <dbl> 43, 67, 72, 31, 22, 22, 15, 20, 21, 7…\n$ prior_signs_mental_health_issues <chr> NA, NA, \"yes\", NA, \"yes\", NA, NA, NA,…\n$ weapons_obtained_legally         <chr> \"yes\", NA, NA, NA, NA, \"yes\", NA, \"ye…\n$ race                             <chr> \"Black\", \"Asian\", \"Asian\", \"Black\", \"…\n$ gender                           <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M…\n$ latitude                         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ longitude                        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ type                             <chr> \"Mass\", \"Spree\", \"Mass\", \"Mass\", \"Mas…\n$ year                             <dbl> 2023, 2023, 2023, 2022, 2022, 2022, 2…\n\n\n🔎 Are there any duplicates? No.\n\nsum(duplicated(mass_shootings))\n\n[1] 0\n\n\n🔎 Number of missing values, NA, per column.\n\nmass_shootings_cln %>% \n  summarise_all(~sum(is.na(.))) %>% \n  # transposing for better visibility\n  pivot_longer(cols = everything(), names_to = \"column\", values_to = \"n_missing\")\n\n# A tibble: 16 × 2\n   column                           n_missing\n   <chr>                                <int>\n 1 case                                     0\n 2 location                                 0\n 3 date                                     0\n 4 summary                                  0\n 5 fatalities                               0\n 6 injured                                  1\n 7 location_2                               0\n 8 age_of_shooter                           2\n 9 prior_signs_mental_health_issues        27\n10 weapons_obtained_legally                16\n11 race                                    12\n12 gender                                   0\n13 latitude                                14\n14 longitude                               14\n15 type                                     0\n16 year                                     0\n\n\n14 of the most recent cases don’t have location coordinates at all. We’ll address this in the final cleanup step.\n\n\n🧹 Step 2. Fixing unique values for categorical variables\n🔎 Let’s take a look at the unique values of the gender column.\n\nmass_shootings_cln %>% \n  group_by(gender) %>% \n  count(sort = TRUE) %>% \n  ungroup()\n\n# A tibble: 5 × 2\n  gender            n\n  <chr>         <int>\n1 Male             70\n2 M                65\n3 Female            2\n4 Male & Female     2\n5 F                 1\n\n\nAlmost all categorical variables need unique values correction.\nTo make a long story short, I’ll correct them all in one step using case_when function, and we’ll look at them later during the analysis.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  mutate(gender = case_when(gender == \"F\" ~ \"Female\",\n                            gender == \"M\" ~ \"Male\", \n                            TRUE ~ gender),\n         race = case_when(race == \"white\" ~ \"White\",\n                          race == \"black\" ~ \"Black\",\n                          race == \"unclear\" ~ \"Unclear\",\n                          TRUE ~ race),\n         location_2 = \n           case_when(location_2 %in% c(\"workplace\", \"\\nWorkplace\") ~ \"Workplace\",\n                                location_2 == \"Other\\n\" ~ \"Other\",\n                                location_2 == \"religious\" ~ \"Religious\",\n                                TRUE ~ location_2),\n         prior_signs_mental_health_issues = \n           case_when(prior_signs_mental_health_issues == \"yes\" ~ \"Yes\",\n                     prior_signs_mental_health_issues == \"TBD\" ~ \"To be determined\",\n                     TRUE ~ prior_signs_mental_health_issues),\n         weapons_obtained_legally = \n           case_when(weapons_obtained_legally %in% c(\"yes\", \"\\nYes\") ~ \"Yes\",\n                     weapons_obtained_legally == \"TBD\" ~ \"To be determined\",\n                     weapons_obtained_legally %>% str_detect(\"Kelley\") ~ \"Unknown\",\n                     weapons_obtained_legally %>% str_detect(\"some\") ~ \"Partially\",\n                     TRUE ~ weapons_obtained_legally))\n\n\n\n🧹 Step 3. Geocoding locations with missing coordinates\nThere are 14 cases with missing location coordinates. In this step we’ll convert locations to coordinates with geocoding. and use them later to create a leaflet map for a shiny app.\nThe tidygeocoder package provides geocoding services. It’s designed to work easily with the tidyverse. It also provides access to several different geocoding services, including LocationIQ which I’m going to use here. LocationIQ is a freemium service that provides a free tier, which doesn’t require you to give them your billing details. When you sign up to LocationIQ, they’ll take you to the Manage Your API Access Tokens page, which is where we obtain our API token. Next, you need to provide the tidygeocoder package with your API key.\nThe first step is to select only locations with missing coordinates and geocode them.\n\ngeocoded_locations <- mass_shootings_cln %>% \n  filter(is.na(latitude) | is.na(longitude)) %>% \n  select(location) %>% \n  geocode(location, method = \"iq\")\n\ngeocoded_locations\n\n# A tibble: 14 × 3\n   location                     lat   long\n   <chr>                      <dbl>  <dbl>\n 1 East Lansing, Michigan      42.7  -84.5\n 2 Half Moon Bay, California   37.5 -122. \n 3 Monterey Park, California   34.1 -118. \n 4 Chesapeake, Virginia        36.7  -76.2\n 5 Colorado Springs, Colorado  38.8 -105. \n 6 Charlottesville, Virginia   38.0  -78.5\n 7 Hedingham, North Carolina   35.8  -78.5\n 8 Greenwood, Indiana          39.6  -86.1\n 9 Highland Park, Illinois     42.2  -87.8\n10 Birmingham, Alabama         33.5  -86.8\n11 Smithsburg, Maryland        39.7  -77.6\n12 Tulsa, Oklahoma             36.2  -96.0\n13 Uvalde, Texas               29.3  -99.8\n14 Buffalo, New York           42.9  -78.9\n\n\nThe next step is to join mass shootings table with geocoded locations and replace missing latitudes and longitudes with geocoded.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  left_join(geocoded_locations, by = \"location\") %>% \n  mutate(latitude = ifelse(is.na(latitude), lat, latitude),\n         longitude = ifelse(is.na(longitude), long, longitude))\n\nsum(is.na(mass_shootings_cln$latitude))\n\n[1] 0\n\nsum(is.na(mass_shootings_cln$longitude))\n\n[1] 0\n\n\nOK, this looks fine."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#bar-plots-for-character-variables",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#bar-plots-for-character-variables",
    "title": "Shiny App: Mass Shootings in the USA",
    "section": "Bar plots for character variables",
    "text": "Bar plots for character variables\n\nmass_shootings_cln %>% \n  ggplot(aes(y = fct_rev(fct_infreq(gender)))) +\n  geom_bar(fill = \"steelblue\", width = 0.6) \n\n\n\n\n\nmass_shootings_cln %>% \n  ggplot(aes(y = fct_rev(fct_infreq(race)))) +\n  geom_bar(fill = \"steelblue\", width = 0.7) \n\n\n\n\n\nmass_shootings_cln %>% \n  ggplot(aes(y = fct_rev(fct_infreq(location_2)))) +\n  geom_bar(fill = \"steelblue\", width = 0.7) \n\n\n\n\n\nmass_shootings_cln %>% \n  ggplot(aes(y = fct_rev(fct_infreq(prior_signs_mental_health_issues)))) +\n  geom_bar(fill = \"steelblue\", width = 0.7) \n\n\n\n\n\nmass_shootings_cln %>% \n  ggplot(aes(y = fct_rev(fct_infreq(weapons_obtained_legally)))) +\n  geom_bar(fill = \"steelblue\", width = 0.7) \n\n\n\n\n\nmass_shootings_cln %>% \n  ggplot(aes(y = fct_rev(fct_infreq(type)))) +\n  geom_bar(fill = \"steelblue\", width = 0.7) \n\n\n\n\n\nmass_shootings_cln %>% \n  ggplot(aes(age_of_shooter)) +\n  geom_histogram(fill = \"indianred\", color = \"white\", binwidth = 5) \n\n\n\n\n\nskimr::skim_without_charts(mass_shootings_cln)\n\n\nData summary\n\n\nName\nmass_shootings_cln\n\n\nNumber of rows\n140\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nDate\n1\n\n\nfactor\n6\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncase\n0\n1\n12\n45\n0\n140\n0\n\n\nlocation\n0\n1\n12\n36\n0\n129\n0\n\n\nsummary\n0\n1\n83\n908\n0\n140\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n1982-08-20\n2023-02-13\n2014-08-08\n140\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nlocation_2\n0\n1.00\nFALSE\n6\nOth: 53, Wor: 51, Sch: 21, Rel: 8\n\n\nprior_signs_mental_health_issues\n27\n0.81\nFALSE\n5\nYes: 66, Unc: 24, No: 17, To : 5\n\n\nweapons_obtained_legally\n16\n0.89\nFALSE\n5\nYes: 93, No: 16, To : 7, Unk: 7\n\n\nrace\n12\n0.91\nFALSE\n7\nWhi: 73, Bla: 25, Lat: 11, Asi: 10\n\n\ngender\n0\n1.00\nFALSE\n3\nMal: 135, Fem: 3, Mal: 2\n\n\ntype\n0\n1.00\nFALSE\n2\nMas: 119, Spr: 21\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nfatalities\n0\n1.00\n7.82\n7.49\n3.00\n4.00\n6.00\n8.25\n58.00\n\n\ninjured\n1\n0.99\n11.36\n46.90\n0.00\n1.00\n3.00\n10.00\n546.00\n\n\nage_of_shooter\n2\n0.99\n34.06\n13.25\n11.00\n22.25\n32.50\n43.00\n72.00\n\n\nlatitude\n14\n0.90\n37.38\n5.62\n21.32\n33.80\n38.32\n41.47\n48.46\n\n\nlongitude\n14\n0.90\n-96.84\n17.91\n-157.88\n-117.56\n-90.87\n-81.51\n-71.08\n\n\nyear\n0\n1.00\n2010.29\n10.78\n1982.00\n2004.75\n2014.00\n2018.00\n2023.00\n\n\n\n\n\n\nTotal Cases by Year\n\np1 <- mass_shootings_cln %>%\n  group_by(year) %>%\n  count() %>% \n  ggplot(aes(year, n)) +\n  geom_col(fill = \"steelblue\") + \n  labs(title = \"Total Cases by Year\") +\n  scale_y_continuous(limits = c(0,13)) +\n  theme_classic() \n\nggplotly(p1)\n\n\n\n\n\n\nskimr::skim(mass_shootings_cln) %>%\n  dplyr::select(skim_variable, n_missing)\n\n# A tibble: 16 × 2\n   skim_variable                    n_missing\n   <chr>                                <int>\n 1 case                                     0\n 2 location                                 0\n 3 summary                                  0\n 4 date                                     0\n 5 location_2                               0\n 6 prior_signs_mental_health_issues        27\n 7 weapons_obtained_legally                16\n 8 race                                    12\n 9 gender                                   0\n10 type                                     0\n11 fatalities                               0\n12 injured                                  1\n13 age_of_shooter                           2\n14 latitude                                14\n15 longitude                               14\n16 year                                     0"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#relationship-between-fatalities-and-injured",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#relationship-between-fatalities-and-injured",
    "title": "Shiny App: Mass Shootings in the USA",
    "section": "Relationship between Fatalities and Injured",
    "text": "Relationship between Fatalities and Injured\n\np3 <- mass_shootings_cln %>%\n  ggplot(aes(fatalities, sqrt(injured))) +\n  geom_jitter() +\n  theme_classic()\n\nggplotly(p3)"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\n📄 Breakdown by categorical variables\n\nGender\n\nmass_shootings_cln %>% \n  group_by(gender) %>% \n  summarise(n = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(n/sum(n), accuracy = 0.1)) %>% \n  arrange(desc(n))\n\n# A tibble: 3 × 3\n  gender            n percent\n  <chr>         <int> <chr>  \n1 Male            135 96.4%  \n2 Female            3 2.1%   \n3 Male & Female     2 1.4%   \n\n\n\n\nRace\n\nmass_shootings_cln %>% \n  group_by(race) %>% \n  summarise(n = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(n/sum(n), accuracy = 0.1)) %>% \n  arrange(desc(n))\n\n# A tibble: 8 × 3\n  race                n percent\n  <chr>           <int> <chr>  \n1 White              73 52.1%  \n2 Black              25 17.9%  \n3 <NA>               12 8.6%   \n4 Latino             11 7.9%   \n5 Asian              10 7.1%   \n6 Other               5 3.6%   \n7 Native American     3 2.1%   \n8 Unclear             1 0.7%   \n\n\n\n\nSpecific location\n\nmass_shootings_cln %>% \n  group_by(location_2) %>% \n  summarise(n = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(n/sum(n), accuracy = 0.1)) %>% \n  arrange(desc(n))\n\n# A tibble: 6 × 3\n  location_2     n percent\n  <chr>      <int> <chr>  \n1 Other         53 37.9%  \n2 Workplace     51 36.4%  \n3 School        21 15.0%  \n4 Religious      8 5.7%   \n5 Military       6 4.3%   \n6 Airport        1 0.7%   \n\n\n\n\nPrior signs of mental health issues\n\nmass_shootings_cln %>% \n  group_by(prior_signs_mental_health_issues) %>% \n  summarise(n = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(n/sum(n), accuracy = 0.1)) %>% \n  arrange(desc(n))\n\n# A tibble: 6 × 3\n  prior_signs_mental_health_issues     n percent\n  <chr>                            <int> <chr>  \n1 Yes                                 66 47.1%  \n2 <NA>                                27 19.3%  \n3 Unclear                             24 17.1%  \n4 No                                  17 12.1%  \n5 To be determined                     5 3.6%   \n6 Unknown                              1 0.7%   \n\n\n\n\nWeapons obtained legally\n\nmass_shootings_cln %>% \n  group_by(weapons_obtained_legally) %>% \n  summarise(n = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(n/sum(n), accuracy = 0.1)) %>% \n  arrange(desc(n))\n\n# A tibble: 6 × 3\n  weapons_obtained_legally     n percent\n  <chr>                    <int> <chr>  \n1 Yes                         93 66.4%  \n2 No                          16 11.4%  \n3 <NA>                        16 11.4%  \n4 To be determined             7 5.0%   \n5 Unknown                      7 5.0%   \n6 Partially                    1 0.7%   \n\n\n\n\nType\n\nmass_shootings_cln %>% \n  group_by(type) %>% \n  summarise(n = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(n/sum(n), accuracy = 0.1)) %>% \n  arrange(desc(n))\n\n# A tibble: 2 × 3\n  type      n percent\n  <chr> <int> <chr>  \n1 Mass    119 85.0%  \n2 Spree    21 15.0%  \n\n\nNote: Spree shootings here have three or more victims in a short time in multiple locations.\n\n\n\n📊 Age of shooter distribution\n\nmass_shootings_cln %>% \n  ggplot(aes(age_of_shooter)) +\n  geom_histogram(fill = \"indianred\", color = \"white\", binwidth = 5) +\n  labs(title = \"Age Distribution\", x = \"age of shooter\")\n\n\n\n\n\nThe vast majority of shooters were between 20 and 50 years old.\nMost shooters were in the 20-25 age group.\n\n🔎 Who was the youngest shooter?\n\nindex <- which.min(mass_shootings_cln$age_of_shooter)\n\nmass_shootings_cln[index, ] %>% \n  select(case, date, summary, fatalities) %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\ncase\ndate\nsummary\nfatalities\n\n\n\n\nWestside Middle School killings\n1998-03-24\nMitchell Scott Johnson, 13, and Andrew Douglas Golden, 11, two juveniles, ambushed students and teachers as they left the school; they were apprehended by police at the scene.\n5\n\n\n\n\n\n\n\n📊 Number of cases per year\n\np1 <- mass_shootings_cln %>%\n  group_by(year) %>%\n  summarise(count = n()) %>% \n  ggplot(aes(year, count)) +\n  geom_col(fill = \"steelblue\") + \n  geom_vline(xintercept = 2012, color = \"red\") +\n  labs(title = \"Number of Cases per Year\") \n\nggplotly(p1)\n\n\n\n\n\n\n\nWe can see an increase in mass shootings in the last 10 years.\n2020 has a smaller number of cases probably due to Covid restrictions.\nThe data for 2023 is incomplete, but 3 cases in the first two months seems a lot.\n\n\n\n📊 Fatalities-Injured relationship\n\np3 <- mass_shootings_cln %>%\n  ggplot(aes(x = fatalities, y = injured)) +\n  geom_jitter() +\n  scale_y_log10() +\n  labs(title = \"Fatalities-Injured Relationship\")\n  \nggplotly(p3)\n\n\n\n\n\n\nPlease note that the Injured values are log-scaled for better visibility.\nSummary of fatalities\n\nsummary(mass_shootings_cln$fatalities)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   4.000   6.000   7.821   8.250  58.000 \n\n\nSummary of injured people\n\nsummary(mass_shootings_cln$injured)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    1.00    3.00   11.36   10.00  546.00       1"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Shiny app",
    "text": "Shiny app\nThe app you can see below is embedded in this quarto document since my website is static. It was originally published on shinyapps.io and you can also interact with it here.\nA quick note: With a free account I have 25 active hours (when my applications are not idle). If these 25 active hours are exceeded, my app will not be available again until the following month cycle. Hope you get lucky! 😊\n📢 By clicking on each circle, you can read a summary of the mass shooting case."
  },
  {
    "objectID": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#foreword",
    "href": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#foreword",
    "title": "8 Week SQL Challenge #2 - Pizza Runner",
    "section": "Foreword",
    "text": "Foreword\nA few words before we start. This is the second 8 Week SQL Challenge made by Danny My. For more information please visit his web page. I am very grateful for this opportunity to practice SQL skills Thanks Danny! Here I will try to solve the challenge and present my solutions.\nFor the purposes of this project, I’ve created the local pizza_runner Postgres database on my machine and run the .sql script to create and populate the tables.\n\nSetting up the programming environment\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, you first need to establish a DBI connection to a database (typically via the DBI::dbConnect() function).\n\n\nCreating a connection to the pizza_runner database\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"pizza_runner\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> pizza_runner@localhost:5432"
  },
  {
    "objectID": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#introduction",
    "href": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#introduction",
    "title": "8 Week SQL Challenge #2 - Pizza Runner",
    "section": "Introduction",
    "text": "Introduction\nDid you know that over 115 million kilograms of pizza is consumed daily worldwide??? (Well according to Wikipedia anyway…)\nDanny was scrolling through his Instagram feed when something really caught his eye - “80s Retro Styling and Pizza Is The Future!”\nDanny was sold on the idea, but he knew that pizza alone was not going to help him get seed funding to expand his new Pizza Empire - so he had one more genius idea to combine with it - he was going to Uberize it - and so Pizza Runner was launched!\nDanny started by recruiting “runners” to deliver fresh pizza from Pizza Runner Headquarters (otherwise known as Danny’s house) and also maxed out his credit card to pay freelance developers to build a mobile app to accept orders from customers."
  },
  {
    "objectID": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#entity-relationship-diagram",
    "href": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#entity-relationship-diagram",
    "title": "8 Week SQL Challenge #2 - Pizza Runner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\nAfter running the .sql file to create and populate the tables, I’ve noticed that the primary and foreign keys are not defined. So I made the following ERD in pgAdmin 4 (a GUI tool used to interact with the Postgres database sessions, both locally and remote servers as well)."
  },
  {
    "objectID": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#data-cleaning",
    "href": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#data-cleaning",
    "title": "8 Week SQL Challenge #2 - Pizza Runner",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nData issues in the existing schema include:\n\ncustomer_orders table\n\nnull values entered as text\nusing both NaN and null values\n\nrunner_orders table\n\nnull values entered as text\nusing both NaN and null values\nunits manually entered in distance and duration columns\n\n\nLet’s preview these two tables.\n\nSELECT *\nFROM pizza_runner.customer_orders\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2020-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2020-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2020-01-02 23:51:23\n\n\n3\n102\n2\n\nNA\n2020-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2020-01-04 13:23:46\n\n\n5\n104\n1\nnull\n1\n2020-01-08 21:00:29\n\n\n6\n101\n2\nnull\nnull\n2020-01-08 21:03:13\n\n\n7\n105\n2\nnull\n1\n2020-01-08 21:20:29\n\n\n\n\n\n\nSELECT *\nFROM pizza_runner.runner_orders\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2020-01-01 18:15:34\n20km\n32 minutes\n\n\n\n2\n1\n2020-01-01 19:10:54\n20km\n27 minutes\n\n\n\n3\n1\n2020-01-03 00:12:37\n13.4km\n20 mins\nNA\n\n\n4\n2\n2020-01-04 13:53:03\n23.4\n40\nNA\n\n\n5\n3\n2020-01-08 21:10:57\n10\n15\nNA\n\n\n6\n3\nnull\nnull\nnull\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25km\n25mins\nnull\n\n\n8\n2\n2020-01-10 00:15:02\n23.4 km\n15 minute\nnull\n\n\n9\n2\nnull\nnull\nnull\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n\n\n\n\n\ncustomer_orders\n\nConverting null and NaN values into blanks '' in exclusions and extras\n\nBlanks indicate that the customer requested no extras/exclusions for the pizza, whereas null values would be ambiguous.\n\nSaving the transformations in a temporary table\n\nWe want to avoid permanently changing the raw data via UPDATE commands if possible.\n\n\n\nCREATE TABLE IF NOT EXISTS pizza_runner.customer_orders_cln AS (\n  SELECT\n    order_id,\n    customer_id,\n    pizza_id,\n    CASE \n      WHEN exclusions IS NULL \n        OR exclusions LIKE 'null' THEN ''\n      ELSE exclusions \n    END AS exclusions,\n    CASE \n      WHEN extras IS NULL\n        OR extras LIKE 'null' THEN ''\n      ELSE extras \n    END AS extras,\n    order_time\n  FROM pizza_runner.customer_orders\n)\n\n\nSELECT *\nFROM pizza_runner.customer_orders_cln\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\norder_id\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n1\n101\n1\n\n\n2020-01-01 18:05:02\n\n\n2\n101\n1\n\n\n2020-01-01 19:00:52\n\n\n3\n102\n1\n\n\n2020-01-02 23:51:23\n\n\n3\n102\n2\n\n\n2020-01-02 23:51:23\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n1\n4\n\n2020-01-04 13:23:46\n\n\n4\n103\n2\n4\n\n2020-01-04 13:23:46\n\n\n5\n104\n1\n\n1\n2020-01-08 21:00:29\n\n\n6\n101\n2\n\n\n2020-01-08 21:03:13\n\n\n7\n105\n2\n\n1\n2020-01-08 21:20:29\n\n\n\n\n\nrunner_orders\n\nConverting 'null' text values into null values for pickup_time, distance and duration\nExtracting only numbers and decimal spaces for the distance and duration columns\n\nUse regular expressions and NULLIFto convert non-numeric entries to null values\n\nConverting blanks, 'null' and NaN into null values for cancellation\nSaving the transformations in a temporary table\n\n\nCREATE TABLE IF NOT EXISTS pizza_runner.runner_orders_cln AS (\n  SELECT\n    order_id,\n    runner_id,\n    CASE WHEN pickup_time LIKE 'null' THEN null ELSE pickup_time END::timestamp AS pickup_time,\n    NULLIF(regexp_replace(distance, '[^0-9.]','','g'), '')::numeric AS distance,\n    NULLIF(regexp_replace(duration, '[^0-9.]','','g'), '')::numeric AS duration,\n    CASE WHEN cancellation IN ('null', 'NaN', '') THEN null ELSE cancellation END AS cancellation\n  FROM pizza_runner.runner_orders\n)\n\n\nSELECT *\nFROM pizza_runner.runner_orders_cln\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\n\n\n\n\n1\n1\n2020-01-01 18:15:34\n20.0\n32\nNA\n\n\n2\n1\n2020-01-01 19:10:54\n20.0\n27\nNA\n\n\n3\n1\n2020-01-03 00:12:37\n13.4\n20\nNA\n\n\n4\n2\n2020-01-04 13:53:03\n23.4\n40\nNA\n\n\n5\n3\n2020-01-08 21:10:57\n10.0\n15\nNA\n\n\n6\n3\nNA\nNA\nNA\nRestaurant Cancellation\n\n\n7\n2\n2020-01-08 21:30:45\n25.0\n25\nNA\n\n\n8\n2\n2020-01-10 00:15:02\n23.4\n15\nNA\n\n\n9\n2\nNA\nNA\nNA\nCustomer Cancellation\n\n\n10\n1\n2020-01-11 18:50:20\n10.0\n10\nNA\n\n\n\n\n\nLet’s preview the other four tables.\nrunners\n\nSELECT *\nFROM pizza_runner.runners\n\n\n4 records\n\n\nrunner_id\nregistration_date\n\n\n\n\n1\n2021-01-01\n\n\n2\n2021-01-03\n\n\n3\n2021-01-08\n\n\n4\n2021-01-15\n\n\n\n\n\npizza_names\n\nSELECT *\nFROM pizza_runner.pizza_names\n\n\n2 records\n\n\npizza_id\npizza_name\n\n\n\n\n1\nMeatlovers\n\n\n2\nVegetarian\n\n\n\n\n\npizza_recipes\n\nSELECT *\nFROM pizza_runner.pizza_recipes\n\n\n2 records\n\n\npizza_id\ntoppings\n\n\n\n\n1\n1, 2, 3, 4, 5, 6, 8, 10\n\n\n2\n4, 6, 7, 9, 11, 12\n\n\n\n\n\npizza_toppings\n\nSELECT *\nFROM pizza_runner.pizza_toppings\n\n\nDisplaying records 1 - 10\n\n\ntopping_id\ntopping_name\n\n\n\n\n1\nBacon\n\n\n2\nBBQ Sauce\n\n\n3\nBeef\n\n\n4\nCheese\n\n\n5\nChicken\n\n\n6\nMushrooms\n\n\n7\nOnions\n\n\n8\nPepperoni\n\n\n9\nPeppers\n\n\n10\nSalami"
  },
  {
    "objectID": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#case-study-questions",
    "href": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#case-study-questions",
    "title": "8 Week SQL Challenge #2 - Pizza Runner",
    "section": "Case Study Questions",
    "text": "Case Study Questions\nThis case study has LOTS of questions - they are broken up by area of focus including:\n\nA. Pizza Metrics\nB. Runner and Customer Experience\nC. Ingredient Optimisation\nD. Pricing and Ratings\nE. Bonus DML Challenges (DML = Data Manipulation Language)"
  },
  {
    "objectID": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#a.-pizza-metrics",
    "href": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#a.-pizza-metrics",
    "title": "8 Week SQL Challenge #2 - Pizza Runner",
    "section": "A. Pizza Metrics",
    "text": "A. Pizza Metrics\n\nHow many pizzas were ordered?\nHow many unique customer orders were made?\nHow many successful orders were delivered by each runner?\nHow many of each type of pizza was delivered?\nHow many Vegetarian and Meatlovers were ordered by each customer?\nWhat was the maximum number of pizzas delivered in a single order?\nFor each customer, how many delivered pizzas had at least 1 change and how many had no changes?\nHow many pizzas were delivered that had both exclusions and extras?\nWhat was the total volume of pizzas ordered for each hour of the day?\nWhat was the volume of orders for each day of the week?\n\n\nA. Q1\nHow many pizzas were ordered?\n\nSELECT \n  COUNT(*) AS pizza_count\nFROM pizza_runner.customer_orders_cln\n\n\n1 records\n\n\npizza_count\n\n\n\n\n14\n\n\n\n\n\n\n\nA. Q2\nHow many unique customer orders were made?\n\nSELECT \n  COUNT(DISTINCT order_id) AS order_count\nFROM pizza_runner.customer_orders_cln\n\n\n1 records\n\n\norder_count\n\n\n\n\n10\n\n\n\n\n\n\n\nA. Q3\nHow many successful orders were delivered by each runner?\n\nSELECT \n  runner_id,\n  COUNT(order_id) AS successful_orders\nFROM pizza_runner.runner_orders_cln\nWHERE cancellation IS NULL\nGROUP BY runner_id\n\n\n3 records\n\n\nrunner_id\nsuccessful_orders\n\n\n\n\n1\n4\n\n\n2\n3\n\n\n3\n1\n\n\n\n\n\n\n\nA. Q4\nHow many of each type of pizza was delivered?\n\nSELECT \n  pizza_name,\n  COUNT(pizza_id) AS count\nFROM pizza_runner.runner_orders_cln \nJOIN pizza_runner.customer_orders_cln\nUSING (order_id)\nJOIN pizza_runner.pizza_names\nUSING (pizza_id)\nWHERE cancellation IS NULL\nGROUP BY pizza_name\nORDER BY pizza_name\n\n\n2 records\n\n\npizza_name\ncount\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\nA. Q5\nHow many Vegetarian and Meatlovers were ordered by each customer?\n\nSELECT \n  customer_id,\n  SUM(CASE WHEN pizza_id = 1 THEN 1 ELSE 0 END) AS meatlovers,\n  SUM(CASE WHEN pizza_id = 2 THEN 1 ELSE 0 END) AS vegetarian\nFROM pizza_runner.customer_orders_cln\nGROUP BY 1\nORDER BY 1\n\n\n5 records\n\n\ncustomer_id\nmeatlovers\nvegetarian\n\n\n\n\n101\n2\n1\n\n\n102\n2\n1\n\n\n103\n3\n1\n\n\n104\n3\n0\n\n\n105\n0\n1\n\n\n\n\n\n\n\nA. Q6\nWhat was the maximum number of pizzas delivered in a single order?\n\nSELECT MAX(pizza_count) AS max_count\nFROM (\n    SELECT \n      order_id,\n      COUNT(*) AS pizza_count\n    FROM pizza_runner.runner_orders_cln\n    JOIN pizza_runner.customer_orders_cln\n    USING (order_id)\n    WHERE cancellation IS NULL\n    GROUP BY 1\n    ) AS subq;\n\n\n1 records\n\n\nmax_count\n\n\n\n\n3\n\n\n\n\n\n\n\nA. Q7\nFor each customer, how many delivered pizzas had at least 1 change and how many had no changes?\n\nSELECT \n  customer_id,\n  SUM(CASE WHEN exclusions = '' AND extras = '' THEN 1 ELSE 0 END) AS no_change,\n  SUM(CASE WHEN exclusions != '' OR extras != '' THEN 1 ELSE 0 END) AS change\nFROM pizza_runner.runner_orders_cln\nJOIN pizza_runner.customer_orders_cln \nUSING (order_id)\nWHERE cancellation IS NULL\nGROUP BY 1\n\n\n5 records\n\n\ncustomer_id\nno_change\nchange\n\n\n\n\n101\n2\n0\n\n\n102\n3\n0\n\n\n103\n0\n3\n\n\n104\n1\n2\n\n\n105\n0\n1\n\n\n\n\n\n\n\nA. Q8\nHow many pizzas were delivered that had both exclusions and extras?\n\nSELECT \n  SUM(CASE WHEN exclusions != '' AND extras != '' THEN 1 ELSE 0 END) \n    AS excl_and_extras_count\nFROM pizza_runner.runner_orders_cln\nJOIN pizza_runner.customer_orders_cln\nUSING (order_id)\nWHERE cancellation IS NULL\n\n\n1 records\n\n\nexcl_and_extras_count\n\n\n\n\n1\n\n\n\n\n\n\n\nA. Q9\nWhat was the total volume of pizzas ordered for each hour of the day?\n\nSELECT \n  DATE_PART('hour', order_time) AS hour,\n  COUNT(*) AS pizza_count\nFROM pizza_runner.customer_orders_cln\nGROUP BY 1\nORDER BY 1\n\n\n6 records\n\n\nhour\npizza_count\n\n\n\n\n11\n1\n\n\n13\n3\n\n\n18\n3\n\n\n19\n1\n\n\n21\n3\n\n\n23\n3\n\n\n\n\n\n\n\nA. Q10\nWhat was the volume of orders for each day of the week?\nNote: day of the week (0 - 6; Sunday is 0)\n\nSELECT \n  DATE_PART('dow', order_time) AS dow,\n    TO_CHAR(order_time, 'Day') AS day_of_week,\n  COUNT(*) AS pizza_count\nFROM pizza_runner.customer_orders_cln\nGROUP BY 1, 2\nORDER BY 1\n\n\n4 records\n\n\ndow\nday_of_week\npizza_count\n\n\n\n\n3\nWednesday\n5\n\n\n4\nThursday\n3\n\n\n5\nFriday\n1\n\n\n6\nSaturday\n5"
  },
  {
    "objectID": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#b.-runner-and-customer-experience",
    "href": "posts/sql-pizza-runner/8-week-sql-challenge-2-pizza-runner.html#b.-runner-and-customer-experience",
    "title": "8 Week SQL Challenge #2 - Pizza Runner",
    "section": "B. Runner and Customer Experience",
    "text": "B. Runner and Customer Experience\n\nHow many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\nWhat was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\nIs there any relationship between the number of pizzas and how long the order takes to prepare?\nWhat was the average distance travelled for each customer?\nWhat was the difference between the longest and shortest delivery times for all orders?\nWhat was the average speed for each runner for each delivery and do you notice any trend for these values?\nWhat is the successful delivery percentage for each runner?\n\n\nB. Q1\nHow many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nWITH runner_signups AS (\n  SELECT\n    runner_id,\n    registration_date,\n    registration_date - ((registration_date - '2021-01-01') % 7)  AS start_of_week\n  FROM pizza_runner.runners\n)\n\nSELECT\n  start_of_week,\n  COUNT(runner_id) AS signups\nFROM runner_signups\nGROUP BY start_of_week\nORDER BY start_of_week\n\n\n3 records\n\n\nstart_of_week\nsignups\n\n\n\n\n2021-01-01\n2\n\n\n2021-01-08\n1\n\n\n2021-01-15\n1\n\n\n\n\n\n\n\nB. Q2\nWhat was the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pickup the order?\n\nWITH t1 AS (\n  SELECT\n    order_id,\n    runner_id,\n    DATE_PART('minute', pickup_time - order_time)::numeric + \n      DATE_PART('second', pickup_time - order_time)::numeric / 60 \n      AS pickup_time\n  FROM pizza_runner.runner_orders_cln\n  JOIN pizza_runner.customer_orders_cln\n  USING (order_id)\n  WHERE cancellation IS NULL\n  GROUP BY 1, 2, 3\n  ORDER BY 1\n  )\n  \nSELECT \n  runner_id, \n  ROUND(AVG(pickup_time), 1) AS avg_pickup_time\nFROM t1\nGROUP BY 1\nORDER BY 1\n\n\n3 records\n\n\nrunner_id\navg_pickup_time\n\n\n\n\n1\n14.3\n\n\n2\n20.0\n\n\n3\n10.5\n\n\n\n\n\n\n\nB. Q3\nIs there any relationship between the number of pizzas and how long the order takes to prepare?\n\nWITH t1 AS (\n  SELECT \n    order_id,\n    COUNT(pizza_id) pizza_count,\n    AVG(pickup_time - order_time) AS pickup_time\n  FROM pizza_runner.customer_orders_cln\n  JOIN pizza_runner.runner_orders_cln\n  USING (order_id)\n  WHERE cancellation IS NULL\n  GROUP BY order_id\n  ORDER BY order_id\n  )\n\nSELECT \n  pizza_count,\n  ROUND(AVG(DATE_PART('minute', pickup_time)::numeric + \n    DATE_PART('second', pickup_time)::numeric / 60), 1) AS avg_pickup_time\nFROM t1\nGROUP BY pizza_count\nORDER BY pizza_count\n\n\n3 records\n\n\npizza_count\navg_pickup_time\n\n\n\n\n1\n12.4\n\n\n2\n18.4\n\n\n3\n29.3\n\n\n\n\n\n\n\nB. Q4\nWhat was the average distance travelled for each runner?\n\nSELECT \n  runner_id,\n  ROUND(AVG(distance), 1) AS avg_distance\nFROM pizza_runner.runner_orders_cln\nGROUP BY runner_id\nORDER BY runner_id\n\n\n3 records\n\n\nrunner_id\navg_distance\n\n\n\n\n1\n15.9\n\n\n2\n23.9\n\n\n3\n10.0\n\n\n\n\n\n\n\nB. Q5\nWhat was the difference between the longest and shortest delivery times for all orders?\n\nSELECT \n  MAX(duration) - MIN(duration) AS difference\nFROM pizza_runner.runner_orders_cln\n\n\n1 records\n\n\ndifference\n\n\n\n\n30\n\n\n\n\n\n\n\nB. Q6\nWhat was the average speed for each runner for each delivery and do you notice any trend for these values?\n\nSELECT \n  order_id,\n  runner_id,\n  distance,\n  duration,\n  COUNT(pizza_id) pizza_count,\n  ROUND(distance / (duration/60), 1) AS speed\nFROM pizza_runner.customer_orders_cln\nJOIN pizza_runner.runner_orders_cln\nUSING (order_id)\nWHERE cancellation IS NULL\nGROUP BY 1, 2, 3, 4\nORDER BY runner_id, pizza_count\n\n\n8 records\n\n\norder_id\nrunner_id\ndistance\nduration\npizza_count\nspeed\n\n\n\n\n2\n1\n20.0\n27\n1\n44.4\n\n\n1\n1\n20.0\n32\n1\n37.5\n\n\n10\n1\n10.0\n10\n2\n60.0\n\n\n3\n1\n13.4\n20\n2\n40.2\n\n\n7\n2\n25.0\n25\n1\n60.0\n\n\n8\n2\n23.4\n15\n1\n93.6\n\n\n4\n2\n23.4\n40\n3\n35.1\n\n\n5\n3\n10.0\n15\n1\n40.0\n\n\n\n\n\nWhile the fastest delivery only carried 1 pizza and the slowest delivery carried 3 pizzas, there is no clear trend that more pizzas slow down the delivery speed of an order.\n\n\nB. Q7\nWhat is the successful delivery percentage for each runner?\n\nSELECT\n  runner_id,\n  COUNT(order_id) AS ordered,\n  COUNT(pickup_time) AS delivered,\n  COUNT(pickup_time)::numeric/COUNT(order_id)*100 AS delivery_percentage\nFROM pizza_runner.runner_orders_cln\nGROUP BY runner_id\nORDER BY runner_id\n\n\n3 records\n\n\nrunner_id\nordered\ndelivered\ndelivery_percentage\n\n\n\n\n1\n4\n4\n100\n\n\n2\n4\n3\n75\n\n\n3\n2\n1\n50\n\n\n\n\n\n\ndbDisconnect(mycon)"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "title": "Cryptocurrency SQL Case Study",
    "section": "",
    "text": "On November 18, 2022 I attended the SQL masterclass “SQL and PostgreSQL for Data Analytics”, presented live by Danny Ma on O’Reilly platform.\nThis Github repository contains all the necessary data, sql scripts, and other materials.\nI’m posting some parts of it here for my own reference, but I strongly recommend that you register for that event that takes place every once in a while. It’s free and worth it. Danny Ma is an excellent teacher and his explanations are outstanding.\n\n\nFor the purposes of this project, I created the local trading Postgres database on my machine and ran the sql script to create and populate the tables.\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database (typically via the DBI::dbConnect() function).\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"trading\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> trading@localhost:5432"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "title": "Cryptocurrency SQL Case Study",
    "section": "Introduction",
    "text": "Introduction\nIn this fictitious case study - Danny’s data mentors from the Data With Danny team have been busy trading cryptocurrency markets since 2017.\nThe main purpose for this case study is to analyze the performance of the DWD mentors over time. We will accomplish this by writing SQL queries to utilize all available datasets to answer a series of realistic business questions.\n\nAvailable Datasets\nAll of our data for this case study exists within the trading schema in the PostgreSQL database.\nThere are 3 data tables available in this schema:\n\nmembers\nprices\ntransactions\n\n\n\nEntity Relationship Diagram\nI drew the ERD here.\n\n\n\nData Dictionary and Overview\nThe trading.members table consists of information about the mentors from the Data With Danny team.\n\ntrading.members table\n\n\nColumn name\nDescription\n\n\n\n\nmember_id\nunique id for each mentor\n\n\nfirst_name\nfirst name for each mentor\n\n\nregion\nregion where each mentor is from\n\n\n\n\nSELECT * FROM trading.members\n\n\n14 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\nc9f0f8\nAbe\nUnited States\n\n\n45c48c\nBen\nAustralia\n\n\nd3d944\nEnoch\nAfrica\n\n\n6512bd\nVikram\nIndia\n\n\nc20ad4\nLeah\nAsia\n\n\nc51ce4\nPavan\nAustralia\n\n\naab323\nSonia\nAustralia\n\n\n\n\n\nThe trading.prices table consists of daily price and volume information from January 2017 through to August 2021 for the 2 most popular cryptocurrency tickers: Bitcoin and Ethereum.\n\ntrading.prices table\n\n\nColumn name\nDescription\n\n\n\n\nticker\none of either BTC or ETH\n\n\nmarket_date\nthe date for each record\n\n\nprice\nclosing price at end of day\n\n\nopen\nthe opening price\n\n\nhigh\nthe highest price for that day\n\n\nlow\nthe lowest price for that day\n\n\nvolume\nthe total volume traded\n\n\nchange\n% change in daily price\n\n\n\nThe first 5 rows from this dataset.\n\nSELECT * FROM trading.prices LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nopen\nhigh\nlow\nvolume\nchange\n\n\n\n\nETH\n2021-08-29\n3177.84\n3243.96\n3282.21\n3162.79\n582.04K\n-2.04%\n\n\nETH\n2021-08-28\n3243.90\n3273.78\n3284.58\n3212.24\n466.21K\n-0.91%\n\n\nETH\n2021-08-27\n3273.58\n3093.78\n3279.93\n3063.37\n839.54K\n5.82%\n\n\nETH\n2021-08-26\n3093.54\n3228.03\n3249.62\n3057.48\n118.44K\n-4.17%\n\n\nETH\n2021-08-25\n3228.15\n3172.12\n3247.43\n3080.70\n923.13K\n1.73%\n\n\n\n\n\nThe trading.transactions table consists of buy and sell transactions data for each trade made by the DWD mentors.\n\ntrading.transactions table\n\n\nColumn name\nDescription\n\n\n\n\ntxn_id\nunique ID for each transaction\n\n\nmember_id\nmember identifier for each trade\n\n\nticker\nthe ticker for each trade\n\n\ntxn_date\nthe date for each transaction\n\n\ntxn_type\neither BUY or SELL\n\n\nquantity\nthe total quantity for each trade\n\n\npercentage_fee\n% of total amount charged as fees\n\n\ntxn_time\nthe timestamp for each trade\n\n\n\nThe first 5 frows from this transactions table.\n\nSELECT * FROM trading.transactions LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\ntxn_id\nmember_id\nticker\ntxn_date\ntxn_type\nquantity\npercentage_fee\ntxn_time\n\n\n\n\n1\nc81e72\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n2\neccbc8\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n3\na87ff6\nBTC\n2017-01-01\nBUY\n50\n0.0\n2017-01-01\n\n\n4\ne4da3b\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n5\n167909\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 1️⃣: Basic Data Analysis Techniques",
    "text": "PART 1️⃣: Basic Data Analysis Techniques\n\nQuestion 1.1\nShow only the top 5 rows from the trading.members table.\n\nSELECT * \nFROM trading.members\nLIMIT 5\n\n\n5 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n\n\n\n\n\nQuestion 1.2\nSort all the rows in the trading.members table by first_name in alphabetical order and show the top 3 rows with all columns.\n\nSELECT * \nFROM trading.members\nORDER BY first_name\nLIMIT 3\n\n\n3 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc9f0f8\nAbe\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n\n\n\n\n\nQuestion 1.3\nCount the number of records from the trading.members table which have United States as the region value.\n\nSELECT \n  COUNT(*) AS record_count\nFROM trading.members\nWHERE region = 'United States'\n\n\n1 records\n\n\nrecord_count\n\n\n\n\n7\n\n\n\n\n\n\n\nQuestion 1.4\nSelect only the first_name and region columns for mentors who are not from Australia.\n\nSELECT \n  first_name, \n  region\nFROM trading.members\nWHERE region != 'Australia'\n\n\nDisplaying records 1 - 10\n\n\nfirst_name\nregion\n\n\n\n\nVipul\nUnited States\n\n\nCharlie\nUnited States\n\n\nNandita\nUnited States\n\n\nRowan\nUnited States\n\n\nAyush\nUnited States\n\n\nAlex\nUnited States\n\n\nAbe\nUnited States\n\n\nEnoch\nAfrica\n\n\nVikram\nIndia\n\n\nLeah\nAsia\n\n\n\n\n\n\n\nQuestion 1.5\nReturn only the unique region values from the trading.members table and sort the output by reverse alphabetical order.\n\nSELECT DISTINCT region\nFROM trading.members\nORDER BY region DESC\n\n\n5 records\n\n\nregion\n\n\n\n\nUnited States\n\n\nIndia\n\n\nAustralia\n\n\nAsia\n\n\nAfrica"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 2️⃣: Aggregate Functions for Data Analysis",
    "text": "PART 2️⃣: Aggregate Functions for Data Analysis\n\nQuestion 2.1\nHow many records are there per ticker value in the trading.prices table?\n\nSELECT\n  ticker,\n  COUNT(*) AS record_count\nFROM trading.prices\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nrecord_count\n\n\n\n\nBTC\n1702\n\n\nETH\n1702\n\n\n\n\n\n\n\nQuestion 2.2\nWhat is the maximum, minimum values for the price column for both Bitcoin and Ethereum in 2020?\n\nSELECT \n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nmin_price\nmax_price\n\n\n\n\nBTC\n4826.0\n28949.4\n\n\nETH\n107.9\n751.8\n\n\n\n\n\n\n\nQuestion 2.3\nWhat is the annual minimum, maximum and average price for each ticker?\n\nInclude a calendar_year column with the year from 2017 through to 2021\nCalculate a spread column which calculates the difference between the min and max prices\nRound the average price output to 2 decimal places\nSort the output in chronological order with Bitcoin records before Ethereum within each year\n\n\nSELECT\n  EXTRACT(YEAR FROM market_date) AS calendar_year,\n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price,\n  ROUND(AVG(price)::NUMERIC, 2) AS avg_price,\n  MAX(price) - MIN(price) AS spread\nFROM trading.prices\nGROUP BY calendar_year, ticker\nORDER BY calendar_year, ticker\n\n\nDisplaying records 1 - 10\n\n\ncalendar_year\nticker\nmin_price\nmax_price\navg_price\nspread\n\n\n\n\n2017\nBTC\n785.40\n19345.50\n3981.07\n18560.10\n\n\n2017\nETH\n8.20\n799.98\n220.34\n791.78\n\n\n2018\nBTC\n3228.70\n17172.30\n7552.16\n13943.60\n\n\n2018\nETH\n83.81\n1380.00\n481.33\n1296.19\n\n\n2019\nBTC\n3397.70\n13063.80\n7371.82\n9666.10\n\n\n2019\nETH\n104.55\n338.54\n180.99\n233.99\n\n\n2020\nBTC\n4826.00\n28949.40\n11111.63\n24123.40\n\n\n2020\nETH\n107.90\n751.80\n307.30\n643.90\n\n\n2021\nBTC\n29359.90\n63540.90\n44353.55\n34181.00\n\n\n2021\nETH\n729.12\n4167.78\n2199.12\n3438.66\n\n\n\n\n\n\n\nQuestion 2.4\nWhat is the monthly average of the price column for each ticker from January 2020 and after?\n\nCreate a month_start column with the first day of each month\nSort the output by ticker in alphabetical order and months in chronological order\nRound the average_price column to 2 decimal places\n\n\nSELECT\n  ticker,\n  DATE_TRUNC('MON', market_date)::DATE AS month_start,\n  ROUND(AVG(price)::NUMERIC, 2) AS average_price\nFROM trading.prices\nWHERE market_date >= '2020-01-01'\nGROUP BY ticker, month_start\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\naverage_price\n\n\n\n\nBTC\n2020-01-01\n8378.80\n\n\nBTC\n2020-02-01\n9636.56\n\n\nBTC\n2020-03-01\n6863.11\n\n\nBTC\n2020-04-01\n7211.03\n\n\nBTC\n2020-05-01\n9253.55\n\n\nBTC\n2020-06-01\n9481.85\n\n\nBTC\n2020-07-01\n9592.10\n\n\nBTC\n2020-08-01\n11638.41\n\n\nBTC\n2020-09-01\n10643.33\n\n\nBTC\n2020-10-01\n11888.36"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 3️⃣: Case When Statements",
    "text": "PART 3️⃣: Case When Statements\n\nQuestion 3.1\nConvert the volume column in the trading.prices table with an adjusted integer value to take into the unit values\n\nReturn only the market_date, price, volume and adjusted_volume columns for the first 10 days of August 2021 for Ethereum only\n\n\nSELECT\n  market_date,\n  price,\n  volume,    \n  CASE\n    WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000 \n    WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000000 \n    WHEN volume = '-' THEN 0\n    END AS adjusted_volume\nFROM trading.prices\nWHERE ticker = 'ETH'\n  AND market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY market_date\n\n\nDisplaying records 1 - 10\n\n\nmarket_date\nprice\nvolume\nadjusted_volume\n\n\n\n\n2021-08-01\n2556.23\n1.20M\n1200000\n\n\n2021-08-02\n2608.04\n970.67K\n970670\n\n\n2021-08-03\n2506.65\n158.45K\n158450\n\n\n2021-08-04\n2725.29\n1.23M\n1230000\n\n\n2021-08-05\n2827.21\n1.65M\n1650000\n\n\n2021-08-06\n2889.43\n1.06M\n1060000\n\n\n2021-08-07\n3158.00\n64.84K\n64840\n\n\n2021-08-08\n3012.07\n1.25M\n1250000\n\n\n2021-08-09\n3162.93\n1.44M\n1440000\n\n\n2021-08-10\n3140.71\n1.12M\n1120000\n\n\n\n\n\n\n\nQuestion 3.2\nHow many “breakout” days were there in 2020 where the price column is greater than the open column for each ticker? In the same query also calculate the number of “non breakout” days where the price column was lower than or equal to the open column.\n\nSELECT\n  ticker,\n  SUM(CASE WHEN price > open THEN 1 ELSE 0 END) AS breakout_days,\n  SUM(CASE WHEN price <= open THEN 1 ELSE 0 END) AS non_breakout_days\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nbreakout_days\nnon_breakout_days\n\n\n\n\nBTC\n207\n159\n\n\nETH\n200\n166\n\n\n\n\n\n\n\nQuestion 3.3\nWhat was the final quantity Bitcoin and Ethereum held by all Data With Danny mentors based off the trading.transactions table?\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'SELL' THEN -quantity ELSE quantity END) AS final_btc_holding\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nfinal_btc_holding\n\n\n\n\nBTC\n42848.67\n\n\nETH\n32801.04\n\n\n\n\n\nDivided by quantity bought and quantity sold.\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'BUY' THEN quantity ELSE 0 END) AS qty_bought,\n  SUM(CASE WHEN txn_type = 'SELL' THEN quantity ELSE 0 END) AS qty_sold\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nqty_bought\nqty_sold\n\n\n\n\nBTC\n53250.15\n10401.485\n\n\nETH\n42599.20\n9798.154"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 4️⃣: Window Functions",
    "text": "PART 4️⃣: Window Functions\n\nQuestion 4.1\nWhat are the market_date, price and volume and price_rank values for the days with the top 5 highest price values for each tickers in the trading.prices table?\n\nThe price_rank column is the ranking for price values for each ticker with rank = 1 for the highest value.\nReturn the output for Bitcoin, followed by Ethereum in price rank order.\n\n\nWITH cte_rank AS (\n  SELECT\n    ticker,\n    market_date,\n    price, \n    volume,\n    RANK() OVER (PARTITION BY ticker ORDER BY price DESC) AS price_rank\n  FROM trading.prices\n)\n\nSELECT *\nFROM cte_rank\nWHERE price_rank <= 5\nORDER BY ticker, price_rank\n\n\nDisplaying records 1 - 10\n\n\nticker\nmarket_date\nprice\nvolume\nprice_rank\n\n\n\n\nBTC\n2021-04-13\n63540.90\n126.56K\n1\n\n\nBTC\n2021-04-15\n63216.00\n76.97K\n2\n\n\nBTC\n2021-04-14\n62980.40\n130.43K\n3\n\n\nBTC\n2021-04-16\n61379.70\n136.85K\n4\n\n\nBTC\n2021-03-13\n61195.30\n134.64K\n5\n\n\nETH\n2021-05-11\n4167.78\n1.27M\n1\n\n\nETH\n2021-05-14\n4075.38\n2.06M\n2\n\n\nETH\n2021-05-10\n3947.90\n2.70M\n3\n\n\nETH\n2021-05-09\n3922.23\n1.94M\n4\n\n\nETH\n2021-05-08\n3905.55\n1.34M\n5\n\n\n\n\n\n\nQuestion 4.1 -  solution\n\nmembers <- read_csv(\"data/members_tbl.csv\")\nprices <- read_csv(\"data/prices_tbl.csv\")\ntransactions <- read_csv(\"data/transactions_tbl.csv\")\n\nprices %>% head()\n\n# A tibble: 6 × 8\n  ticker market_date price  open  high   low volume  change\n  <chr>  <date>      <dbl> <dbl> <dbl> <dbl> <chr>   <chr> \n1 ETH    2021-08-29  3178. 3244. 3282. 3163. 582.04K -2.04%\n2 ETH    2021-08-28  3244. 3274. 3285. 3212. 466.21K -0.91%\n3 ETH    2021-08-27  3274. 3094. 3280. 3063. 839.54K 5.82% \n4 ETH    2021-08-26  3094. 3228. 3250. 3057. 118.44K -4.17%\n5 ETH    2021-08-25  3228. 3172. 3247. 3081. 923.13K 1.73% \n6 ETH    2021-08-24  3173. 3323. 3358. 3151. 988.82K -4.41%\n\n\n\n# R solution\nprices %>% \n  select(ticker, market_date, price, volume) %>% \n  group_by(ticker) %>% \n  arrange(ticker, desc(price)) %>% \n  mutate(price_rank = row_number()) %>% \n  filter(price_rank <=5)\n\n# A tibble: 10 × 5\n# Groups:   ticker [2]\n   ticker market_date  price volume  price_rank\n   <chr>  <date>       <dbl> <chr>        <int>\n 1 BTC    2021-04-13  63541. 126.56K          1\n 2 BTC    2021-04-15  63216  76.97K           2\n 3 BTC    2021-04-14  62980. 130.43K          3\n 4 BTC    2021-04-16  61380. 136.85K          4\n 5 BTC    2021-03-13  61195. 134.64K          5\n 6 ETH    2021-05-11   4168. 1.27M            1\n 7 ETH    2021-05-14   4075. 2.06M            2\n 8 ETH    2021-05-10   3948. 2.70M            3\n 9 ETH    2021-05-09   3922. 1.94M            4\n10 ETH    2021-05-08   3906. 1.34M            5\n\n\n\n\n\nQuestion 4.2\nCalculate a 7 day rolling average for the price and volume columns in the trading.prices table for each ticker.\n\nReturn only the first 10 days of August 2021\n\n\n-- Step 1 - Adjusted prices CTE\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\n-- Step 2 - Moving Averages CTE\ncte_moving_averages AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    AVG(price) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_price,\n    volume,\n    AVG(volume) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_volume\n  FROM cte_adjusted_prices\n)\n\n-- final output\nSELECT * FROM cte_moving_averages\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date;\n\n\n20 records\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nmoving_avg_price\nvolume\nmoving_avg_volume\n\n\n\n\nBTC\n2021-08-01\n39878.30\n40052.657\n80330\n103645.71\n\n\nBTC\n2021-08-02\n39168.40\n40322.914\n74810\n88957.14\n\n\nBTC\n2021-08-03\n38130.30\n40134.100\n260\n74674.29\n\n\nBTC\n2021-08-04\n39736.90\n40096.057\n79220\n64717.14\n\n\nBTC\n2021-08-05\n40867.20\n40219.743\n130600\n72617.14\n\n\nBTC\n2021-08-06\n42795.40\n40304.314\n111930\n74542.86\n\n\nBTC\n2021-08-07\n44614.20\n40741.529\n112840\n84284.29\n\n\nBTC\n2021-08-08\n43792.80\n41300.743\n105250\n87844.29\n\n\nBTC\n2021-08-09\n46284.30\n42317.300\n117080\n93882.86\n\n\nBTC\n2021-08-10\n45593.80\n43383.514\n80550\n105352.86\n\n\nETH\n2021-08-01\n2556.23\n2394.166\n1200000\n1069824.29\n\n\nETH\n2021-08-02\n2608.04\n2448.239\n970670\n938491.43\n\n\nETH\n2021-08-03\n2506.65\n2477.729\n158450\n782555.71\n\n\nETH\n2021-08-04\n2725.29\n2538.611\n1230000\n819850.00\n\n\nETH\n2021-08-05\n2827.21\n2602.366\n1650000\n963742.86\n\n\nETH\n2021-08-06\n2889.43\n2663.577\n1060000\n968028.57\n\n\nETH\n2021-08-07\n3158.00\n2752.979\n64840\n904851.43\n\n\nETH\n2021-08-08\n3012.07\n2818.099\n1250000\n911994.29\n\n\nETH\n2021-08-09\n3162.93\n2897.369\n1440000\n979041.43\n\n\nETH\n2021-08-10\n3140.71\n2987.949\n1120000\n1116405.71\n\n\n\n\n\n\nQuestion 4.2 -  solution\n\nprices %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  group_by(ticker) %>% \n  arrange(ticker, market_date) %>% \n  mutate(moving_avg_price = zoo::rollmean(price, k = 7, align = \"right\", fill = NA),\n         moving_avg_volume = zoo::rollmean(volume, k = 7, align = \"right\", fill = NA)) %>% \n  select(ticker, market_date, price, moving_avg_price, volume, moving_avg_volume) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 6\n# Groups:   ticker [2]\n   ticker market_date  price moving_avg_price  volume moving_avg_volume\n   <chr>  <date>       <dbl>            <dbl>   <dbl>             <dbl>\n 1 BTC    2021-08-01  39878.           40053.   80330           103646.\n 2 BTC    2021-08-02  39168.           40323.   74810            88957.\n 3 BTC    2021-08-03  38130.           40134.     260            74674.\n 4 BTC    2021-08-04  39737.           40096.   79220            64717.\n 5 BTC    2021-08-05  40867.           40220.  130600            72617.\n 6 BTC    2021-08-06  42795.           40304.  111930            74543.\n 7 BTC    2021-08-07  44614.           40742.  112840            84284.\n 8 BTC    2021-08-08  43793.           41301.  105250            87844.\n 9 BTC    2021-08-09  46284.           42317.  117080            93883.\n10 BTC    2021-08-10  45594.           43384.   80550           105353.\n11 ETH    2021-08-01   2556.            2394. 1200000          1069824.\n12 ETH    2021-08-02   2608.            2448.  970670           938491.\n13 ETH    2021-08-03   2507.            2478.  158450           782556.\n14 ETH    2021-08-04   2725.            2539. 1230000           819850 \n15 ETH    2021-08-05   2827.            2602. 1650000           963743.\n16 ETH    2021-08-06   2889.            2664. 1060000           968029.\n17 ETH    2021-08-07   3158             2753.   64840           904851.\n18 ETH    2021-08-08   3012.            2818. 1250000           911994.\n19 ETH    2021-08-09   3163.            2897. 1440000           979041.\n20 ETH    2021-08-10   3141.            2988. 1120000          1116406.\n\n\n\n\n\nQuestion 4.3\nCalculate the monthly cumulative volume traded for each ticker in 2020\n\nSort the output by ticker in chronological order with the month_start as the first day of each month\n\n\nWITH cte_monthly_volume AS (\n  SELECT\n    ticker,\n    DATE_TRUNC('MON', market_date)::DATE AS month_start,\n    SUM(\n      CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END\n  ) AS monthly_volume\n  FROM trading.prices\n  WHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\n  GROUP BY ticker, month_start\n)\n\nSELECT\n  ticker,\n  month_start,\n  SUM(monthly_volume) OVER (\n    PARTITION BY ticker\n    ORDER BY month_start\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n  ) AS cumulative_monthly_volume\nFROM cte_monthly_volume\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\ncumulative_monthly_volume\n\n\n\n\nBTC\n2020-01-01\n23451920\n\n\nBTC\n2020-02-01\n46839130\n\n\nBTC\n2020-03-01\n94680450\n\n\nBTC\n2020-04-01\n134302740\n\n\nBTC\n2020-05-01\n172687010\n\n\nBTC\n2020-06-01\n188026610\n\n\nBTC\n2020-07-01\n201272600\n\n\nBTC\n2020-08-01\n216762630\n\n\nBTC\n2020-09-01\n300641440\n\n\nBTC\n2020-10-01\n303060020\n\n\n\n\n\n\nQuestion 4.3 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  filter(market_date >= \"2020-01-01\", market_date <= \"2020-12-31\") %>% \n  mutate(month_start = lubridate::floor_date(market_date, unit = \"month\")) %>% \n  group_by(ticker, month_start) %>% \n  summarise(monthly_volume = sum(\n    case_when(\n      str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n      str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n      str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n    )\n  ) %>% \n  ungroup() %>% \n  group_by(ticker) %>% \n  mutate(cumulative_monthly_volume = cumsum(monthly_volume)) %>% \n  ungroup()\n\n# A tibble: 24 × 4\n   ticker month_start monthly_volume cumulative_monthly_volume\n   <chr>  <date>               <dbl>                     <dbl>\n 1 BTC    2020-01-01        23451920                  23451920\n 2 BTC    2020-02-01        23387210                  46839130\n 3 BTC    2020-03-01        47841320                  94680450\n 4 BTC    2020-04-01        39622290                 134302740\n 5 BTC    2020-05-01        38384270                 172687010\n 6 BTC    2020-06-01        15339600                 188026610\n 7 BTC    2020-07-01        13245990                 201272600\n 8 BTC    2020-08-01        15490030                 216762630\n 9 BTC    2020-09-01        83878810                 300641440\n10 BTC    2020-10-01         2418580                 303060020\n# … with 14 more rows\n\n\n\n\n\nQuestion 4.4\nCalculate the daily percentage change in volume for each ticker in the trading.prices table\n\nPercentage change can be calculated as (current - previous) / previous\nMultiply the percentage by 100 and round the value to 2 decimal places\nReturn data for the first 10 days of August 2021\n\n\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\ncte_previous_volume AS (\n  SELECT\n    ticker,\n    market_date,\n    volume,\n    LAG(volume) OVER (PARTITION BY ticker ORDER BY market_date) AS previous_volume\n  FROM cte_adjusted_prices\n  WHERE volume != 0\n)\n  \nSELECT\n  ticker,\n  market_date,\n  volume,\n  previous_volume,\n  ROUND(100 * (volume - previous_volume) / previous_volume, 2) AS daily_change\nFROM cte_previous_volume\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date\n\n\n20 records\n\n\nticker\nmarket_date\nvolume\nprevious_volume\ndaily_change\n\n\n\n\nBTC\n2021-08-01\n80330\n44650\n79.91\n\n\nBTC\n2021-08-02\n74810\n80330\n-6.87\n\n\nBTC\n2021-08-03\n260\n74810\n-99.65\n\n\nBTC\n2021-08-04\n79220\n260\n30369.23\n\n\nBTC\n2021-08-05\n130600\n79220\n64.86\n\n\nBTC\n2021-08-06\n111930\n130600\n-14.30\n\n\nBTC\n2021-08-07\n112840\n111930\n0.81\n\n\nBTC\n2021-08-08\n105250\n112840\n-6.73\n\n\nBTC\n2021-08-09\n117080\n105250\n11.24\n\n\nBTC\n2021-08-10\n80550\n117080\n-31.20\n\n\nETH\n2021-08-01\n1200000\n507080\n136.65\n\n\nETH\n2021-08-02\n970670\n1200000\n-19.11\n\n\nETH\n2021-08-03\n158450\n970670\n-83.68\n\n\nETH\n2021-08-04\n1230000\n158450\n676.27\n\n\nETH\n2021-08-05\n1650000\n1230000\n34.15\n\n\nETH\n2021-08-06\n1060000\n1650000\n-35.76\n\n\nETH\n2021-08-07\n64840\n1060000\n-93.88\n\n\nETH\n2021-08-08\n1250000\n64840\n1827.82\n\n\nETH\n2021-08-09\n1440000\n1250000\n15.20\n\n\nETH\n2021-08-10\n1120000\n1440000\n-22.22\n\n\n\n\n\n\nQuestion 4.4 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  arrange(ticker, market_date) %>% \n  mutate(volume_prev_day = lag(volume)) %>% \n  mutate(daily_change_perc = round(100*(volume - volume_prev_day)/volume_prev_day, 2)) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 5\n   ticker market_date  volume volume_prev_day daily_change_perc\n   <chr>  <date>        <dbl>           <dbl>             <dbl>\n 1 BTC    2021-08-01    80330           44650             79.9 \n 2 BTC    2021-08-02    74810           80330             -6.87\n 3 BTC    2021-08-03      260           74810            -99.6 \n 4 BTC    2021-08-04    79220             260          30369.  \n 5 BTC    2021-08-05   130600           79220             64.9 \n 6 BTC    2021-08-06   111930          130600            -14.3 \n 7 BTC    2021-08-07   112840          111930              0.81\n 8 BTC    2021-08-08   105250          112840             -6.73\n 9 BTC    2021-08-09   117080          105250             11.2 \n10 BTC    2021-08-10    80550          117080            -31.2 \n11 ETH    2021-08-01  1200000          507080            137.  \n12 ETH    2021-08-02   970670         1200000            -19.1 \n13 ETH    2021-08-03   158450          970670            -83.7 \n14 ETH    2021-08-04  1230000          158450            676.  \n15 ETH    2021-08-05  1650000         1230000             34.2 \n16 ETH    2021-08-06  1060000         1650000            -35.8 \n17 ETH    2021-08-07    64840         1060000            -93.9 \n18 ETH    2021-08-08  1250000           64840           1828.  \n19 ETH    2021-08-09  1440000         1250000             15.2 \n20 ETH    2021-08-10  1120000         1440000            -22.2"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 5️⃣: Table Joins",
    "text": "PART 5️⃣: Table Joins\n\nQuestion 5.1 - Inner Joins\nWhich top 3 mentors have the most Bitcoin quantity? Return the first_name of the mentors and sort the output from highest to lowest total_quantity.\n\nSELECT\n  m.first_name,\n  SUM(\n    CASE\n      WHEN t.txn_type = 'BUY' THEN t.quantity \n      WHEN t.txn_type = 'SELL' THEN -t.quantity \n      END\n  ) AS total_quantity\nFROM trading.transactions t\nINNER JOIN trading.members m\n  ON t.member_id = m.member_id\nWHERE ticker = 'BTC' \nGROUP By m.first_name\nORDER BY total_quantity DESC\nLIMIT 3\n\n\n3 records\n\n\nfirst_name\ntotal_quantity\n\n\n\n\nNandita\n4160.220\n\n\nLeah\n4046.091\n\n\nAyush\n3945.198\n\n\n\n\n\n\n\nQuestion 5.2 - Left Joins\nShow the market_date values which have less than 5 transactions? Sort the output in reverse chronological order.\n\nSELECT \n  p.market_date,\n  COUNT(t.txn_id) AS transaction_count\nFROM trading.prices p\nLEFT JOIN trading.transactions t\n  ON p.market_date = t.txn_date\n  AND p.ticker = t.ticker\nGROUP BY p.market_date\nHAVING COUNT(t.txn_id) < 5\nORDER BY p.market_date DESC\n\n\n8 records\n\n\nmarket_date\ntransaction_count\n\n\n\n\n2021-08-29\n0\n\n\n2021-08-28\n0\n\n\n2021-07-17\n3\n\n\n2021-01-06\n4\n\n\n2020-01-17\n4\n\n\n2019-07-15\n4\n\n\n2019-06-14\n3\n\n\n2018-10-20\n4\n\n\n\n\n\n\n\nQuestion 5.3 - Multiple Table Joins\nPart 1: Calculate the Dollar Cost Average\nWhat is the dollar cost average (btc_dca) for all Bitcoin purchases by region for each calendar year?\n\nCreate a column called year_start and use the start of the calendar year\nThe dollar cost average calculation is btc_dca = SUM(quantity x price) / SUM(quantity)\n\nPart 2: Yearly Dollar Cost Average Ranking\nUse this btc_dca value to generate a dca_ranking column for each year\n\nThe region with the lowest btc_dca each year has a rank of 1\n\nPart 3: Dollar Cost Average Yearly Percentage Change\nCalculate the yearly percentage change in DCA for each region to 2 decimal places\n\nThis calculation is (current - previous) / previous\n\nFinally order the output by region and year_start columns.\n\nWITH cte_dollar_cost_average AS (\n  SELECT\n    DATE_TRUNC('YEAR', transactions.txn_date)::DATE AS year_start,\n    members.region,\n    SUM(transactions.quantity * prices.price) / SUM(transactions.quantity) AS btc_dca\n  FROM trading.transactions\n  INNER JOIN trading.prices\n    ON transactions.ticker = prices.ticker\n    AND transactions.txn_date = prices.market_date\n  INNER JOIN trading.members\n    ON transactions.member_id = members.member_id\n  WHERE transactions.ticker = 'BTC'\n    AND transactions.txn_type = 'BUY'\n  GROUP BY year_start, members.region\n),\n  \ncte_window_functions AS (\n  SELECT\n    year_start,\n    region,\n    btc_dca,\n    RANK() OVER (PARTITION BY year_start ORDER BY btc_dca) AS dca_ranking,\n    LAG(btc_dca) OVER (PARTITION BY region ORDER BY year_start) AS previous_btc_dca\n  FROM cte_dollar_cost_average\n)\n  \nSELECT\n  year_start,\n  region,\n  btc_dca,\n  dca_ranking,\n  ROUND(\n    (100 * (btc_dca - previous_btc_dca) / previous_btc_dca)::NUMERIC,\n    2\n  ) AS dca_percentage_change\nFROM cte_window_functions\nORDER BY region, year_start\n\n\nDisplaying records 1 - 10\n\n\nyear_start\nregion\nbtc_dca\ndca_ranking\ndca_percentage_change\n\n\n\n\n2017-01-01\nAfrica\n3987.626\n4\nNA\n\n\n2018-01-01\nAfrica\n7690.713\n3\n92.86\n\n\n2019-01-01\nAfrica\n7368.820\n4\n-4.19\n\n\n2020-01-01\nAfrica\n11114.125\n3\n50.83\n\n\n2021-01-01\nAfrica\n44247.215\n2\n298.12\n\n\n2017-01-01\nAsia\n4002.939\n5\nNA\n\n\n2018-01-01\nAsia\n7829.999\n4\n95.61\n\n\n2019-01-01\nAsia\n7267.679\n1\n-7.18\n\n\n2020-01-01\nAsia\n10759.621\n2\n48.05\n\n\n2021-01-01\nAsia\n44570.901\n4\n314.24\n\n\n\n\n\n\nThanks for reading!"
  }
]