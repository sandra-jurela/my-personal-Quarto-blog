[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Let’s connect",
    "section": "",
    "text": "Hi and welcome to my personal blog. My name is Sandra Jurela and here I will be sharing my journey into Data Science. Recently I have developed a real passion for Data Science and this blog truly is a manifestation of that.\nI hold a Master’s degree in Civil Engineering (Hydrology and Water Resources Science). I started my career as a construction designer, and then for 10 years worked as a hydrologist at the Croatian Meteorological and Hydrological Service. I currently work for my family’s business, and freelance as a data analyst.\nI live in Zagreb, Croatia, with my two boys (life partner and our sweet son).\nIf you’d like to connect with me, please do.\nI hope you enjoy your visit!\n\n\nFeel free to visit my Tableau Public repository.."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sandra Jurela",
    "section": "",
    "text": "Cyclistic Bike-Share Analysis\n\n\n\n\n\n\n\neda\n\n\ndata cleaning\n\n\ndata wrangling\n\n\ndata visualization\n\n\nr\n\n\n\n\nThe latest version of my capstone project for the Google Data Analytics Professional Certificate.\n\n\n\n\n\n\nAug 25, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nBank Customer Retention Analysis in Python\n\n\n\n\n\n\n\npython\n\n\naltair\n\n\neda\n\n\n\n\nWhat marketing campaigns could help reduce customer churn?\n\n\n\n\n\n\nJun 10, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nShiny App - Mass Shootings in the USA\n\n\n\n\n\n\n\nr\n\n\nshiny\n\n\neda\n\n\ndata cleaning\n\n\ndata wrangling\n\n\n\n\nEDA with Shiny app on mass shootings between August 20th, 1982 and October 25th, 2023.\n\n\n\n\n\n\nMar 1, 2023\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Segmentation With K-Means and UMAP\n\n\n\n\n\n\n\nr\n\n\nk-means\n\n\numap\n\n\n\n\nA report for the non-technical marketing team.\n\n\n\n\n\n\nDec 15, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCryptocurrency SQL Case Study\n\n\n\n\n\n\n\nsql\n\n\npostgresql\n\n\neda\n\n\n\n\nData with Danny - SQL masterclass - O’Reilly. With R solutions for Part 4: Window functions.\n\n\n\n\n\n\nNov 25, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nScraping 200 Best Movies of 2010s from Rotten Tomatoes\n\n\n\n\n\n\n\nweb scraping\n\n\ndata wrangling\n\n\nrvest\n\n\nr\n\n\ntableau\n\n\n\n\nScraping data on movies from Rotten Tomatoes and finally creating a dashboard in Tableau\n\n\n\n\n\n\nSep 1, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nTitanic Survival Exercises\n\n\n\n\n\n\n\nr\n\n\nggplot2\n\n\ndata visualization\n\n\n\n\nAssessment of visualization skills acquired in the HarvardX’s Data Science: Visualization course.\n\n\n\n\n\n\nJun 9, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Analysis – Advanced Plots With {ggplot2}\n\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\nsql\n\n\npostgresql\n\n\nr\n\n\n\n\nQuerying database in R code chunk and making some useful plots with ggplot2.\n\n\n\n\n\n\nMay 20, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\n  \n\n\n\n\nAnswering Business Questions Using SQL\n\n\n\n\n\n\n\nsql\n\n\nsqlite\n\n\ndata visualization\n\n\nr\n\n\n\n\nMy SQL project for the “Intermediate SQL for Data Analysis” course at Dataquest.\n\n\n\n\n\n\nFeb 15, 2022\n\n\nSandra Jurela\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html",
    "href": "posts/answering-business-questions-using-sql/chinook.html",
    "title": "Answering Business Questions Using SQL",
    "section": "",
    "text": "The aim of this project is to explore a modified version of the Chinook database using SQL and answer some business questions. The Chinook database represents a fictional digital media shop, based on real data from an iTunes Library and manually generated data. The database is provided as a SQLite database file called chinook.db.\nHere’s a schema diagram for the Chinook database:"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "href": "posts/answering-business-questions-using-sql/chinook.html#connecting-to-the-database-and-data-overview",
    "title": "Answering Business Questions Using SQL",
    "section": "Connecting to the Database and Data Overview",
    "text": "Connecting to the Database and Data Overview\n\nlibrary(DBI)\n\ndb <- dbConnect(RSQLite::SQLite(), dbname = \"data/chinook.db\")\n\nListing all tables in the Chinook database.\n\nSELECT\n  name,\n  type\nFROM sqlite_master\nWHERE type IN (\"table\", \"view\")\n\n\n11 records\n\n\nname\ntype\n\n\n\n\nalbum\ntable\n\n\nartist\ntable\n\n\ncustomer\ntable\n\n\nemployee\ntable\n\n\ngenre\ntable\n\n\ninvoice\ntable\n\n\ninvoice_line\ntable\n\n\nmedia_type\ntable\n\n\nplaylist\ntable\n\n\nplaylist_track\ntable\n\n\ntrack\ntable\n\n\n\n\n\nThe database consists of 11 tables containing information about artists, albums, media tracks, playlists, invoices, customers, and shop employees. Let’s start by getting familiar with our data from the main tables:\nemployee table\n\nSELECT *\nFROM employee\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ncustomer table\n\nSELECT *\nFROM customer\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice table\n\nSELECT *\nFROM invoice\nLIMIT 5\n\n\n\n\n\n  \n\n\n\ninvoice_line table\n\nSELECT *\nFROM invoice_line\nLIMIT 30\n\n\n\n\n\n  \n\n\n\ntrack table\n\nSELECT *\nFROM track\nLIMIT 20"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "href": "posts/answering-business-questions-using-sql/chinook.html#selecting-albums-to-purchase",
    "title": "Answering Business Questions Using SQL",
    "section": "1. Selecting Albums to Purchase",
    "text": "1. Selecting Albums to Purchase\nThe Chinook record store has just signed a deal with a new record label, and you’ve been tasked with selecting the first three albums that will be added to the store, from a list of four. All four albums are by artists that don’t have any tracks in the store right now - we have the artist names, and the genre of music they produce:\n\n\n\nArtist Name\nGenre\n\n\n\n\nRegal\nHip-Hop\n\n\nRed Tone\nPunk\n\n\nMeteor and the Girls\nPop\n\n\nSlim Jim Bites\nBlues\n\n\n\nThe record label specializes in artists from the USA, and they have given Chinook some money to advertise the new albums in the USA, so we’re interested in finding out which genres sell the best in the USA.\nYou’ll need to write a query to find out which genres sell the most tracks in the USA, write up a summary of your findings, and make a recommendation for the three artists whose albums we should purchase for the store.\nInstructions\n\nWrite a query that returns each genre, with the number of tracks sold in the USA:\n\nin absolute numbers\nin percentages.\n\nWrite a paragraph that interprets the data and makes a recommendation for the three artists whose albums we should purchase for the store, based on sales of tracks from their genres.\n\n\nSELECT \n  g.name AS genre,\n  SUM(il.quantity) AS tracks_sold,\n  ROUND(CAST(SUM(il.quantity) AS FLOAT)/\n  (\n    SELECT SUM(il.quantity) \n    FROM invoice i\n    INNER JOIN invoice_line il\n    ON i.invoice_id = il.invoice_id\n    WHERE i.billing_country = 'USA'\n  ) \n  , 4) AS percentage_sold\nFROM invoice i\nINNER JOIN invoice_line il\nON i.invoice_id = il.invoice_id\nINNER JOIN track t \nON il.track_id = t.track_id\nINNER JOIN genre g\nON t.genre_id = g.genre_id\nWHERE i.billing_country = 'USA'\nGROUP BY genre\nORDER BY tracks_sold DESC\n\n\n\n\n\n\ngenre\ntracks_sold\npercentage_sold\n\n\n\n\nRock\n561\n0.5338\n\n\nAlternative & Punk\n130\n0.1237\n\n\nMetal\n124\n0.1180\n\n\nR&B/Soul\n53\n0.0504\n\n\nBlues\n36\n0.0343\n\n\nAlternative\n35\n0.0333\n\n\nPop\n22\n0.0209\n\n\nLatin\n22\n0.0209\n\n\nHip Hop/Rap\n20\n0.0190\n\n\nJazz\n14\n0.0133\n\n\nEasy Listening\n13\n0.0124\n\n\nReggae\n6\n0.0057\n\n\nElectronica/Dance\n5\n0.0048\n\n\nClassical\n4\n0.0038\n\n\nHeavy Metal\n3\n0.0029\n\n\nSoundtrack\n2\n0.0019\n\n\nTV Shows\n1\n0.0010\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_classic())\n\ngenre_of_interest <- c(\"Alternative & Punk\", \"Hip Hop/Rap\", \"Pop\", \"Blues\")\n\ngenre_perc %>% \n  mutate(of_interest = ifelse(genre %in% genre_of_interest, \"yes\", \"no\"),\n         perc_text = scales::percent(percentage_sold, accuracy = 0.1)) %>% \n  ggplot(aes(x=tracks_sold, y=reorder(genre, tracks_sold, sum), fill=of_interest)) + \n  geom_bar(stat = 'identity', width = 0.7) +\n  geom_text(aes(label = perc_text), hjust = -0.2) +\n  labs(title = \"Sold Tracks by Genre, USA\", x = \"Tracks Sold\", y = \"Genre\", \n       fill = \"Genre of Interest\") + \n  scale_fill_manual(values = c(\"gray74\", \"orange\")) +\n  scale_x_continuous(limits = c(0, 600)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe most popular genres in the USA are Rock, Alternative & Punk, and Metal, followed with a big gap by all the others. Since our choice is limited by Hip-Hop, Punk, Pop, and Blues genres, and since we have to choose 3 out of 4 albums, we should purchase the new albums by the following artists:\n\nRed Tone (Punk)\nSlim Jim Bites (Blues)\nMeteor and the Girls (Pop)"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-employee-sales-performance",
    "title": "Answering Business Questions Using SQL",
    "section": "2. Analyzing Employee Sales Performance",
    "text": "2. Analyzing Employee Sales Performance\nEach customer for the Chinook store gets assigned to a sales support agent within the company when they first make a purchase. You have been asked to analyze the purchases of customers belonging to each employee to see if any sales support agent is performing either better or worse than the others.\nYou might like to consider whether any extra columns from the employee table explain any variance you see, or whether the variance might instead be indicative of employee performance.\nInstructions\n\nWrite a query that finds the total dollar amount of sales assigned to each sales support agent within the company. Add any extra attributes for that employee that you find are relevant to the analysis.\nWrite a short statement describing your results, and providing a possible interpretation.\n\n\nSELECT \n  e.first_name || ' ' || e.last_name AS sales_support_agent,\n  e.hire_date,\n  COUNT(DISTINCT c.customer_id) AS customers,\n  SUM(i.total) AS total_sales\nFROM employee e\nINNER JOIN customer c\nON e.employee_id = c.support_rep_id\nINNER JOIN invoice i\nON c.customer_id = i.customer_id\nGROUP BY sales_support_agent\n\n\n3 records\n\n\nsales_support_agent\nhire_date\ncustomers\ntotal_sales\n\n\n\n\nJane Peacock\n2017-04-01 00:00:00\n21\n1731.51\n\n\nMargaret Park\n2017-05-03 00:00:00\n20\n1584.00\n\n\nSteve Johnson\n2017-10-17 00:00:00\n18\n1393.92\n\n\n\n\n\nWhile there is a 20% difference in sales between Jane (the top employee) and Steve (the bottom employee), the difference roughly corresponds with the differences in their hiring dates."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "href": "posts/answering-business-questions-using-sql/chinook.html#analyzing-sales-by-country",
    "title": "Answering Business Questions Using SQL",
    "section": "3. Analyzing Sales by Country",
    "text": "3. Analyzing Sales by Country\nYour next task is to analyze the sales data for customers from each different country. You have been given guidance to use the country value from the customers table, and ignore the country from the billing address in the invoice table.\nInstructions\n\nWrite a query that collates data on purchases from different countries.\nWhere a country has only one customer, collect them into an “Other” group.\nThe results should be sorted by the total sales from highest to lowest, with the “Other” group at the very bottom.\nFor each country, include:\n\ntotal number of customers\ntotal value of sales\naverage value of sales per customer\naverage order value\n\n\n\nWITH t1 AS (\n  SELECT\n    CASE\n      WHEN COUNT(DISTINCT c.customer_id) = 1 THEN 'Other'\n      ELSE c.country\n      END AS country,\n    COUNT(DISTINCT c.customer_id) AS customers,\n    SUM(i.total) AS total_sales,\n    SUM(i.total)/COUNT(DISTINCT c.customer_id) AS avg_sales_per_cust,\n    AVG(i.total) AS avg_order\n  FROM customer c\n  INNER JOIN invoice i\n  ON c.customer_id = i.customer_id\n  GROUP BY country\n)\n\nSELECT \n  country,\n  SUM(customers) AS customers,\n  SUM(total_sales) AS total_sales,\n  AVG(avg_sales_per_cust) AS avg_sales_per_cust,\n  AVG(avg_order) AS avg_order\nFROM \n  (\n    SELECT\n      t1.*,\n      CASE \n        WHEN country = 'Other' THEN 1\n        ELSE 0\n        END AS sorted\n    FROM t1\n  )\nGROUP BY country\nORDER BY sorted, total_sales DESC\n\n\n\n\n\n\ncountry\ncustomers\ntotal_sales\navg_sales_per_cust\navg_order\n\n\n\n\nUSA\n13\n1040.49\n80.03769\n7.942672\n\n\nCanada\n8\n535.59\n66.94875\n7.047237\n\n\nBrazil\n5\n427.68\n85.53600\n7.011147\n\n\nFrance\n5\n389.07\n77.81400\n7.781400\n\n\nGermany\n4\n334.62\n83.65500\n8.161463\n\n\nCzech Republic\n2\n273.24\n136.62000\n9.108000\n\n\nUnited Kingdom\n3\n245.52\n81.84000\n8.768571\n\n\nPortugal\n2\n185.13\n92.56500\n6.383793\n\n\nIndia\n2\n183.15\n91.57500\n8.721429\n\n\nOther\n15\n1094.94\n72.99600\n7.445071\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, customers, total_sales) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(customers = customers/sum(customers),\n         total_sales = total_sales/sum(total_sales)) %>%\n  pivot_longer(-country, names_to = \"variable\", values_to = \"value\") %>% \n  ggplot(aes(x=value, y=country, fill=variable)) +\n  geom_bar(stat = \"identity\", width = 0.65, position = position_dodge(0.8)) +\n  labs(title=\"Share of Customers and Sales by Country\", x=\"share\", fill=\"\") +\n  scale_x_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"gray77\", \"seagreen3\")) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nCode\nsales_by_country %>% \n  select(country, avg_order, avg_sales_per_cust) %>% \n  mutate(country = factor(country, levels = c(country)) %>% fct_rev()) %>% \n  mutate(avg_order = (avg_order - mean(avg_order)) / mean(avg_order),\n         avg_sales_per_cust = (avg_sales_per_cust - mean(avg_sales_per_cust)) / \n                                 mean(avg_sales_per_cust)) %>%\n  rename(`Average Order` = avg_order,\n         `Average Sales per Customer` = avg_sales_per_cust) %>% \n  pivot_longer(-country, names_to = \"variable\", values_to = \"pct_diff_from_mean\") %>% \n  ggplot(aes(x=pct_diff_from_mean, y=country, fill=variable)) +\n  geom_col(width = 0.65, position = position_dodge(0.8)) +\n  facet_wrap(~ variable, scales = \"free_x\") + \n  labs(title=\"Average Order & Average Sales per Customer\", \n       subtitle = \"(Percent Difference from Mean)\", x=\"pct diff from mean\", fill=\"\") +\n  scale_fill_manual(values = c(\"steelblue\", \"lightskyblue2\")) +\n  scale_x_continuous(labels = scales::percent) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe USA has the largest customer base and, consequently, the highest total sales.\nBased on the data, there may be opportunity in the following countries:\n\nCzech Republic\nUnited Kingdom\nIndia\n\nIt’s worth keeping in mind that the amount of data from each of these countries is relatively low. Because of this, we should be cautious spending too much money on new marketing campaigns, as the sample size is not large enough to give us high confidence. A better approach would be to run small campaigns in these countries, collecting and analyzing the new customers to make sure that these trends hold with new customers."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "href": "posts/answering-business-questions-using-sql/chinook.html#albums-vs-individual-tracks",
    "title": "Answering Business Questions Using SQL",
    "section": "4. Albums vs Individual Tracks",
    "text": "4. Albums vs Individual Tracks\nThe Chinook store is setup in a way that allows customer to make purchases in one of the two ways:\n\npurchase a whole album\npurchase a collection of one or more individual tracks.\n\nThe store does not let customers purchase a whole album, and then add individual tracks to that same purchase (unless they do that by choosing each track manually). When customers purchase albums they are charged the same price as if they had purchased each of those tracks separately.\nManagement are currently considering changing their purchasing strategy to save money. The strategy they are considering is to purchase only the most popular tracks from each album from record companies, instead of purchasing every track from an album.\nWe have been asked to find out what percentage of purchases are individual tracks vs whole albums, so that management can use this data to understand the effect this decision might have on overall revenue.\nInstructions\n\nWrite a query that categorizes each invoice as either an album purchase or not, and calculates the following summary statistics:\n\nNumber of invoices\nPercentage of invoices\n\nWrite one to two sentences explaining your findings, and making a prospective recommendation on whether the Chinook store should continue to buy full albums from record companies\n\n\nWITH cat_purchase AS (\n  SELECT\n    il.invoice_id,\n    CASE\n      WHEN\n      COUNT(DISTINCT t.album_id) = 1\n      AND \n      COUNT(DISTINCT t.track_id) = c.count_album_tracks\n      THEN 'album'\n      ELSE 'individual track(s)'\n      END AS purchase_type,\n      c.count_album_tracks\n    FROM track t\n    JOIN invoice_line il\n    ON il.track_id = t.track_id\n    JOIN (SELECT COUNT(*) AS count_album_tracks, album_id\n          FROM track\n          GROUP BY album_id) c\n    ON c.album_id = t.album_id\n    GROUP BY invoice_id\n)\n\nSELECT\n  purchase_type,\n  COUNT(*) AS number_of_invoices,\n  ROUND(CAST(COUNT(*) AS float) / CAST(\n    (SELECT COUNT(*)\n    FROM invoice) AS FLOAT), 2) AS percentage_of_invoices\nFROM cat_purchase\nGROUP BY purchase_type\n\n\n2 records\n\n\npurchase_type\nnumber_of_invoices\npercentage_of_invoices\n\n\n\n\nalbum\n114\n0.19\n\n\nindividual track(s)\n500\n0.81\n\n\n\n\n\nAlbum purchases account for 19% of all purchases. Based on this data, I would recommend against purchasing only the most popular tracks from each album from record companies, since there is a potential of losing a significant portion of revenue."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "href": "posts/answering-business-questions-using-sql/chinook.html#which-artist-is-used-in-the-most-playlists",
    "title": "Answering Business Questions Using SQL",
    "section": "5. Which artist is used in the most playlists?",
    "text": "5. Which artist is used in the most playlists?\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY number_of_playlists DESC, unique_tracks DESC\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nEugene Ormandy\nClassical\n7\n3\n\n\nBerliner Philharmoniker & Herbert Von Karajan\nClassical\n6\n3\n\n\nThe King’s Singers\nClassical\n6\n2\n\n\nEnglish Concert & Trevor Pinnock\nClassical\n6\n2\n\n\nAcademy of St. Martin in the Fields & Sir Neville Marriner\nClassical\n6\n2\n\n\nMichael Tilson Thomas & San Francisco Symphony\nClassical\n5\n2\n\n\nYo-Yo Ma\nClassical\n5\n1\n\n\nWilhelm Kempff\nClassical\n5\n1\n\n\nTon Koopman\nClassical\n5\n1\n\n\nSir Georg Solti, Sumi Jo & Wiener Philharmoniker\nOpera\n5\n1\n\n\n\n\n\nEugene Ormandy takes the first place with only 3 unique tracks in 7 different playlists. His music belongs to the classical genre, which we have previously seen is one of the least popular genres in the USA.\nIf we order this table by number of unique tracks, we get a completely different list.\n\nSELECT \n  ar.name AS artist,\n  g.name AS genre,\n  COUNT(DISTINCT pt.playlist_id) AS number_of_playlists,\n  COUNT(DISTINCT t.track_id) AS unique_tracks\nFROM artist ar\nJOIN album al ON ar.artist_id=al.artist_id\nJOIN track t ON al.album_id=t.album_id\nJOIN playlist_track pt ON pt.track_id = t.track_id\nJOIN genre g ON g.genre_id = t.genre_id\nGROUP BY artist, genre\nORDER BY unique_tracks DESC, number_of_playlists DESC\n\n\nDisplaying records 1 - 10\n\n\nartist\ngenre\nnumber_of_playlists\nunique_tracks\n\n\n\n\nLed Zeppelin\nRock\n3\n114\n\n\nMetallica\nMetal\n4\n112\n\n\nU2\nRock\n3\n112\n\n\nIron Maiden\nMetal\n4\n95\n\n\nDeep Purple\nRock\n3\n92\n\n\nIron Maiden\nRock\n3\n81\n\n\nPearl Jam\nRock\n4\n54\n\n\nVan Halen\nRock\n3\n52\n\n\nOs Paralamas Do Sucesso\nLatin\n3\n49\n\n\nLost\nTV Shows\n2\n48"
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "href": "posts/answering-business-questions-using-sql/chinook.html#how-many-tracks-have-been-purchased-vs-not-purchased",
    "title": "Answering Business Questions Using SQL",
    "section": "6. How many tracks have been purchased vs not purchased?",
    "text": "6. How many tracks have been purchased vs not purchased?\n\nWITH all_and_purchased_tracks AS (\n  SELECT \n    t.track_id AS all_tracks,\n    il.track_id AS purch_tracks\n  FROM track t\n  LEFT JOIN invoice_line il\n  ON t.track_id = il.track_id\n)\n  \nSELECT\n  COUNT(DISTINCT all_tracks) AS total_tracks,\n  COUNT(DISTINCT purch_tracks) AS pirchased,\n  COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) AS not_purchased,\n  ROUND(CAST(COUNT(DISTINCT purch_tracks) AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_purchased,\n  ROUND(CAST(COUNT(DISTINCT all_tracks) - COUNT(DISTINCT purch_tracks) \n    AS FLOAT)/COUNT(DISTINCT all_tracks), 3)\n    AS perc_not_purchased\nFROM all_and_purchased_tracks\n\n\n1 records\n\n\n\n\n\n\n\n\n\ntotal_tracks\npirchased\nnot_purchased\nperc_purchased\nperc_not_purchased\n\n\n\n\n3503\n1806\n1697\n0.516\n0.484\n\n\n\n\n\n\n\nCode\npie(c(51.6, 48.4), labels = c(\"purchased\", \"not purchased\"), \n    main = \"Purchased vs not purchased tracks\")\n\n\n\n\n\nAlmost half of all the unique tracks available in the Chinook store were never bought, probably being of unpopular genre or unpopular artists."
  },
  {
    "objectID": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "href": "posts/answering-business-questions-using-sql/chinook.html#do-protected-vs-non-protected-media-types-have-an-effect-on-popularity",
    "title": "Answering Business Questions Using SQL",
    "section": "7. Do protected vs non-protected media types have an effect on popularity?",
    "text": "7. Do protected vs non-protected media types have an effect on popularity?\nLet’s take a look at the media_type table.\n\nSELECT *\nFROM media_type\n\n\n5 records\n\n\nmedia_type_id\nname\n\n\n\n\n1\nMPEG audio file\n\n\n2\nProtected AAC audio file\n\n\n3\nProtected MPEG-4 video file\n\n\n4\nPurchased AAC audio file\n\n\n5\nAAC audio file\n\n\n\n\n\nThere are 2 out of 5 media types that are protected.\n\nWITH t AS (\n  SELECT \n    t.track_id,\n    CASE\n      WHEN mt.name LIKE \"%protected%\" THEN \"yes\" ELSE \"no\"\n      END AS protected\n  FROM track t\n  JOIN media_type mt ON mt.media_type_id = t.media_type_id\n)\n\nSELECT \n  protected,\n  COUNT(DISTINCT t.track_id) AS unique_tracks,\n  ROUND(CAST(COUNT(DISTINCT t.track_id) AS FLOAT) / (SELECT COUNT(*) FROM track), 2)\n    AS 'unique_tracks_%',\n  COUNT(DISTINCT il.track_id) AS sold_unique_tracks,\n  COUNT(il.track_id) AS sold_tracks,\n  ROUND(CAST(COUNT(il.track_id) AS FLOAT) / (SELECT COUNT(*) FROM invoice_line), 2)\n    AS 'sold_tracks_%'\nFROM t\nLEFT JOIN invoice_line il ON t.track_id = il.track_id\nGROUP BY protected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprotected\nunique_tracks\nunique_tracks_%\nsold_unique_tracks\nsold_tracks\nsold_tracks_%\n\n\n\n\nno\n3052\n0.87\n1652\n4315\n0.91\n\n\nyes\n451\n0.13\n154\n442\n0.09\n\n\n\n\n\n\n\nCode\nby_media_type %>% \n  select(protected, unique_tracks, sold_tracks) %>% \n  rename(unique = unique_tracks, \n         sold = sold_tracks) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  mutate(tracks = as.factor(tracks) %>% fct_rev()) %>% \n  group_by(tracks) %>% \n  mutate(pct = count/sum(count) %>% round(2)) %>% \n  ungroup() %>% \n  ggplot(aes(x=tracks, y=count, fill=protected)) +\n  geom_col(width = 0.5, position = \"stack\", color = \"white\") +\n  geom_text(aes(label = count), position = position_stack(vjust = .5),\n            color = \"white\", fontface = \"bold\")+\n  scale_fill_manual(values = c(\"#fbc02d\", \"#03a9f4\")) +\n  theme(legend.position = \"top\")\nby_media_type %>% \n  select(protected, unique_tracks, sold_unique_tracks) %>% \n  rename(unique = unique_tracks, \n         unique_sold = sold_unique_tracks) %>% \n  mutate(unique_unsold = unique - unique_sold) %>% \n  pivot_longer(-protected, names_to = \"tracks\", values_to = \"count\") %>% \n  filter(tracks != \"unique\") %>% \n  group_by(protected) %>% \n  mutate(percentage = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n  ggplot(aes(x=protected, y=count, fill=tracks)) +\n  geom_col(width = 0.5, position = \"fill\", color = \"white\") +\n  scale_fill_manual(values = c(\"seagreen3\", \"tomato\")) +\n  geom_text(aes(label = percentage), position = position_fill(vjust = .5), \n            color = \"white\", fontface = \"bold\") +\n  labs(y=\"proportion\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can make the following observations:\n\nOnly 13% of all the unique tracks available in the Chinook store are of protected media types.\nAmong all the tracks that were sold, those of protected media types amounts only to 9%.\nFrom all the unique tracks of protected media types, only 34,1% were sold, while from those of non-protected ones 54,1%.\n\nWe can conclude that the tracks of protected media types are much less popular than those of non-protected."
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html",
    "href": "posts/credit-card-retention/analysis.html",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "",
    "text": "In this hypothetical case study I’ve been hired as a financial analyst by the marketing department within a bank.\nThey are faced with more and more customers leaving their credit card services. They would like to understand what characteristics lend themselves to someone who is going to churn so they can proactively go to the customer to provide them better services and turn customers’ decisions in the opposite direction.\nAs a part of my first analysis, they’ve asked me to take a look at the available data and help them understand how to increase customer retention."
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#problem-statement",
    "href": "posts/credit-card-retention/analysis.html#problem-statement",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nWhat marketing campaigns could we implement to help reduce customer churn?"
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#imports",
    "href": "posts/credit-card-retention/analysis.html#imports",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport altair as alt\n\n# Show all columns (instead of cascading columns in the middle)\npd.set_option(\"display.max_columns\", None)\n# Don't show numbers in scientific notation\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\n\n# Enable plotting more than 5000 points\nalt.data_transformers.disable_max_rows()\n\nDataTransformerRegistry.enable('default')\n\n\nI will use Altair for data visualizations. It’s a declarative statistical visualization library for Python, built on top of the powerful Vega-Lite grammar, It’s very intuitive to use, and after some experience with Matplotlib and Seaborn, I’m pleasantly surprised with it. 😇"
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#data-overview",
    "href": "posts/credit-card-retention/analysis.html#data-overview",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Data Overview",
    "text": "Data Overview\nThe dataset consists of about 10,000 customers described with their age, income level, marital status, credit card limit, credit card category, etc. It’s available on Kaggle at the following link.\n\nData Dictionary\n\nCLIENTNUM - Unique identifier for the customer holding the account\nAttrition_Flag - If the account is closed then Attrited Customer, else Existing Customer\nCustomer_Age - Customer’s age in years\nGender - Male = M, Female = F\nDependent_count - Number of dependents\nEducation_Level - Educational qualification of the account holder (example: high school, college graduate, etc.)\nMarital_Status - Married, Single, Divorced, Unknown\nIncome_Category - Annual income category of the account holder (Less than $40K, $40K-$60K, $60K-$80K, $80K-$120K, $120K +\nCard_Category - Type of card (Blue, Silver, Gold, Platinum)\nMonths_on_book - Period of relationship with bank\nTotal_Relationship_count - Total number of products held by the customer\nMonths_Inactive_12_mon - Number of months inactive in the last 12 months\nContacts_Count_12_mon - Number of contacts in the last 12 months\nCredit_Limit - Credit limit on the credit card\nTotal_Revolving_Bal - Total revolving balance on the credit card\nAvg_Open_To_Buy- Open to buy credit line (average of last 12 months)\nTotal_Amt_Chng_Q4_Q1 - Change in transaction amount (Q4 over Q1)\nTotal_Trans_Amt - Total transaction amount (last 12 months)\nTotal_Trans_Ct - Total transaction count (last 12 months)\nTotal_Ct_Chng_Q4_Q1 - Change in transaction count (Q4 over Q1)\nAvg_Utilization_Ratio - Average card utilization ratio\n\n\n# Read in the data and remove the last two columns irrelevant for the analysis\ndata = pd.read_csv(\"data/BankChurners.csv\").iloc[: , :-2]\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Attrition_Flag\n      Customer_Age\n      Gender\n      Dependent_count\n      Education_Level\n      Marital_Status\n      Income_Category\n      Card_Category\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n    \n  \n  \n    \n      0\n      768805383\n      Existing Customer\n      45\n      M\n      3\n      High School\n      Married\n      $60K - $80K\n      Blue\n      39\n      5\n      1\n      3\n      12691.00\n      777\n      11914.00\n      1.33\n      1144\n      42\n      1.62\n      0.06\n    \n    \n      1\n      818770008\n      Existing Customer\n      49\n      F\n      5\n      Graduate\n      Single\n      Less than $40K\n      Blue\n      44\n      6\n      1\n      2\n      8256.00\n      864\n      7392.00\n      1.54\n      1291\n      33\n      3.71\n      0.10\n    \n    \n      2\n      713982108\n      Existing Customer\n      51\n      M\n      3\n      Graduate\n      Married\n      $80K - $120K\n      Blue\n      36\n      4\n      1\n      0\n      3418.00\n      0\n      3418.00\n      2.59\n      1887\n      20\n      2.33\n      0.00\n    \n    \n      3\n      769911858\n      Existing Customer\n      40\n      F\n      4\n      High School\n      Unknown\n      Less than $40K\n      Blue\n      34\n      3\n      4\n      1\n      3313.00\n      2517\n      796.00\n      1.41\n      1171\n      20\n      2.33\n      0.76\n    \n    \n      4\n      709106358\n      Existing Customer\n      40\n      M\n      3\n      Uneducated\n      Married\n      $60K - $80K\n      Blue\n      21\n      5\n      1\n      0\n      4716.00\n      0\n      4716.00\n      2.17\n      816\n      28\n      2.50\n      0.00\n    \n  \n\n\n\n\n\ndata.shape\n\n(10127, 21)\n\n\nWe have a total of 10127 customers described with 21 attributes.\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nData columns (total 21 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   CLIENTNUM                 10127 non-null  int64  \n 1   Attrition_Flag            10127 non-null  object \n 2   Customer_Age              10127 non-null  int64  \n 3   Gender                    10127 non-null  object \n 4   Dependent_count           10127 non-null  int64  \n 5   Education_Level           10127 non-null  object \n 6   Marital_Status            10127 non-null  object \n 7   Income_Category           10127 non-null  object \n 8   Card_Category             10127 non-null  object \n 9   Months_on_book            10127 non-null  int64  \n 10  Total_Relationship_Count  10127 non-null  int64  \n 11  Months_Inactive_12_mon    10127 non-null  int64  \n 12  Contacts_Count_12_mon     10127 non-null  int64  \n 13  Credit_Limit              10127 non-null  float64\n 14  Total_Revolving_Bal       10127 non-null  int64  \n 15  Avg_Open_To_Buy           10127 non-null  float64\n 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float64\n 17  Total_Trans_Amt           10127 non-null  int64  \n 18  Total_Trans_Ct            10127 non-null  int64  \n 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float64\n 20  Avg_Utilization_Ratio     10127 non-null  float64\ndtypes: float64(5), int64(10), object(6)\nmemory usage: 1.6+ MB\n\n\nThis dataset is actually very clean. We’ll only do some preprocessing steps before the analysis."
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#data-preprocessing",
    "href": "posts/credit-card-retention/analysis.html#data-preprocessing",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nBinning Customer Age\nWe’ll create specific age categories and place ages within a range in these categories. For instance, category 20s will contain ages between 20 and 29, and so on.\n\n# looking for min and max age\nprint(data['Customer_Age'].min())\nprint(data['Customer_Age'].max())\n\n26\n73\n\n\n\nbins = [20, 30, 40, 50, 60, 70, 80]\nlabels = ['20s', '30s', '40s', '50s', '60s', '70s']\n\ndata['Customer_Age_bins'] = pd.cut(\n    data['Customer_Age'], \n    bins=bins, \n    labels=labels, \n    include_lowest=True, \n    right=False\n)\n\n\n\nAverage Transaction Value\nThe average transaction value is the average amount a customer spends on a single purchase.\nWe can calculate it by dividing the Total_Trans_Amt by the Total_Trans_Ct.\n\ndata['Avg_Transaction_Value'] = data['Total_Trans_Amt'] / data['Total_Trans_Ct']\n\n\n\nConverting data types for memory optimization\n\ndata.info(verbose=False, show_counts=False, memory_usage='deep') \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nColumns: 23 entries, CLIENTNUM to Avg_Transaction_Value\ndtypes: category(1), float64(6), int64(10), object(6)\nmemory usage: 5.0 MB\n\n\nCurrently, the total deep memory usage of the DataFrame is 5 MB. We’ll reduce it with smaller data types.\n\ndata = data.astype(\n    {   \n        'Attrition_Flag': 'category',    # categories\n        'Gender': 'category',           \n        'Education_Level': 'category', \n        'Marital_Status': 'category', \n        'Income_Category': 'category', \n        'Card_Category': 'category',\n        'Customer_Age': 'int8',          # integers\n        'Dependent_count': 'int8',\n        'Months_on_book': 'int8',\n        'Total_Relationship_Count':'int8',\n        'Months_Inactive_12_mon':'int8',\n        'Contacts_Count_12_mon':'int8',\n        'Total_Revolving_Bal':'int16',\n        'Total_Trans_Amt':'int16',\n        'Total_Trans_Ct':'uint8',             \n        'Credit_Limit': 'float32',       # floats\n        'Avg_Open_To_Buy': 'float32', \n        'Total_Amt_Chng_Q4_Q1': 'float16', \n        'Total_Ct_Chng_Q4_Q1': 'float16', \n        'Avg_Utilization_Ratio': 'float16',\n        'Avg_Transaction_Value': 'float32'\n    }\n)\n\n\ndata.info(memory_usage='deep') \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10127 entries, 0 to 10126\nData columns (total 23 columns):\n #   Column                    Non-Null Count  Dtype   \n---  ------                    --------------  -----   \n 0   CLIENTNUM                 10127 non-null  int64   \n 1   Attrition_Flag            10127 non-null  category\n 2   Customer_Age              10127 non-null  int8    \n 3   Gender                    10127 non-null  category\n 4   Dependent_count           10127 non-null  int8    \n 5   Education_Level           10127 non-null  category\n 6   Marital_Status            10127 non-null  category\n 7   Income_Category           10127 non-null  category\n 8   Card_Category             10127 non-null  category\n 9   Months_on_book            10127 non-null  int8    \n 10  Total_Relationship_Count  10127 non-null  int8    \n 11  Months_Inactive_12_mon    10127 non-null  int8    \n 12  Contacts_Count_12_mon     10127 non-null  int8    \n 13  Credit_Limit              10127 non-null  float32 \n 14  Total_Revolving_Bal       10127 non-null  int16   \n 15  Avg_Open_To_Buy           10127 non-null  float32 \n 16  Total_Amt_Chng_Q4_Q1      10127 non-null  float16 \n 17  Total_Trans_Amt           10127 non-null  int16   \n 18  Total_Trans_Ct            10127 non-null  uint8   \n 19  Total_Ct_Chng_Q4_Q1       10127 non-null  float16 \n 20  Avg_Utilization_Ratio     10127 non-null  float16 \n 21  Customer_Age_bins         10127 non-null  category\n 22  Avg_Transaction_Value     10127 non-null  float32 \ndtypes: category(7), float16(3), float32(3), int16(2), int64(1), int8(6), uint8(1)\nmemory usage: 438.4 KB\n\n\nWe can see at the bottom that the memory usage is now only 438.4 KB (10 times lower)."
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#exploratory-data-analysis-eda",
    "href": "posts/credit-card-retention/analysis.html#exploratory-data-analysis-eda",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n🔍 Percentage of churned customers\n\ndata['Attrition_Flag'].value_counts()   \n\nExisting Customer    8500\nAttrited Customer    1627\nName: Attrition_Flag, dtype: int64\n\n\nWe’ll use normalize=True to get the share of churned and existing customers.\n\nchurned = data['Attrition_Flag'].value_counts(normalize=True)['Attrited Customer']\n\nprint(round(churned * 100 , 2), '% of customers have churned.')\n\n16.07 % of customers have churned.\n\n\n\n\n📝 Summary Statistics\n\ndata.describe()\n\n\n\n\n\n  \n    \n      \n      CLIENTNUM\n      Customer_Age\n      Dependent_count\n      Months_on_book\n      Total_Relationship_Count\n      Months_Inactive_12_mon\n      Contacts_Count_12_mon\n      Credit_Limit\n      Total_Revolving_Bal\n      Avg_Open_To_Buy\n      Total_Amt_Chng_Q4_Q1\n      Total_Trans_Amt\n      Total_Trans_Ct\n      Total_Ct_Chng_Q4_Q1\n      Avg_Utilization_Ratio\n      Avg_Transaction_Value\n    \n  \n  \n    \n      count\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n      10127.00\n    \n    \n      mean\n      739177606.33\n      46.33\n      2.35\n      35.93\n      3.81\n      2.34\n      2.46\n      8632.00\n      1162.81\n      7469.17\n      0.76\n      4404.09\n      64.86\n      0.71\n      0.27\n      62.61\n    \n    \n      std\n      36903783.45\n      8.02\n      1.30\n      7.99\n      1.55\n      1.01\n      1.11\n      9088.76\n      814.99\n      9090.68\n      0.22\n      3397.13\n      23.47\n      0.24\n      0.28\n      26.40\n    \n    \n      min\n      708082083.00\n      26.00\n      0.00\n      13.00\n      1.00\n      0.00\n      0.00\n      1438.30\n      0.00\n      3.00\n      0.00\n      510.00\n      10.00\n      0.00\n      0.00\n      19.14\n    \n    \n      25%\n      713036770.50\n      41.00\n      1.00\n      31.00\n      3.00\n      2.00\n      2.00\n      2555.00\n      359.00\n      1324.50\n      0.63\n      2155.50\n      45.00\n      0.58\n      0.02\n      47.51\n    \n    \n      50%\n      717926358.00\n      46.00\n      2.00\n      36.00\n      4.00\n      2.00\n      2.00\n      4549.00\n      1276.00\n      3474.00\n      0.74\n      3899.00\n      67.00\n      0.70\n      0.18\n      55.79\n    \n    \n      75%\n      773143533.00\n      52.00\n      3.00\n      40.00\n      5.00\n      3.00\n      3.00\n      11067.50\n      1784.00\n      9859.00\n      0.86\n      4741.00\n      81.00\n      0.82\n      0.50\n      65.48\n    \n    \n      max\n      828343083.00\n      73.00\n      5.00\n      56.00\n      6.00\n      6.00\n      6.00\n      34516.00\n      2517.00\n      34516.00\n      3.40\n      18484.00\n      139.00\n      3.71\n      1.00\n      190.19\n    \n  \n\n\n\n\n\nHere we can see:\n\nThe longest customer in this dataset has been around for 56 months, or about 4 years and a half. (Max)\nThe average number of relationships a customer has is 4. (Mean and median agree here.)\nThe average credit limit is $8.6K, but the median credit limit is much lower at $4.5K. (signals some skew in this variable)\n\n\n\n📊 Distributions\n\nHistograms\nLet’s take a look at the distribution for the Months_on_book variable.\n\nalt.Chart(data).mark_bar().encode(   \n    alt.X('Months_on_book', bin=alt.Bin(maxbins=30), title=\"Months on book\"),\n    alt.Y('count()')\n).properties(width=500)\n\n\n\n\n\n\nIt looks pretty normal outside of a very strong peak at ~36 months (roughly 2800 customers). It might be a data error (default months for those without a record), or there was a major marketing campaign that brought in a lot of users 36 months ago.\nNow let’s take a look at the Credit_Limit.\n\nhist = alt.Chart(data).mark_bar().encode(   \n    alt.X('Credit_Limit', bin=alt.Bin(step=1500), title=\"Credit Limit\"),\n    alt.Y('count()')\n).properties(width=500)\n\naggregates = alt.Chart(data).transform_aggregate(\n    mean='mean(Credit_Limit)',\n    median='median(Credit_Limit)',\n).transform_fold(\n    ['mean', 'median']\n).mark_rule().encode(\n    x='value:Q',\n    color='key:N',\n    size=alt.value(2)\n)\n\nhist + aggregates\n\n\n\n\n\n\nIt’s clear that the Credit_Limit is a skewed variable, being skewed higher (or to the right) by a few customers with high credit limits.\n\n\nBoxplot\nLet’s look at the total number of transactions by gender.\n\ncolor_scale = alt.Scale(domain=['M', 'F'],\n                        range=['#1f77b4', '#e377c2'])\n                        \nalt.Chart(data).mark_boxplot(size=25).encode(\n    alt.X('Gender', axis=alt.Axis(labelAngle=-0)),\n    alt.Y('Total_Trans_Ct'),\n    alt.Color('Gender', scale=color_scale)\n).properties(width=150)\n\n\n\n\n\n\nThis plot quickly shows us that the median transaction count is slightly higher for females than males and outliers seem to be present among both groups.\n\n\nPyramid chart\nWas there a difference in the distribution of customers by age and gender? A pyramid chart can help us find the answer.\nTo get the data in the format we need, we’ll aggregate the data up to the age and gender level.\n\npyramid_df = (data\n    .groupby(['Gender', 'Customer_Age_bins'])\n    ['CLIENTNUM']\n    .nunique()\n    .reset_index()\n)\n\nleft = alt.Chart(pyramid_df[pyramid_df['Gender']==\"F\"]).mark_bar().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.X('CLIENTNUM', title='Customers', sort='descending'),\n    alt.Color('Gender', scale=color_scale, legend=None)\n).properties(title='Female', width=350)\n\nmiddle = alt.Chart(pyramid_df).mark_text().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.Text('Customer_Age_bins'),\n).properties(width=20)\n\nright = alt.Chart(pyramid_df[pyramid_df['Gender']==\"M\"]).mark_bar().encode(\n    alt.Y('Customer_Age_bins', axis=None),\n    alt.X('CLIENTNUM', title='Customers'),\n    alt.Color('Gender', scale=color_scale, legend=None)\n).properties(title='Male', width=300)\n\nleft | middle | right\n\n\n\n\n\n\nHere we see that there is no real difference in the distribution of customers by age and gender. The majority of customers are in their 40s. We have only two male customers in their 70s, which is hardly visible here.\n\n\n\n📊 Categorical variables\nNext, we’ll see how attrited and existing customers are represented in all category groups.\n\n# get the list of categorical variables, remove `Attrition_Flag`\ncat_list = sorted((list(set(data.columns) - set(data._get_numeric_data().columns))))[1:]\ncat_list\n\n['Card_Category',\n 'Customer_Age_bins',\n 'Education_Level',\n 'Gender',\n 'Income_Category',\n 'Marital_Status']\n\n\n\ndef cat_column_bar(cat_column):\n\n    return alt.Chart(data).mark_bar().encode(\n        alt.X('count()', stack=\"normalize\", axis=alt.Axis(format='%'), title='Percent'),\n        alt.Y(cat_column),\n        color=alt.Color('Attrition_Flag', sort=\"descending\")\n    ).properties(width=250)\n    \n(cat_column_bar(cat_list[0]) | \n cat_column_bar(cat_list[1]) | \n cat_column_bar(cat_list[2]) | \n cat_column_bar(cat_list[3]) | \n cat_column_bar(cat_list[4]) | \n cat_column_bar(cat_list[5]) \n)\n\n\n\n\n\n\n\nNothing strange here. Churned customers are almost equally distributed across all category groups.\nIndividual values for customers with a platinum credit card and the age group 70s deviate from the average due to the small number of customers in these groups.\n\n📊 Comparing Categories\nWe will look at the average Credit_Limit by age group.\n\nbar = alt.Chart(data).mark_bar().encode(\n    alt.X('Customer_Age_bins', axis=alt.Axis(labelAngle=-0), title='Customer age group'),\n    alt.Y('mean(Credit_Limit)'),\n    alt.Color('Customer_Age_bins', legend=None)\n).properties(width=300)\n\nerror_bars = alt.Chart(data).mark_errorbar(extent='ci').encode(  \n    x='Customer_Age_bins',\n    y='Credit_Limit',\n    strokeWidth=alt.value(2)\n)\n\nbar + error_bars\n\n\n\n\n\n\nAnd now we’ll look at the Credit_Limit by age group and gender.\n\ngrouped_bar = alt.Chart(data).mark_bar().encode(\n    alt.X('Gender', axis=alt.Axis(title=None, labels=False, ticks=False)),\n    alt.Y('mean(Credit_Limit)', title=\"Mean Credit Limit\", axis=alt.Axis(grid=False)),\n    alt.Color('Gender', scale=color_scale)\n).properties(width=50)\n\nerror_bars = alt.Chart(data).mark_errorbar(extent='ci').encode(  \n    x='Gender',\n    y=alt.Y('Credit_Limit', title='Credit Limit'),\n    strokeWidth=alt.value(2)\n)\n\n(grouped_bar + error_bars).facet(\n    column=alt.Column('Customer_Age_bins', title='Customer age group')\n).configure_headerColumn(\n    titleOrient='bottom', \n    labelOrient='bottom'\n).configure_view(\n    stroke='transparent'\n)\n\n\n\n\n\n\nFemales on average are getting credit limits much lower than males! They probably have lower wages. Let’s check this out.\n\nincome_cat_order = ['Less than $40K', '$40K - $60K', '$60K - $80K', \n                    '$80K - $120K', '$120K +', 'Unknown']\n                    \nalt.Chart(data).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('Income_Category', sort=income_cat_order, title='Income Category'),\n    alt.Color('Gender', scale=color_scale)\n).properties(height=130)\n\n\n\n\n\n\nThis is interesting. There is not a single woman in this dataset who earns $60K-$80K, $80K-$120K, or $120K + a year.\nDoes this follow their education level?\n\nedu_cat_order = ['Uneducated', 'High School', 'College', 'Graduate', \n                 'Post-Graduate', 'Doctorate', 'Unknown']\n\nalt.Chart(data).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('Education_Level', sort=edu_cat_order, title='Education Level'),\n    alt.Color('Gender', scale=color_scale)\n).properties(height=150)\n\n\n\n\n\n\nNo. Men and women are equally educated. But compared to women in general, men are clearly more represented in better-paid jobs and in better-paid positions.\n\n\n\n📝 Pivot table\nComing back to the business problem, we want to understand something about customers who are leaving. Let’s see if we can see anything obvious about their behavior.\n\n(data\n    .groupby(['Attrition_Flag'])\n    .agg({'CLIENTNUM':'nunique',  # number of unique customers in each group\n          'Customer_Age': 'mean',  # the rest are looking at the mean per group\n          'Dependent_count': 'mean',\n          'Months_on_book': 'mean',\n          'Total_Relationship_Count': 'mean',\n          'Months_Inactive_12_mon': 'mean',\n          'Contacts_Count_12_mon': 'mean',\n          'Credit_Limit': 'mean',\n          'Total_Revolving_Bal': 'mean',\n          'Avg_Open_To_Buy': 'mean',\n          'Total_Amt_Chng_Q4_Q1': 'mean',\n          'Total_Trans_Amt': 'mean',\n          'Total_Trans_Ct': 'mean',\n          'Total_Ct_Chng_Q4_Q1': 'mean',\n          'Avg_Utilization_Ratio': 'mean',\n          'Avg_Transaction_Value': 'mean',})\n    .transpose()\n    .assign(Diff = (lambda x: (x['Attrited Customer'] / x['Existing Customer'] - 1)))\n    .sort_values('Diff')\n    .rename_axis(None, axis=1)\n)\n\n\n\n\n\n  \n    \n      \n      Attrited Customer\n      Existing Customer\n      Diff\n    \n  \n  \n    \n      CLIENTNUM\n      1627.00\n      8500.00\n      -0.81\n    \n    \n      Total_Revolving_Bal\n      672.82\n      1256.60\n      -0.46\n    \n    \n      Avg_Utilization_Ratio\n      0.16\n      0.30\n      -0.45\n    \n    \n      Total_Trans_Ct\n      44.93\n      68.67\n      -0.35\n    \n    \n      Total_Trans_Amt\n      3095.03\n      4654.66\n      -0.34\n    \n    \n      Total_Ct_Chng_Q4_Q1\n      0.55\n      0.74\n      -0.25\n    \n    \n      Total_Relationship_Count\n      3.28\n      3.91\n      -0.16\n    \n    \n      Total_Amt_Chng_Q4_Q1\n      0.69\n      0.77\n      -0.10\n    \n    \n      Credit_Limit\n      8136.04\n      8726.88\n      -0.07\n    \n    \n      Avg_Open_To_Buy\n      7463.22\n      7470.27\n      -0.00\n    \n    \n      Months_on_book\n      36.18\n      35.88\n      0.01\n    \n    \n      Customer_Age\n      46.66\n      46.26\n      0.01\n    \n    \n      Avg_Transaction_Value\n      63.59\n      62.43\n      0.02\n    \n    \n      Dependent_count\n      2.40\n      2.34\n      0.03\n    \n    \n      Months_Inactive_12_mon\n      2.69\n      2.27\n      0.18\n    \n    \n      Contacts_Count_12_mon\n      2.97\n      2.36\n      0.26\n    \n  \n\n\n\n\nWe can see that the Total_Revolving_Bal and Avg_Utilization_Ratio show the biggest differences (45% below existing customers), along with Total_Trans_Ct and Total_Trans_Amt (35% below existing customers), and Total_Ct_Chng_Q4_Q1 (25% below existing customers).\nAverage Card Utilization Ratio\nIt represents the amount of revolving credit customer is using divided by the total credit available to them.\nLet’s look at the Avg_Utilization_Ratio distribution.\n\nalt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('Avg_Utilization_Ratio', bin=alt.Bin(step=0.025), \n          axis=alt.Axis(format='%'), title=\"Average Utilization Ratio\"),\n    alt.Y('count()', stack=False),\n    alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=700)\n\n\n\n\n\n\nWe can see that 57% (934) of churned customers have the average utilization ratio less than 2.5%.\nHow many churned customers have a credit card utilization ratio of zero?\n\ndata.query('(Avg_Utilization_Ratio == 0) and (Attrition_Flag==\"Attrited Customer\")').shape[0]\n\n893\n\n\nThere are 893 (55%) churned customers whose credit card utilization ratio is zero, which means they pay off their balances in full before the statement due date.\n\n\n📊 Transaction Amounts and Counts relationship\nScatter plot with Transaction Amounts and Counts looks interesting.\n\nalt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n) #.properties(width=500, height=300)\n\n\n\n\n\n\nThis plot clearly shows three distinct groups. Will anything show up if we add Attrition_Flag as a color? We’ll also add histograms for transaction amounts and counts.\n\nscatter = alt.Chart(data).mark_circle(stroke=\"white\", strokeWidth=0.4).encode(\n    alt.X('Total_Trans_Amt', title='Total Transaction Amount'),\n    alt.Y('Total_Trans_Ct', title='Total Transaction Count'),\n    alt.Color('Attrition_Flag', sort='descending', title='Attrition Flag')\n).properties(width=625, height=400)\n\nhist_amt = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('Total_Trans_Amt', bin=alt.Bin(step=500), title='', \n          scale=alt.Scale(domain=[0, 20000])),\n    alt.Y('count()', stack=False, title=''),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=625, height=150)\n\nhist_ct = alt.Chart(data).mark_bar(opacity=0.7).encode(\n    alt.X('count()', stack=False, title=''), \n    alt.Y('Total_Trans_Ct', bin=alt.Bin(maxbins=30), title='',\n          scale=alt.Scale(domain=[0, 140])),\n    alt.Color('Attrition_Flag', sort='descending')\n).properties(width=100, height=400)\n\n# putting them all together!\n\nplot_title = alt.TitleParams(\n    \"Transaction Amounts and Counts for Customers\",\n    subtitle = \"No Churned Customers above $11K of Spend\"\n)\n\n(hist_amt & (scatter | hist_ct)).properties(title=plot_title\n).configure_title(fontSize=16, dy=-10\n).configure_legend(\n    offset=-120,\n    strokeColor='gray',\n    padding=10,\n    cornerRadius=10,\n)\n\n\n\n\n\n\nWe can see that there are no customers who have churned that have spent $11K or more,\nWe can also see churned customers with transaction amounts between $4K and $11K who have separated from existing customers.\nWe’ll create bins and separate these groups from each other so we can effectively look at their differences.\n\ndata['Total_Trans_Amt_bin'] = pd.cut(\n    data['Total_Trans_Amt'],\n    bins=[0, 4000, 11000, 20000],\n    labels=['Bellow $4K','$4K-$11K', 'Above $11K'],\n    include_lowest=True,\n    right=False)\n    \ndata.groupby(['Total_Trans_Amt_bin', 'Attrition_Flag']).agg({\n     'CLIENTNUM':'nunique',\n    'Customer_Age': 'mean',\n    'Dependent_count': 'mean',\n    'Months_on_book': 'mean',\n    'Total_Relationship_Count': 'mean',\n    'Months_Inactive_12_mon': 'mean',\n    'Contacts_Count_12_mon': 'mean',\n    'Credit_Limit': 'mean',\n    'Total_Revolving_Bal': 'mean',\n    'Avg_Open_To_Buy': 'mean',\n    'Total_Amt_Chng_Q4_Q1': 'mean',\n    'Total_Trans_Amt': 'mean',\n    'Total_Trans_Ct': 'mean',\n    'Total_Ct_Chng_Q4_Q1': 'mean',\n    'Avg_Utilization_Ratio': 'mean',\n    'Avg_Transaction_Value': 'mean'\n}).T\n\n\n\n\n\n  \n    \n      Total_Trans_Amt_bin\n      Bellow $4K\n      $4K-$11K\n      Above $11K\n    \n    \n      Attrition_Flag\n      Attrited Customer\n      Existing Customer\n      Attrited Customer\n      Existing Customer\n      Attrited Customer\n      Existing Customer\n    \n  \n  \n    \n      CLIENTNUM\n      1318.00\n      3969.00\n      309.00\n      3784.00\n      0.00\n      747.00\n    \n    \n      Customer_Age\n      46.92\n      45.96\n      45.54\n      46.81\n      NaN\n      45.10\n    \n    \n      Dependent_count\n      2.41\n      2.22\n      2.37\n      2.47\n      NaN\n      2.27\n    \n    \n      Months_on_book\n      36.44\n      35.68\n      35.06\n      36.27\n      NaN\n      35.01\n    \n    \n      Total_Relationship_Count\n      3.34\n      4.43\n      3.01\n      3.69\n      NaN\n      2.36\n    \n    \n      Months_Inactive_12_mon\n      2.70\n      2.25\n      2.64\n      2.31\n      NaN\n      2.22\n    \n    \n      Contacts_Count_12_mon\n      3.00\n      2.53\n      2.86\n      2.20\n      NaN\n      2.22\n    \n    \n      Credit_Limit\n      6709.90\n      9528.69\n      14219.05\n      6834.27\n      NaN\n      14053.80\n    \n    \n      Total_Revolving_Bal\n      659.87\n      1299.79\n      728.06\n      1189.39\n      NaN\n      1367.62\n    \n    \n      Avg_Open_To_Buy\n      6050.03\n      8228.90\n      13490.99\n      5644.88\n      NaN\n      12686.18\n    \n    \n      Total_Amt_Chng_Q4_Q1\n      0.66\n      0.80\n      0.86\n      0.75\n      NaN\n      0.78\n    \n    \n      Total_Trans_Amt\n      2071.31\n      2443.86\n      7461.54\n      4990.81\n      NaN\n      14698.40\n    \n    \n      Total_Trans_Ct\n      39.81\n      51.49\n      66.79\n      78.47\n      NaN\n      110.34\n    \n    \n      Total_Ct_Chng_Q4_Q1\n      0.51\n      0.74\n      0.74\n      0.74\n      NaN\n      0.75\n    \n    \n      Avg_Utilization_Ratio\n      0.18\n      0.28\n      0.09\n      0.33\n      NaN\n      0.18\n    \n    \n      Avg_Transaction_Value\n      52.20\n      47.66\n      112.19\n      63.73\n      NaN\n      134.28\n    \n  \n\n\n\n\nIn terms of average credit limit, churned customers with transaction amounts between $4K and $11K are more like existing customers with transaction amounts above $11K (very similar average credit limit, around $14K). Simply put, they spent less. And as a result, the translation of those points (churned customers) to the left and downward may be seen on the previous graph.\n\n\n📊 Can we influence the Q4 to Q1 dip?\nThe Q4-Q1 change (Total_Ct_Chng_Q4_Q1) is significantly different for churned customers too! This indicates that churned customers are spending significantly less (-25% lower than their counterparts) after the holiday season, pulling back much more sigificantly. If we can impact this variable, we can also directly impact the Total Transaction Count!\n\nalt.Chart(data).transform_density(\n     'Total_Ct_Chng_Q4_Q1',\n     groupby=['Attrition_Flag'],\n     as_=['Total_Ct_Chng_Q4_Q1', 'density']\n).mark_area(opacity=0.7, clip=True).encode(                \n     alt.X('Total_Ct_Chng_Q4_Q1', scale=alt.Scale(domain=[0, 2]),\n           title='Total Transaction Count Change (Q4 to Q1)'),   \n     alt.Y('density:Q', scale=alt.Scale(domain=[0, 3])),  \n     alt.Color('Attrition_Flag', sort='descending')\n).properties(title='Change in Transaction Count (Q4 over Q1)', width=500, height=300\n).configure_title(fontSize=16, dy=-10, anchor='start')"
  },
  {
    "objectID": "posts/credit-card-retention/analysis.html#recommendations",
    "href": "posts/credit-card-retention/analysis.html#recommendations",
    "title": "Bank Customer Retention Analysis in Python",
    "section": "Recommendations",
    "text": "Recommendations\n\nPromotions during Q1 time to keep the spending levels high. The more we spend the more attached we feel to the card, it’s benefits, etc. A “prevent the cliff” campaign where all customers who have historically shown a strong drop off, will get targeted with this promo.\nCustomer surveys. For those who are spending above $11,000 to understand why they love our card and what keeps them around. If we were also able to get responses from some of our churn customers, we can develop stronger marketing campaigns around our findings.\nOffer loyalty points, cash back, etc.\nLook at any historical marketing campaigns to see what we can learn from what worked / didn’t work.\n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html",
    "title": "Cryptocurrency SQL Case Study",
    "section": "",
    "text": "On November 18, 2022 I attended the SQL masterclass “SQL and PostgreSQL for Data Analytics”, presented live by Danny Ma on O’Reilly platform.\nThis Github repository contains all the necessary data, sql scripts, and other materials.\nI’m posting some parts of it here for my own reference, but I strongly recommend that you register for that event that takes place every once in a while. It’s free and worth it. Danny Ma is an excellent teacher and his explanations are outstanding.\n\n\nFor the purposes of this project, I created the local trading Postgres database on my machine and ran the sql script to create and populate the tables.\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database (typically via the DBI::dbConnect() function).\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"trading\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> trading@localhost:5432"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#introduction",
    "title": "Cryptocurrency SQL Case Study",
    "section": "Introduction",
    "text": "Introduction\nIn this fictitious case study - Danny’s data mentors from the Data With Danny team have been busy trading cryptocurrency markets since 2017.\nThe main purpose for this case study is to analyze the performance of the DWD mentors over time. We will accomplish this by writing SQL queries to utilize all available datasets to answer a series of realistic business questions.\n\nAvailable Datasets\nAll of our data for this case study exists within the trading schema in the PostgreSQL database.\nThere are 3 data tables available in this schema:\n\nmembers\nprices\ntransactions\n\n\n\nEntity Relationship Diagram\nI drew the ERD here.\n\n\n\nData Dictionary and Overview\nThe trading.members table consists of information about the mentors from the Data With Danny team.\n\ntrading.members table\n\n\nColumn name\nDescription\n\n\n\n\nmember_id\nunique id for each mentor\n\n\nfirst_name\nfirst name for each mentor\n\n\nregion\nregion where each mentor is from\n\n\n\n\nSELECT * FROM trading.members\n\n\n14 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\nc9f0f8\nAbe\nUnited States\n\n\n45c48c\nBen\nAustralia\n\n\nd3d944\nEnoch\nAfrica\n\n\n6512bd\nVikram\nIndia\n\n\nc20ad4\nLeah\nAsia\n\n\nc51ce4\nPavan\nAustralia\n\n\naab323\nSonia\nAustralia\n\n\n\n\n\nThe trading.prices table consists of daily price and volume information from January 2017 through to August 2021 for the 2 most popular cryptocurrency tickers: Bitcoin and Ethereum.\n\ntrading.prices table\n\n\nColumn name\nDescription\n\n\n\n\nticker\none of either BTC or ETH\n\n\nmarket_date\nthe date for each record\n\n\nprice\nclosing price at end of day\n\n\nopen\nthe opening price\n\n\nhigh\nthe highest price for that day\n\n\nlow\nthe lowest price for that day\n\n\nvolume\nthe total volume traded\n\n\nchange\n% change in daily price\n\n\n\nThe first 5 rows from this dataset.\n\nSELECT * FROM trading.prices LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nopen\nhigh\nlow\nvolume\nchange\n\n\n\n\nETH\n2021-08-29\n3177.84\n3243.96\n3282.21\n3162.79\n582.04K\n-2.04%\n\n\nETH\n2021-08-28\n3243.90\n3273.78\n3284.58\n3212.24\n466.21K\n-0.91%\n\n\nETH\n2021-08-27\n3273.58\n3093.78\n3279.93\n3063.37\n839.54K\n5.82%\n\n\nETH\n2021-08-26\n3093.54\n3228.03\n3249.62\n3057.48\n118.44K\n-4.17%\n\n\nETH\n2021-08-25\n3228.15\n3172.12\n3247.43\n3080.70\n923.13K\n1.73%\n\n\n\n\n\nThe trading.transactions table consists of buy and sell transactions data for each trade made by the DWD mentors.\n\ntrading.transactions table\n\n\nColumn name\nDescription\n\n\n\n\ntxn_id\nunique ID for each transaction\n\n\nmember_id\nmember identifier for each trade\n\n\nticker\nthe ticker for each trade\n\n\ntxn_date\nthe date for each transaction\n\n\ntxn_type\neither BUY or SELL\n\n\nquantity\nthe total quantity for each trade\n\n\npercentage_fee\n% of total amount charged as fees\n\n\ntxn_time\nthe timestamp for each trade\n\n\n\nThe first 5 frows from this transactions table.\n\nSELECT * FROM trading.transactions LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\ntxn_id\nmember_id\nticker\ntxn_date\ntxn_type\nquantity\npercentage_fee\ntxn_time\n\n\n\n\n1\nc81e72\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n2\neccbc8\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n3\na87ff6\nBTC\n2017-01-01\nBUY\n50\n0.0\n2017-01-01\n\n\n4\ne4da3b\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01\n\n\n5\n167909\nBTC\n2017-01-01\nBUY\n50\n0.3\n2017-01-01"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-1-basic-data-analysis-techniques",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 1️⃣: Basic Data Analysis Techniques",
    "text": "PART 1️⃣: Basic Data Analysis Techniques\n\nQuestion 1.1\nShow only the top 5 rows from the trading.members table.\n\nSELECT * \nFROM trading.members\nLIMIT 5\n\n\n5 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc4ca42\nDanny\nAustralia\n\n\nc81e72\nVipul\nUnited States\n\n\neccbc8\nCharlie\nUnited States\n\n\na87ff6\nNandita\nUnited States\n\n\ne4da3b\nRowan\nUnited States\n\n\n\n\n\n\n\nQuestion 1.2\nSort all the rows in the trading.members table by first_name in alphabetical order and show the top 3 rows with all columns.\n\nSELECT * \nFROM trading.members\nORDER BY first_name\nLIMIT 3\n\n\n3 records\n\n\nmember_id\nfirst_name\nregion\n\n\n\n\nc9f0f8\nAbe\nUnited States\n\n\n8f14e4\nAlex\nUnited States\n\n\n167909\nAyush\nUnited States\n\n\n\n\n\n\n\nQuestion 1.3\nCount the number of records from the trading.members table which have United States as the region value.\n\nSELECT \n  COUNT(*) AS record_count\nFROM trading.members\nWHERE region = 'United States'\n\n\n1 records\n\n\nrecord_count\n\n\n\n\n7\n\n\n\n\n\n\n\nQuestion 1.4\nSelect only the first_name and region columns for mentors who are not from Australia.\n\nSELECT \n  first_name, \n  region\nFROM trading.members\nWHERE region != 'Australia'\n\n\nDisplaying records 1 - 10\n\n\nfirst_name\nregion\n\n\n\n\nVipul\nUnited States\n\n\nCharlie\nUnited States\n\n\nNandita\nUnited States\n\n\nRowan\nUnited States\n\n\nAyush\nUnited States\n\n\nAlex\nUnited States\n\n\nAbe\nUnited States\n\n\nEnoch\nAfrica\n\n\nVikram\nIndia\n\n\nLeah\nAsia\n\n\n\n\n\n\n\nQuestion 1.5\nReturn only the unique region values from the trading.members table and sort the output by reverse alphabetical order.\n\nSELECT DISTINCT region\nFROM trading.members\nORDER BY region DESC\n\n\n5 records\n\n\nregion\n\n\n\n\nUnited States\n\n\nIndia\n\n\nAustralia\n\n\nAsia\n\n\nAfrica"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-2-aggregate-functions-for-data-analysis",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 2️⃣: Aggregate Functions for Data Analysis",
    "text": "PART 2️⃣: Aggregate Functions for Data Analysis\n\nQuestion 2.1\nHow many records are there per ticker value in the trading.prices table?\n\nSELECT\n  ticker,\n  COUNT(*) AS record_count\nFROM trading.prices\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nrecord_count\n\n\n\n\nBTC\n1702\n\n\nETH\n1702\n\n\n\n\n\n\n\nQuestion 2.2\nWhat is the maximum, minimum values for the price column for both Bitcoin and Ethereum in 2020?\n\nSELECT \n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nmin_price\nmax_price\n\n\n\n\nBTC\n4826.0\n28949.4\n\n\nETH\n107.9\n751.8\n\n\n\n\n\n\n\nQuestion 2.3\nWhat is the annual minimum, maximum and average price for each ticker?\n\nInclude a calendar_year column with the year from 2017 through to 2021\nCalculate a spread column which calculates the difference between the min and max prices\nRound the average price output to 2 decimal places\nSort the output in chronological order with Bitcoin records before Ethereum within each year\n\n\nSELECT\n  EXTRACT(YEAR FROM market_date) AS calendar_year,\n  ticker,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price,\n  ROUND(AVG(price)::NUMERIC, 2) AS avg_price,\n  MAX(price) - MIN(price) AS spread\nFROM trading.prices\nGROUP BY calendar_year, ticker\nORDER BY calendar_year, ticker\n\n\nDisplaying records 1 - 10\n\n\ncalendar_year\nticker\nmin_price\nmax_price\navg_price\nspread\n\n\n\n\n2017\nBTC\n785.40\n19345.50\n3981.07\n18560.10\n\n\n2017\nETH\n8.20\n799.98\n220.34\n791.78\n\n\n2018\nBTC\n3228.70\n17172.30\n7552.16\n13943.60\n\n\n2018\nETH\n83.81\n1380.00\n481.33\n1296.19\n\n\n2019\nBTC\n3397.70\n13063.80\n7371.82\n9666.10\n\n\n2019\nETH\n104.55\n338.54\n180.99\n233.99\n\n\n2020\nBTC\n4826.00\n28949.40\n11111.63\n24123.40\n\n\n2020\nETH\n107.90\n751.80\n307.30\n643.90\n\n\n2021\nBTC\n29359.90\n63540.90\n44353.55\n34181.00\n\n\n2021\nETH\n729.12\n4167.78\n2199.12\n3438.66\n\n\n\n\n\n\n\nQuestion 2.4\nWhat is the monthly average of the price column for each ticker from January 2020 and after?\n\nCreate a month_start column with the first day of each month\nSort the output by ticker in alphabetical order and months in chronological order\nRound the average_price column to 2 decimal places\n\n\nSELECT\n  ticker,\n  DATE_TRUNC('MON', market_date)::DATE AS month_start,\n  ROUND(AVG(price)::NUMERIC, 2) AS average_price\nFROM trading.prices\nWHERE market_date >= '2020-01-01'\nGROUP BY ticker, month_start\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\naverage_price\n\n\n\n\nBTC\n2020-01-01\n8378.80\n\n\nBTC\n2020-02-01\n9636.56\n\n\nBTC\n2020-03-01\n6863.11\n\n\nBTC\n2020-04-01\n7211.03\n\n\nBTC\n2020-05-01\n9253.55\n\n\nBTC\n2020-06-01\n9481.85\n\n\nBTC\n2020-07-01\n9592.10\n\n\nBTC\n2020-08-01\n11638.41\n\n\nBTC\n2020-09-01\n10643.33\n\n\nBTC\n2020-10-01\n11888.36"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-3-case-when-statements",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 3️⃣: Case When Statements",
    "text": "PART 3️⃣: Case When Statements\n\nQuestion 3.1\nConvert the volume column in the trading.prices table with an adjusted integer value to take into the unit values\n\nReturn only the market_date, price, volume and adjusted_volume columns for the first 10 days of August 2021 for Ethereum only\n\n\nSELECT\n  market_date,\n  price,\n  volume,    \n  CASE\n    WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000 \n    WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume) - 1)::NUMERIC * 1000000 \n    WHEN volume = '-' THEN 0\n    END AS adjusted_volume\nFROM trading.prices\nWHERE ticker = 'ETH'\n  AND market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY market_date\n\n\nDisplaying records 1 - 10\n\n\nmarket_date\nprice\nvolume\nadjusted_volume\n\n\n\n\n2021-08-01\n2556.23\n1.20M\n1200000\n\n\n2021-08-02\n2608.04\n970.67K\n970670\n\n\n2021-08-03\n2506.65\n158.45K\n158450\n\n\n2021-08-04\n2725.29\n1.23M\n1230000\n\n\n2021-08-05\n2827.21\n1.65M\n1650000\n\n\n2021-08-06\n2889.43\n1.06M\n1060000\n\n\n2021-08-07\n3158.00\n64.84K\n64840\n\n\n2021-08-08\n3012.07\n1.25M\n1250000\n\n\n2021-08-09\n3162.93\n1.44M\n1440000\n\n\n2021-08-10\n3140.71\n1.12M\n1120000\n\n\n\n\n\n\n\nQuestion 3.2\nHow many “breakout” days were there in 2020 where the price column is greater than the open column for each ticker? In the same query also calculate the number of “non breakout” days where the price column was lower than or equal to the open column.\n\nSELECT\n  ticker,\n  SUM(CASE WHEN price > open THEN 1 ELSE 0 END) AS breakout_days,\n  SUM(CASE WHEN price <= open THEN 1 ELSE 0 END) AS non_breakout_days\nFROM trading.prices\nWHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nbreakout_days\nnon_breakout_days\n\n\n\n\nBTC\n207\n159\n\n\nETH\n200\n166\n\n\n\n\n\n\n\nQuestion 3.3\nWhat was the final quantity Bitcoin and Ethereum held by all Data With Danny mentors based off the trading.transactions table?\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'SELL' THEN -quantity ELSE quantity END) AS final_btc_holding\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nfinal_btc_holding\n\n\n\n\nBTC\n42848.67\n\n\nETH\n32801.04\n\n\n\n\n\nDivided by quantity bought and quantity sold.\n\nSELECT \n  ticker,\n  SUM(CASE WHEN txn_type = 'BUY' THEN quantity ELSE 0 END) AS qty_bought,\n  SUM(CASE WHEN txn_type = 'SELL' THEN quantity ELSE 0 END) AS qty_sold\nFROM trading.transactions\nGROUP BY ticker\n\n\n2 records\n\n\nticker\nqty_bought\nqty_sold\n\n\n\n\nBTC\n53250.15\n10401.485\n\n\nETH\n42599.20\n9798.154"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-4-window-functions",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 4️⃣: Window Functions",
    "text": "PART 4️⃣: Window Functions\n\nQuestion 4.1\nWhat are the market_date, price and volume and price_rank values for the days with the top 5 highest price values for each tickers in the trading.prices table?\n\nThe price_rank column is the ranking for price values for each ticker with rank = 1 for the highest value.\nReturn the output for Bitcoin, followed by Ethereum in price rank order.\n\n\nWITH cte_rank AS (\n  SELECT\n    ticker,\n    market_date,\n    price, \n    volume,\n    RANK() OVER (PARTITION BY ticker ORDER BY price DESC) AS price_rank\n  FROM trading.prices\n)\n\nSELECT *\nFROM cte_rank\nWHERE price_rank <= 5\nORDER BY ticker, price_rank\n\n\nDisplaying records 1 - 10\n\n\nticker\nmarket_date\nprice\nvolume\nprice_rank\n\n\n\n\nBTC\n2021-04-13\n63540.90\n126.56K\n1\n\n\nBTC\n2021-04-15\n63216.00\n76.97K\n2\n\n\nBTC\n2021-04-14\n62980.40\n130.43K\n3\n\n\nBTC\n2021-04-16\n61379.70\n136.85K\n4\n\n\nBTC\n2021-03-13\n61195.30\n134.64K\n5\n\n\nETH\n2021-05-11\n4167.78\n1.27M\n1\n\n\nETH\n2021-05-14\n4075.38\n2.06M\n2\n\n\nETH\n2021-05-10\n3947.90\n2.70M\n3\n\n\nETH\n2021-05-09\n3922.23\n1.94M\n4\n\n\nETH\n2021-05-08\n3905.55\n1.34M\n5\n\n\n\n\n\n\nQuestion 4.1 -  solution\n\nmembers <- read_csv(\"data/members_tbl.csv\")\nprices <- read_csv(\"data/prices_tbl.csv\")\ntransactions <- read_csv(\"data/transactions_tbl.csv\")\n\nprices %>% head()\n\n# A tibble: 6 × 8\n  ticker market_date price  open  high   low volume  change\n  <chr>  <date>      <dbl> <dbl> <dbl> <dbl> <chr>   <chr> \n1 ETH    2021-08-29  3178. 3244. 3282. 3163. 582.04K -2.04%\n2 ETH    2021-08-28  3244. 3274. 3285. 3212. 466.21K -0.91%\n3 ETH    2021-08-27  3274. 3094. 3280. 3063. 839.54K 5.82% \n4 ETH    2021-08-26  3094. 3228. 3250. 3057. 118.44K -4.17%\n5 ETH    2021-08-25  3228. 3172. 3247. 3081. 923.13K 1.73% \n6 ETH    2021-08-24  3173. 3323. 3358. 3151. 988.82K -4.41%\n\n\n\n# R solution\nprices %>% \n  select(ticker, market_date, price, volume) %>% \n  group_by(ticker) %>% \n  arrange(ticker, desc(price)) %>% \n  mutate(price_rank = row_number()) %>% \n  filter(price_rank <=5)\n\n# A tibble: 10 × 5\n# Groups:   ticker [2]\n   ticker market_date  price volume  price_rank\n   <chr>  <date>       <dbl> <chr>        <int>\n 1 BTC    2021-04-13  63541. 126.56K          1\n 2 BTC    2021-04-15  63216  76.97K           2\n 3 BTC    2021-04-14  62980. 130.43K          3\n 4 BTC    2021-04-16  61380. 136.85K          4\n 5 BTC    2021-03-13  61195. 134.64K          5\n 6 ETH    2021-05-11   4168. 1.27M            1\n 7 ETH    2021-05-14   4075. 2.06M            2\n 8 ETH    2021-05-10   3948. 2.70M            3\n 9 ETH    2021-05-09   3922. 1.94M            4\n10 ETH    2021-05-08   3906. 1.34M            5\n\n\n\n\n\nQuestion 4.2\nCalculate a 7 day rolling average for the price and volume columns in the trading.prices table for each ticker.\n\nReturn only the first 10 days of August 2021\n\n\n-- Step 1 - Adjusted prices CTE\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\n-- Step 2 - Moving Averages CTE\ncte_moving_averages AS (\n  SELECT\n    ticker,\n    market_date,\n    price,\n    AVG(price) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_price,\n    volume,\n    AVG(volume) OVER (\n      PARTITION BY ticker\n      ORDER BY market_date\n      RANGE BETWEEN '6 DAYS' PRECEDING AND CURRENT ROW  \n    ) AS moving_avg_volume\n  FROM cte_adjusted_prices\n)\n\n-- final output\nSELECT * FROM cte_moving_averages\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date;\n\n\n20 records\n\n\n\n\n\n\n\n\n\n\nticker\nmarket_date\nprice\nmoving_avg_price\nvolume\nmoving_avg_volume\n\n\n\n\nBTC\n2021-08-01\n39878.30\n40052.657\n80330\n103645.71\n\n\nBTC\n2021-08-02\n39168.40\n40322.914\n74810\n88957.14\n\n\nBTC\n2021-08-03\n38130.30\n40134.100\n260\n74674.29\n\n\nBTC\n2021-08-04\n39736.90\n40096.057\n79220\n64717.14\n\n\nBTC\n2021-08-05\n40867.20\n40219.743\n130600\n72617.14\n\n\nBTC\n2021-08-06\n42795.40\n40304.314\n111930\n74542.86\n\n\nBTC\n2021-08-07\n44614.20\n40741.529\n112840\n84284.29\n\n\nBTC\n2021-08-08\n43792.80\n41300.743\n105250\n87844.29\n\n\nBTC\n2021-08-09\n46284.30\n42317.300\n117080\n93882.86\n\n\nBTC\n2021-08-10\n45593.80\n43383.514\n80550\n105352.86\n\n\nETH\n2021-08-01\n2556.23\n2394.166\n1200000\n1069824.29\n\n\nETH\n2021-08-02\n2608.04\n2448.239\n970670\n938491.43\n\n\nETH\n2021-08-03\n2506.65\n2477.729\n158450\n782555.71\n\n\nETH\n2021-08-04\n2725.29\n2538.611\n1230000\n819850.00\n\n\nETH\n2021-08-05\n2827.21\n2602.366\n1650000\n963742.86\n\n\nETH\n2021-08-06\n2889.43\n2663.577\n1060000\n968028.57\n\n\nETH\n2021-08-07\n3158.00\n2752.979\n64840\n904851.43\n\n\nETH\n2021-08-08\n3012.07\n2818.099\n1250000\n911994.29\n\n\nETH\n2021-08-09\n3162.93\n2897.369\n1440000\n979041.43\n\n\nETH\n2021-08-10\n3140.71\n2987.949\n1120000\n1116405.71\n\n\n\n\n\n\nQuestion 4.2 -  solution\n\nprices %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  group_by(ticker) %>% \n  arrange(ticker, market_date) %>% \n  mutate(moving_avg_price = zoo::rollmean(price, k = 7, align = \"right\", fill = NA),\n         moving_avg_volume = zoo::rollmean(volume, k = 7, align = \"right\", fill = NA)) %>% \n  select(ticker, market_date, price, moving_avg_price, volume, moving_avg_volume) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 6\n# Groups:   ticker [2]\n   ticker market_date  price moving_avg_price  volume moving_avg_volume\n   <chr>  <date>       <dbl>            <dbl>   <dbl>             <dbl>\n 1 BTC    2021-08-01  39878.           40053.   80330           103646.\n 2 BTC    2021-08-02  39168.           40323.   74810            88957.\n 3 BTC    2021-08-03  38130.           40134.     260            74674.\n 4 BTC    2021-08-04  39737.           40096.   79220            64717.\n 5 BTC    2021-08-05  40867.           40220.  130600            72617.\n 6 BTC    2021-08-06  42795.           40304.  111930            74543.\n 7 BTC    2021-08-07  44614.           40742.  112840            84284.\n 8 BTC    2021-08-08  43793.           41301.  105250            87844.\n 9 BTC    2021-08-09  46284.           42317.  117080            93883.\n10 BTC    2021-08-10  45594.           43384.   80550           105353.\n11 ETH    2021-08-01   2556.            2394. 1200000          1069824.\n12 ETH    2021-08-02   2608.            2448.  970670           938491.\n13 ETH    2021-08-03   2507.            2478.  158450           782556.\n14 ETH    2021-08-04   2725.            2539. 1230000           819850 \n15 ETH    2021-08-05   2827.            2602. 1650000           963743.\n16 ETH    2021-08-06   2889.            2664. 1060000           968029.\n17 ETH    2021-08-07   3158             2753.   64840           904851.\n18 ETH    2021-08-08   3012.            2818. 1250000           911994.\n19 ETH    2021-08-09   3163.            2897. 1440000           979041.\n20 ETH    2021-08-10   3141.            2988. 1120000          1116406.\n\n\n\n\n\nQuestion 4.3\nCalculate the monthly cumulative volume traded for each ticker in 2020\n\nSort the output by ticker in chronological order with the month_start as the first day of each month\n\n\nWITH cte_monthly_volume AS (\n  SELECT\n    ticker,\n    DATE_TRUNC('MON', market_date)::DATE AS month_start,\n    SUM(\n      CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END\n  ) AS monthly_volume\n  FROM trading.prices\n  WHERE market_date BETWEEN '2020-01-01' AND '2020-12-31'\n  GROUP BY ticker, month_start\n)\n\nSELECT\n  ticker,\n  month_start,\n  SUM(monthly_volume) OVER (\n    PARTITION BY ticker\n    ORDER BY month_start\n    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n  ) AS cumulative_monthly_volume\nFROM cte_monthly_volume\nORDER BY ticker, month_start\n\n\nDisplaying records 1 - 10\n\n\nticker\nmonth_start\ncumulative_monthly_volume\n\n\n\n\nBTC\n2020-01-01\n23451920\n\n\nBTC\n2020-02-01\n46839130\n\n\nBTC\n2020-03-01\n94680450\n\n\nBTC\n2020-04-01\n134302740\n\n\nBTC\n2020-05-01\n172687010\n\n\nBTC\n2020-06-01\n188026610\n\n\nBTC\n2020-07-01\n201272600\n\n\nBTC\n2020-08-01\n216762630\n\n\nBTC\n2020-09-01\n300641440\n\n\nBTC\n2020-10-01\n303060020\n\n\n\n\n\n\nQuestion 4.3 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  filter(market_date >= \"2020-01-01\", market_date <= \"2020-12-31\") %>% \n  mutate(month_start = lubridate::floor_date(market_date, unit = \"month\")) %>% \n  group_by(ticker, month_start) %>% \n  summarise(monthly_volume = sum(\n    case_when(\n      str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n      str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n      str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n    )\n  ) %>% \n  ungroup() %>% \n  group_by(ticker) %>% \n  mutate(cumulative_monthly_volume = cumsum(monthly_volume)) %>% \n  ungroup()\n\n# A tibble: 24 × 4\n   ticker month_start monthly_volume cumulative_monthly_volume\n   <chr>  <date>               <dbl>                     <dbl>\n 1 BTC    2020-01-01        23451920                  23451920\n 2 BTC    2020-02-01        23387210                  46839130\n 3 BTC    2020-03-01        47841320                  94680450\n 4 BTC    2020-04-01        39622290                 134302740\n 5 BTC    2020-05-01        38384270                 172687010\n 6 BTC    2020-06-01        15339600                 188026610\n 7 BTC    2020-07-01        13245990                 201272600\n 8 BTC    2020-08-01        15490030                 216762630\n 9 BTC    2020-09-01        83878810                 300641440\n10 BTC    2020-10-01         2418580                 303060020\n# … with 14 more rows\n\n\n\n\n\nQuestion 4.4\nCalculate the daily percentage change in volume for each ticker in the trading.prices table\n\nPercentage change can be calculated as (current - previous) / previous\nMultiply the percentage by 100 and round the value to 2 decimal places\nReturn data for the first 10 days of August 2021\n\n\nWITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    CASE\n      WHEN RIGHT(volume, 1) = 'K' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000\n      WHEN RIGHT(volume, 1) = 'M' THEN LEFT(volume, LENGTH(volume)-1)::NUMERIC * 1000000\n      WHEN volume = '-' THEN 0\n    END AS volume\n  FROM trading.prices\n),\n\ncte_previous_volume AS (\n  SELECT\n    ticker,\n    market_date,\n    volume,\n    LAG(volume) OVER (PARTITION BY ticker ORDER BY market_date) AS previous_volume\n  FROM cte_adjusted_prices\n  WHERE volume != 0\n)\n  \nSELECT\n  ticker,\n  market_date,\n  volume,\n  previous_volume,\n  ROUND(100 * (volume - previous_volume) / previous_volume, 2) AS daily_change\nFROM cte_previous_volume\nWHERE market_date BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date\n\n\n20 records\n\n\nticker\nmarket_date\nvolume\nprevious_volume\ndaily_change\n\n\n\n\nBTC\n2021-08-01\n80330\n44650\n79.91\n\n\nBTC\n2021-08-02\n74810\n80330\n-6.87\n\n\nBTC\n2021-08-03\n260\n74810\n-99.65\n\n\nBTC\n2021-08-04\n79220\n260\n30369.23\n\n\nBTC\n2021-08-05\n130600\n79220\n64.86\n\n\nBTC\n2021-08-06\n111930\n130600\n-14.30\n\n\nBTC\n2021-08-07\n112840\n111930\n0.81\n\n\nBTC\n2021-08-08\n105250\n112840\n-6.73\n\n\nBTC\n2021-08-09\n117080\n105250\n11.24\n\n\nBTC\n2021-08-10\n80550\n117080\n-31.20\n\n\nETH\n2021-08-01\n1200000\n507080\n136.65\n\n\nETH\n2021-08-02\n970670\n1200000\n-19.11\n\n\nETH\n2021-08-03\n158450\n970670\n-83.68\n\n\nETH\n2021-08-04\n1230000\n158450\n676.27\n\n\nETH\n2021-08-05\n1650000\n1230000\n34.15\n\n\nETH\n2021-08-06\n1060000\n1650000\n-35.76\n\n\nETH\n2021-08-07\n64840\n1060000\n-93.88\n\n\nETH\n2021-08-08\n1250000\n64840\n1827.82\n\n\nETH\n2021-08-09\n1440000\n1250000\n15.20\n\n\nETH\n2021-08-10\n1120000\n1440000\n-22.22\n\n\n\n\n\n\nQuestion 4.4 -  solution\n\nprices %>% \n  select(ticker, market_date, volume) %>% \n  mutate(volume = case_when(\n    str_sub(volume, -1) == \"K\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^3,\n    str_sub(volume, -1) == \"M\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 10^6,\n    str_sub(volume, -1) == \"-\" ~ as.numeric(str_sub(volume, 1, str_length(volume) - 1)) * 0)\n  ) %>% \n  arrange(ticker, market_date) %>% \n  mutate(volume_prev_day = lag(volume)) %>% \n  mutate(daily_change_perc = round(100*(volume - volume_prev_day)/volume_prev_day, 2)) %>% \n  filter(market_date >= \"2021-08-01\", market_date <= \"2021-08-10\") \n\n# A tibble: 20 × 5\n   ticker market_date  volume volume_prev_day daily_change_perc\n   <chr>  <date>        <dbl>           <dbl>             <dbl>\n 1 BTC    2021-08-01    80330           44650             79.9 \n 2 BTC    2021-08-02    74810           80330             -6.87\n 3 BTC    2021-08-03      260           74810            -99.6 \n 4 BTC    2021-08-04    79220             260          30369.  \n 5 BTC    2021-08-05   130600           79220             64.9 \n 6 BTC    2021-08-06   111930          130600            -14.3 \n 7 BTC    2021-08-07   112840          111930              0.81\n 8 BTC    2021-08-08   105250          112840             -6.73\n 9 BTC    2021-08-09   117080          105250             11.2 \n10 BTC    2021-08-10    80550          117080            -31.2 \n11 ETH    2021-08-01  1200000          507080            137.  \n12 ETH    2021-08-02   970670         1200000            -19.1 \n13 ETH    2021-08-03   158450          970670            -83.7 \n14 ETH    2021-08-04  1230000          158450            676.  \n15 ETH    2021-08-05  1650000         1230000             34.2 \n16 ETH    2021-08-06  1060000         1650000            -35.8 \n17 ETH    2021-08-07    64840         1060000            -93.9 \n18 ETH    2021-08-08  1250000           64840           1828.  \n19 ETH    2021-08-09  1440000         1250000             15.2 \n20 ETH    2021-08-10  1120000         1440000            -22.2"
  },
  {
    "objectID": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "href": "posts/cryptocurrency-sql-case-study/cryptocurrency.html#part-5-table-joins",
    "title": "Cryptocurrency SQL Case Study",
    "section": "PART 5️⃣: Table Joins",
    "text": "PART 5️⃣: Table Joins\n\nQuestion 5.1 - Inner Joins\nWhich top 3 mentors have the most Bitcoin quantity? Return the first_name of the mentors and sort the output from highest to lowest total_quantity.\n\nSELECT\n  m.first_name,\n  SUM(\n    CASE\n      WHEN t.txn_type = 'BUY' THEN t.quantity \n      WHEN t.txn_type = 'SELL' THEN -t.quantity \n      END\n  ) AS total_quantity\nFROM trading.transactions t\nINNER JOIN trading.members m\n  ON t.member_id = m.member_id\nWHERE ticker = 'BTC' \nGROUP By m.first_name\nORDER BY total_quantity DESC\nLIMIT 3\n\n\n3 records\n\n\nfirst_name\ntotal_quantity\n\n\n\n\nNandita\n4160.220\n\n\nLeah\n4046.091\n\n\nAyush\n3945.198\n\n\n\n\n\n\n\nQuestion 5.2 - Left Joins\nShow the market_date values which have less than 5 transactions? Sort the output in reverse chronological order.\n\nSELECT \n  p.market_date,\n  COUNT(t.txn_id) AS transaction_count\nFROM trading.prices p\nLEFT JOIN trading.transactions t\n  ON p.market_date = t.txn_date\n  AND p.ticker = t.ticker\nGROUP BY p.market_date\nHAVING COUNT(t.txn_id) < 5\nORDER BY p.market_date DESC\n\n\n8 records\n\n\nmarket_date\ntransaction_count\n\n\n\n\n2021-08-29\n0\n\n\n2021-08-28\n0\n\n\n2021-07-17\n3\n\n\n2021-01-06\n4\n\n\n2020-01-17\n4\n\n\n2019-07-15\n4\n\n\n2019-06-14\n3\n\n\n2018-10-20\n4\n\n\n\n\n\n\n\nQuestion 5.3 - Multiple Table Joins\nPart 1: Calculate the Dollar Cost Average\nWhat is the dollar cost average (btc_dca) for all Bitcoin purchases by region for each calendar year?\n\nCreate a column called year_start and use the start of the calendar year\nThe dollar cost average calculation is btc_dca = SUM(quantity x price) / SUM(quantity)\n\nPart 2: Yearly Dollar Cost Average Ranking\nUse this btc_dca value to generate a dca_ranking column for each year\n\nThe region with the lowest btc_dca each year has a rank of 1\n\nPart 3: Dollar Cost Average Yearly Percentage Change\nCalculate the yearly percentage change in DCA for each region to 2 decimal places\n\nThis calculation is (current - previous) / previous\n\nFinally order the output by region and year_start columns.\n\nWITH cte_dollar_cost_average AS (\n  SELECT\n    DATE_TRUNC('YEAR', transactions.txn_date)::DATE AS year_start,\n    members.region,\n    SUM(transactions.quantity * prices.price) / SUM(transactions.quantity) AS btc_dca\n  FROM trading.transactions\n  INNER JOIN trading.prices\n    ON transactions.ticker = prices.ticker\n    AND transactions.txn_date = prices.market_date\n  INNER JOIN trading.members\n    ON transactions.member_id = members.member_id\n  WHERE transactions.ticker = 'BTC'\n    AND transactions.txn_type = 'BUY'\n  GROUP BY year_start, members.region\n),\n  \ncte_window_functions AS (\n  SELECT\n    year_start,\n    region,\n    btc_dca,\n    RANK() OVER (PARTITION BY year_start ORDER BY btc_dca) AS dca_ranking,\n    LAG(btc_dca) OVER (PARTITION BY region ORDER BY year_start) AS previous_btc_dca\n  FROM cte_dollar_cost_average\n)\n  \nSELECT\n  year_start,\n  region,\n  btc_dca,\n  dca_ranking,\n  ROUND(\n    (100 * (btc_dca - previous_btc_dca) / previous_btc_dca)::NUMERIC,\n    2\n  ) AS dca_percentage_change\nFROM cte_window_functions\nORDER BY region, year_start\n\n\nDisplaying records 1 - 10\n\n\nyear_start\nregion\nbtc_dca\ndca_ranking\ndca_percentage_change\n\n\n\n\n2017-01-01\nAfrica\n3987.626\n4\nNA\n\n\n2018-01-01\nAfrica\n7690.713\n3\n92.86\n\n\n2019-01-01\nAfrica\n7368.820\n4\n-4.19\n\n\n2020-01-01\nAfrica\n11114.125\n3\n50.83\n\n\n2021-01-01\nAfrica\n44247.215\n2\n298.12\n\n\n2017-01-01\nAsia\n4002.939\n5\nNA\n\n\n2018-01-01\nAsia\n7829.999\n4\n95.61\n\n\n2019-01-01\nAsia\n7267.679\n1\n-7.18\n\n\n2020-01-01\nAsia\n10759.621\n2\n48.05\n\n\n2021-01-01\nAsia\n44570.901\n4\n314.24\n\n\n\n\n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "",
    "text": "The Bike Sales Database represents a bicycle manufacturer, including tables for products (bikes), customers (bike shops), and transactions (orders).\nIt consists of 3 tables:\n\nbikes table, which includes bicycle models, descriptions, and unit prices that are produced by the manufacturer.\nbikeshops table, which includes customers that the bicycle manufacturer has sold to.\norderlines table, which includes transactional data such as order ID, order line, date, customer, product, and quantity sold.\n\nbike_sales database is the local Postgres database stored on my machine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# loading packages\nlibrary(DBI)\nlibrary(RPostgres)  \nlibrary(tidyverse)\nlibrary(lubridate)\n\nThe sql engine uses the DBI package to execute SQL queries, print their results, and optionally assign the results to a data frame. To use the sql engine, we first need to establish a DBI connection to a database.\n\n\n\n\nmycon <- DBI::dbConnect(RPostgres::Postgres(), \n                        dbname = \"bike_sales\", \n                        host = \"localhost\",  \n                        port = \"5432\",  \n                        user = rstudioapi::askForPassword(\"Database username\"),\n                        password = rstudioapi::askForPassword(\"Database password\"))\n\nThere are several options to secure your credentials in R. Here I use prompting for credentials via rstudioapi.\n\nmycon\n\n<PqConnection> bike_sales@localhost:5432\n\n\n\n# list the database table names\ndbListTables(mycon)\n\n[1] \"bikeshops\"  \"bikes\"      \"orderlines\"\n\n\n\n# read the bikeshops table\ndbReadTable(mycon, \"bikeshops\") %>% head()\n\n  bikeshop.id                bikeshop.name       location\n1           1 Pittsburgh Mountain Machines Pittsburgh, PA\n2           2     Ithaca Mountain Climbers     Ithaca, NY\n3           3      Columbus Race Equipment   Columbus, OH\n4           4               Detroit Cycles    Detroit, MI\n5           5             Cincinnati Speed Cincinnati, OH\n6           6    Louisville Race Equipment Louisville, KY\n\n\n\n# read the bikes table\ndbReadTable(mycon, \"bikes\") %>% head()\n\n  bike.id                          model                description price\n1       1        Supersix Evo Black Inc. Road - Elite Road - Carbon 12790\n2       2       Supersix Evo Hi-Mod Team Road - Elite Road - Carbon 10660\n3       3 Supersix Evo Hi-Mod Dura Ace 1 Road - Elite Road - Carbon  7990\n4       4 Supersix Evo Hi-Mod Dura Ace 2 Road - Elite Road - Carbon  5330\n5       5     Supersix Evo Hi-Mod Utegra Road - Elite Road - Carbon  4260\n6       6               Supersix Evo Red Road - Elite Road - Carbon  3940\n\n\n\n# read the orderlines table\ndbReadTable(mycon, \"orderlines\") %>% head()\n\n  order.id order.line order.date customer.id product.id quantity\n1        1          1 2011-01-07           2         48        1\n2        1          2 2011-01-07           2         52        1\n3        2          1 2011-01-10          10         76        1\n4        2          2 2011-01-10          10         52        1\n5        3          1 2011-01-10           6          2        1\n6        3          2 2011-01-10           6         50        1\n\n\n\n# a simple query example\ndbGetQuery(mycon, \n          \"SELECT model, price \n           FROM bikes WHERE price > 10000 \n           ORDER BY price DESC\")\n\n                     model price\n1  Supersix Evo Black Inc. 12790\n2    Scalpel-Si Black Inc. 12790\n3  Habit Hi-Mod Black Inc. 12250\n4          F-Si Black Inc. 11190\n5 Supersix Evo Hi-Mod Team 10660\n\n\n\n\n\nIn all three tables there are dots in column names. This is not a good practice and I first had to figure out how to join the tables without an error! Here is the solution:\n\nbike_orderlines_joined <- dbGetQuery(mycon, \n\n'SELECT * \nFROM orderlines \nLEFT JOIN bikes\nON orderlines.\"product.id\" = bikes.\"bike.id\"\nLEFT JOIN bikeshops\nON orderlines.\"customer.id\" = bikeshops.\"bikeshop.id\"')\n\nhead(bike_orderlines_joined)\n\n  order.id order.line order.date customer.id product.id quantity bike.id\n1        1          1 2011-01-07           2         48        1      48\n2        1          2 2011-01-07           2         52        1      52\n3        2          1 2011-01-10          10         76        1      76\n4        2          2 2011-01-10          10         52        1      52\n5        3          1 2011-01-10           6          2        1       2\n6        3          2 2011-01-10           6         50        1      50\n                     model                       description price bikeshop.id\n1          Jekyll Carbon 2 Mountain - Over Mountain - Carbon  6070           2\n2         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970           2\n3      Beast of the East 1       Mountain - Trail - Aluminum  2770          10\n4         Trigger Carbon 2 Mountain - Over Mountain - Carbon  5970          10\n5 Supersix Evo Hi-Mod Team        Road - Elite Road - Carbon 10660           6\n6          Jekyll Carbon 4 Mountain - Over Mountain - Carbon  3200           6\n              bikeshop.name        location\n1  Ithaca Mountain Climbers      Ithaca, NY\n2  Ithaca Mountain Climbers      Ithaca, NY\n3         Kansas City 29ers Kansas City, KS\n4         Kansas City 29ers Kansas City, KS\n5 Louisville Race Equipment  Louisville, KY\n6 Louisville Race Equipment  Louisville, KY\n\n\n\nglimpse(bike_orderlines_joined)\n\nRows: 15,644\nColumns: 13\n$ order.id      <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,…\n$ order.line    <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1,…\n$ order.date    <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-01…\n$ customer.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ product.id    <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ quantity      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,…\n$ bike.id       <dbl> 48, 52, 76, 52, 2, 50, 1, 4, 34, 26, 96, 66, 35, 72, 45,…\n$ model         <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Eas…\n$ description   <chr> \"Mountain - Over Mountain - Carbon\", \"Mountain - Over Mo…\n$ price         <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570, …\n$ bikeshop.id   <dbl> 2, 2, 10, 10, 6, 6, 6, 6, 6, 22, 8, 8, 8, 8, 16, 16, 16,…\n$ bikeshop.name <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\", …\n$ location      <chr> \"Ithaca, NY\", \"Ithaca, NY\", \"Kansas City, KS\", \"Kansas C…\n\n\nDisconnecting from the database.\n\ndbDisconnect(mycon)\n\n\n\n\n\nbike_orderlines <- bike_orderlines_joined %>% \n  # rename columns - replacing \".\" with \"_\"\n  set_names(names(.) %>% str_replace_all(\"\\\\.\", \"_\")) %>% \n  # remove the unnecessary columns \n  select(-c(customer_id, product_id, bike_id, bikeshop_id)) %>% \n  # separate description into category_1, category_2, and frame_material\n  separate(description, \n           c(\"category_1\", \"category_2\", \"frame_material\"),\n           sep = \" - \") %>% \n  # separate location into city and state\n  separate(location,\n           c(\"city\", \"state\"),\n           sep = \", \") %>%\n  # create a new column total_price\n  mutate(total_price = price * quantity) %>% \n  # reorder columns\n  select(contains(c(\"date\", \"id\", \"order\")),\n         quantity, price, total_price,\n         everything()) \n\nbike_orderlines %>% head()\n\n  order_date order_id order_line quantity price total_price\n1 2011-01-07        1          1        1  6070        6070\n2 2011-01-07        1          2        1  5970        5970\n3 2011-01-10        2          1        1  2770        2770\n4 2011-01-10        2          2        1  5970        5970\n5 2011-01-10        3          1        1 10660       10660\n6 2011-01-10        3          2        1  3200        3200\n                     model category_1    category_2 frame_material\n1          Jekyll Carbon 2   Mountain Over Mountain         Carbon\n2         Trigger Carbon 2   Mountain Over Mountain         Carbon\n3      Beast of the East 1   Mountain         Trail       Aluminum\n4         Trigger Carbon 2   Mountain Over Mountain         Carbon\n5 Supersix Evo Hi-Mod Team       Road    Elite Road         Carbon\n6          Jekyll Carbon 4   Mountain Over Mountain         Carbon\n              bikeshop_name        city state\n1  Ithaca Mountain Climbers      Ithaca    NY\n2  Ithaca Mountain Climbers      Ithaca    NY\n3         Kansas City 29ers Kansas City    KS\n4         Kansas City 29ers Kansas City    KS\n5 Louisville Race Equipment  Louisville    KY\n6 Louisville Race Equipment  Louisville    KY\n\n\n\nbike_orderlines %>% glimpse()\n\nRows: 15,644\nColumns: 13\n$ order_date     <date> 2011-01-07, 2011-01-07, 2011-01-10, 2011-01-10, 2011-0…\n$ order_id       <dbl> 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7…\n$ order_line     <dbl> 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1…\n$ quantity       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1…\n$ price          <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ total_price    <dbl> 6070, 5970, 2770, 5970, 10660, 3200, 12790, 5330, 1570,…\n$ model          <chr> \"Jekyll Carbon 2\", \"Trigger Carbon 2\", \"Beast of the Ea…\n$ category_1     <chr> \"Mountain\", \"Mountain\", \"Mountain\", \"Mountain\", \"Road\",…\n$ category_2     <chr> \"Over Mountain\", \"Over Mountain\", \"Trail\", \"Over Mounta…\n$ frame_material <chr> \"Carbon\", \"Carbon\", \"Aluminum\", \"Carbon\", \"Carbon\", \"Ca…\n$ bikeshop_name  <chr> \"Ithaca Mountain Climbers\", \"Ithaca Mountain Climbers\",…\n$ city           <chr> \"Ithaca\", \"Ithaca\", \"Kansas City\", \"Kansas City\", \"Loui…\n$ state          <chr> \"NY\", \"NY\", \"KS\", \"KS\", \"KY\", \"KY\", \"KY\", \"KY\", \"KY\", \"…"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#lollipop-chart-top-n-customers",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Lollipop Chart: Top N Customers",
    "text": "Lollipop Chart: Top N Customers\nQuestion: How much purchasing power is in top 10 customers?\nGoal is to visualize top N customers in terms of Revenue, including cumulative percentage.\n\nData manipulation\n\nn <- 10\n\ntop_customers <- bike_orderlines %>% \n  select(bikeshop_name, total_price) %>% \n  mutate(bikeshop_name = as_factor(bikeshop_name) %>% fct_lump_n(n = n, w = total_price)) %>% \n  group_by(bikeshop_name) %>% \n  summarise(revenue = sum(total_price)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_reorder(revenue)) %>% \n  mutate(bikeshop_name = bikeshop_name %>% fct_relevel(\"Other\", after = 0)) %>% \n  arrange(desc(bikeshop_name)) %>% \n  # revenue text\n  mutate(revenue_text = scales::dollar(revenue, scale = 1e-06, suffix = \"M\")) %>% \n  # cumulative percent\n  mutate(cum_pct = cumsum(revenue) / sum(revenue)) %>% \n  mutate(cum_pct_text = scales::percent(cum_pct)) %>% \n  # rank\n  mutate(rank = row_number()) %>% \n  mutate(rank = ifelse(rank == max(rank), NA_integer_, rank)) %>% \n  # label text\n  mutate(label_text = str_glue(\"Rank: {rank}\\nRev: {revenue_text}\\nCumPct: {cum_pct_text}\")) \n\ntop_customers\n\n# A tibble: 11 × 7\n   bikeshop_name                 revenue revenue…¹ cum_pct cum_p…²  rank label…³\n   <fct>                           <dbl> <chr>       <dbl> <chr>   <int> <glue> \n 1 Kansas City 29ers            11535455 $11.54M     0.162 16.2%       1 Rank: …\n 2 Denver Bike Shop              7697670 $7.70M      0.271 27.1%       2 Rank: …\n 3 Ithaca Mountain Climbers      6299335 $6.30M      0.359 35.9%       3 Rank: …\n 4 Phoenix Bi-peds               4168535 $4.17M      0.418 41.8%       4 Rank: …\n 5 Oklahoma City Race Equipment  3450040 $3.45M      0.467 46.7%       5 Rank: …\n 6 Las Vegas Cycles              3073615 $3.07M      0.510 51.0%       6 Rank: …\n 7 New Orleans Velocipedes       2761825 $2.76M      0.549 54.9%       7 Rank: …\n 8 Wichita Speed                 2380385 $2.38M      0.582 58.2%       8 Rank: …\n 9 Miami Race Equipment          2057130 $2.06M      0.611 61.1%       9 Rank: …\n10 Minneapolis Bike Shop         2023220 $2.02M      0.640 64.0%      10 Rank: …\n11 Other                        25585120 $25.59M     1     100.0%     NA Rank: …\n# … with abbreviated variable names ¹​revenue_text, ²​cum_pct_text, ³​label_text\n\n\n\n\nData visualization\n\ntop_customers %>% \n  ggplot(aes(revenue, bikeshop_name)) +\n  # geometries\n  geom_segment(aes(xend = 0, yend = bikeshop_name), \n               color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1],\n               size = 1) +\n  geom_point(color = RColorBrewer::brewer.pal(n = 9, name = \"Set1\")[1], \n             size = 3) +\n  geom_label(aes(label = label_text), \n             hjust = \"left\", \n             size = 3,\n             nudge_x = 0.8e+06) +\n  # formatting\n  scale_x_continuous(labels = scales::dollar_format(scale = 1e-06, suffix = \"M\")) +\n  labs(title = str_glue(\"Top {n} customers in terms of revenue, with cumulative percentage\"),\n       subtitle = str_glue(\"Top {n} customers contribute {top_customers$cum_pct_text[n]} of purchasing power.\"),\n       x = \"Revenue ($M)\",\n       y = \"Customer\",\n       caption = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\")) +\n  expand_limits(x = max(top_customers$revenue) + 6e+06) +\n  # theme\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "href": "posts/customer-analysis-advanced-plots-with-ggplot2/customer-analysis-advanced-plots-with-ggplot2.html#heatmap-customers-purchasing-habits",
    "title": "Customer Analysis – Advanced Plots With {ggplot2}",
    "section": "Heatmap: Customers’ Purchasing Habits",
    "text": "Heatmap: Customers’ Purchasing Habits\nQuestion: Do specific customers have a purchasing preference?\nGoal is to visualize heatmap of proportion of sales by Secondary Product Category.\n\nData manipulation\n\npct_sales_by_customer <- bike_orderlines %>% \n  select(bikeshop_name, category_1, category_2, quantity) %>% \n  group_by(bikeshop_name, category_1, category_2) %>% \n  summarise(total_qty = sum(quantity)) %>% \n  ungroup() %>% \n  group_by(bikeshop_name) %>% \n  mutate(pct = total_qty / sum(total_qty)) %>% \n  ungroup() %>% \n  mutate(bikeshop_name = as.factor(bikeshop_name) %>% fct_rev()) %>%  \n  mutate(bikeshop_name_num = as.numeric(bikeshop_name))\n    \npct_sales_by_customer   \n\n# A tibble: 270 × 6\n   bikeshop_name      category_1 category_2         total_qty    pct bikeshop_…¹\n   <fct>              <chr>      <chr>                  <dbl>  <dbl>       <dbl>\n 1 Albuquerque Cycles Mountain   Cross Country Race        48 0.168           30\n 2 Albuquerque Cycles Mountain   Fat Bike                   9 0.0315          30\n 3 Albuquerque Cycles Mountain   Over Mountain             13 0.0455          30\n 4 Albuquerque Cycles Mountain   Sport                     35 0.122           30\n 5 Albuquerque Cycles Mountain   Trail                     38 0.133           30\n 6 Albuquerque Cycles Road       Cyclocross                 7 0.0245          30\n 7 Albuquerque Cycles Road       Elite Road                69 0.241           30\n 8 Albuquerque Cycles Road       Endurance Road            54 0.189           30\n 9 Albuquerque Cycles Road       Triathalon                13 0.0455          30\n10 Ann Arbor Speed    Mountain   Cross Country Race        32 0.0532          29\n# … with 260 more rows, and abbreviated variable name ¹​bikeshop_name_num\n\n\n\n\nData visualization\n\npct_sales_by_customer %>% \n  ggplot(aes(category_2, bikeshop_name)) + \n  # geometries\n  geom_tile(aes(fill = pct)) +\n  geom_text(aes(label = scales::percent(pct, accuracy = 0.1)),\n            size = 3,\n            color = ifelse(pct_sales_by_customer$pct >= 0.15, \"white\", \"black\")) +\n  facet_wrap(~ category_1, scales = \"free_x\") + \n  # formatting\n  scale_fill_gradient(low = \"white\", high = tidyquant::palette_light()[1]) + \n  labs(title = \"Heatmap of Purchasing Habits\", \n       subtitle = str_glue(\"Year: {year(min(bike_orderlines$order_date))} - {year(max(bike_orderlines$order_date))}\"),\n       x = \"Bike Type\",\n       y = \"Customer\") + \n  # theme\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = tidyquant::palette_light()[1], \n                                        color = \"white\"), \n        strip.text = element_text(color = \"white\", size = 11), \n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\nTop 3 customers that prefer mountain bikes:\n\nIthaca Mountain Climbers\nPittsburgh Mountain Machines\nTampa 29ers\n\nTop 3 customers that prefer road bikes:\n\nAnn Arbor Speed\nAustin Cruisers\nIndianapolis Velocipedes\n\n\nThat’s it! I hope you like it. For those wondering where I learned to make plots like this… in a fabulous course Data Science for Business Part 1 by Matt Dancho. This is probably the best course on R and I highly recommend it."
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#problem-statement",
    "href": "posts/customer-segmentation/cust-segm-report.html#problem-statement",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Problem Statement",
    "text": "Problem Statement\nMarketing team would like to increase email campaign engagement by segmenting the customer-base using their buying habits."
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#solution-summary",
    "href": "posts/customer-segmentation/cust-segm-report.html#solution-summary",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Solution Summary",
    "text": "Solution Summary\nThe 4 customer segments were identified and given descriptions based on the customer’s top product purchases.\n\nSegment 1 Preferences: Road Bikes, Below $3200 (Economical Models) - 27%\nSegment 2 Preferences: Mountain Bikes, Above $3200 (Premium Models) - 10%\nSegment 3 Preferences: Road Bikes, Above $3200 (Premium Models) - 20%\nSegment 4 Preferences: Both Road and Mountain, Below $3200 (Economical Models) - 43%"
  },
  {
    "objectID": "posts/customer-segmentation/cust-segm-report.html#customer-preferences",
    "href": "posts/customer-segmentation/cust-segm-report.html#customer-preferences",
    "title": "Customer Segmentation With K-Means and UMAP",
    "section": "Customer Preferences",
    "text": "Customer Preferences\n\nHeat Map\nOur customer-base consists of 30 bike shops. Several customers have purchasing preferences for Road or Mountain Bikes based on the proportion of bikes purchased by category (mountain or road) and sub-category (Over Mountain, Trail, Elite Road, etc).\n\n\n\n\n\n\n\n\nCustomer Segmentation\nThis is a 2D Projection based on customer similarity that exposes 4 clusters, which are key segments in the customer base.\n\n\n\n\n\n\n\n\nCustomer Preferences By Segment\nThe 4 customer segments were given descriptions based on the customer’s top product purchases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: The table below is sortable. You can sort a column by clicking on its header."
  },
  {
    "objectID": "posts/cyclistic-bike-share-analysis/cyclistic.html#foreword",
    "href": "posts/cyclistic-bike-share-analysis/cyclistic.html#foreword",
    "title": "Cyclistic Bike-Share Analysis",
    "section": "Foreword",
    "text": "Foreword\nIn September 2021, I completed the Google Data Analytics Professional Certificate hosted on Coursera. The program is very extensive and covers all the steps of the data analysis process as taught by Google (ask, prepare, process, analyze, share, and act). It was my first encounter with databases and analytics tools. I really enjoyed it very much, and I have to say, it made some useful corrections to my analytical mind. I am very grateful for that and highly recommend it!\nIt consists of 8 courses, the last of which is dedicated to a capstone project. As a passionate cyclist, I have chosen the Cyclistic bike-share case study to showcase what I have learned. In terms of analytics, this was my very first project and I hope you’ll enjoy it!"
  },
  {
    "objectID": "posts/cyclistic-bike-share-analysis/cyclistic.html#introduction",
    "href": "posts/cyclistic-bike-share-analysis/cyclistic.html#introduction",
    "title": "Cyclistic Bike-Share Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this case study, I am a junior data analyst working in the marketing analyst team at Cyclistic, a fictional bike-share company in Chicago. The director of marketing believes the company’s future success depends on maximizing the number of annual memberships. Therefore, my team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights, my team will design a new marketing strategy to convert casual riders into annual members. But first, Cyclistic executives must approve our recommendations, so they must be backed up with compelling data insights and professional data visualizations.\nThree questions will guide the future marketing program:\n\nHow do annual members and casual riders use Cyclistic bikes differently?\nWhy would casual riders buy Cyclistic annual memberships?\nHow can Cyclistic use digital media to influence casual riders to become members?\n\nLily Moreno, the director of marketing and my manager has assigned me the first question to answer."
  },
  {
    "objectID": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-i-ask",
    "href": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-i-ask",
    "title": "Cyclistic Bike-Share Analysis",
    "section": "✨ PART I: Ask",
    "text": "✨ PART I: Ask\nIn the first step of the data analysis process, we identify the business task and consider key stakeholders.\n\nBusiness taskHow do annual members and casual riders use Cyclistic bikes differently?\n\nPrimary Stakeholders\n\nLily Moreno: the Director of Marketing and my manager\nCyclistic Executive Team: the notoriously detail-oriented executive team who will decide whether to approve the recommended marketing program.\n\nSecondary Stakeholders\n\nCyclistic Marketing Analytics Team: a team of data analysts who are responsible for collecting, analyzing, and reporting data that helps guide Cyclistic marketing strategy (my team)"
  },
  {
    "objectID": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-ii-prepare",
    "href": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-ii-prepare",
    "title": "Cyclistic Bike-Share Analysis",
    "section": "✨ PART II: Prepare",
    "text": "✨ PART II: Prepare\nThe objective of this step is to prepare data for the analysis. I will use the past 12 months of Cyclitis’s historical trip data (from September 2020 to August 2021) to analyze and identify trends. Because Cyclistic is a fictional company, for the purposes of this case study, I will use data from a real bike-share company in Chicago, Divvy. The data has been made available for public use by Motivate International Inc. under this license and can be downloaded here.\nThe data is released on a monthly schedule and anonymized. It is reliable, original, comprehensive, current, and cited.\nI have downloaded 12 CSV files (datasets). Each dataset contains historical trip data for an individual month. In all 12 datasets, each variable has its own column, each observation has its own row, and each value has its own cell. Therefore, I can conclude that the datasets are tidy.\nThere are 13 columns (variables) in each CSV file. Metadata isn’t provided, but most of the variables are self-explanatory.\n\nData dictionary\n\n\n\n\n\n\n\n\n\nNo\nColumn Name\nData Type\nDefinition\n\n\n\n\n1\nride_id\nText\nUnique ride ID\n\n\n2\nrideable_type\nText\nClassic, docked, or electric bike\n\n\n3\nstarted_at\nDate & Time\nTrip start date and time\n\n\n4\nended_at\nDate & Time\nTrip end date and time\n\n\n5\nstart_station_name\nText\nTrip start station name\n\n\n6\nstart_station_id\nText\nTrip start station ID\n\n\n7\nend_station_name\nText\nTrip end station name\n\n\n8\nend_station_id\nText\nTrip end station ID\n\n\n9\nstart_lat\nNumeric\nTrip start station latitude\n\n\n10\nstart_lng\nNumeric\nTrip start station longitude\n\n\n11\nend_lat\nNumeric\nTrip end station latitude\n\n\n12\nend_lng\nNumeric\nTrip end station longitude\n\n\n13\nmember_casual\nText\nUser type (casual or member)\n\n\n\nOnly two variables need further clarification:\n\nrideable_type - there are three possible values for this variable: classic_bike, docked_bike, and electric_bike. classic_bike is actually a classic dockless bike, docked_bike is a classic docked bike, and electric_bike is an e-bike that can be either docked at any station or locked with cable at any e-station for no additional cost. For an extra $2, it’s possible to lock a bike to any public bike rack, light pole, signpost, or retired parking meter outside of a station within the service area. Classic blue Divvy bikes can only be docked at the traditional Divvy stations.\nmember_casual - there are two possible values for this variable: casual and member, representing casual riders and annual members. Casual riders buy a Single Ride Pass (one trip up to 30 minutes) or a Day Pass (unlimited 3-hour rides for 24-hours), while members buy an Annual Membership (unlimited 45-min rides). This is important because, in the cleaning step of the analysis, I will filter out all trips with a ride length longer than 3 hours.\n\n\n\n\n\n\nFig 1: Divvy plans and pricing\n\n\n\n\n\n\nData issues\nIf we want to show the most popular stations for each group on the map, each station must have unique geographical coordinates. This is not the case for trips taken with electric bikes. Each such ride has its own starting and ending coordinates, regardless of the start or end station. The reason for this is that electric bikes can be parked outside of the stations within a service area. The following maps made in Tableau on just one month of data serve to illustrate this issue:\n\n\n\n\n\nFig 2: Start station frequency for classic and docked bikes\n\n\n\n\n\n\n\n\n\nFig 3: Start station frequency for electric bikes\n\n\n\n\nTherefore, we’ll additionally use the publicly available CSV file, Divvy_Bicycle_Stations.csv, that contains a list of all stations and corresponding geographical coordinates (link). This file is updated regularly. My version is from October 2021. We’ll also use it to check current station names.\n\n\nSetting up the programming environment\n\n# loading packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(leaflet)\nlibrary(reactable)\nlibrary(htmltools)\n\n# ggplot theme\ntheme_set(theme_classic())\n\n\n\nReading the datasets\nLet’s see what’s in the data directory.\n\ndata_dir <- \"data\"\n\nfs::dir_ls(data_dir)\n\ndata/202009-divvy-tripdata.csv  data/202010-divvy-tripdata.csv  \ndata/202011-divvy-tripdata.csv  data/202012-divvy-tripdata.csv  \ndata/202101-divvy-tripdata.csv  data/202102-divvy-tripdata.csv  \ndata/202103-divvy-tripdata.csv  data/202104-divvy-tripdata.csv  \ndata/202105-divvy-tripdata.csv  data/202106-divvy-tripdata.csv  \ndata/202107-divvy-tripdata.csv  data/202108-divvy-tripdata.csv  \ndata/Divvy_Bicycle_Stations.csv \n\n\nWe’ll combine dir_ls(), map_dfr() and read_csv() to find data files with monthly trip data in the directory and read them all together into a single data frame. Columns start_station_id and end_station_id in the first three datasets (2020-Sep to 2020-Nov) are of the numeric data type, and all the rest are of characters. To make row binding possible, we’ll give an instruction to read them all as characters.\n\n# monthly trip data\nall_trips <- data_dir %>% \n  fs::dir_ls(regexp = \"tripdata\") %>% \n  map_dfr(read_csv, \n          col_types = cols(\"start_station_id\" = col_character(), \n                           \"end_station_id\" = col_character()))\n\n# current Divvy stations with coordinates\ndivvy_stations <- read_csv(\"data/Divvy_Bicycle_Stations.csv\") %>% \n  clean_names()\n\n\n\nData overview\nMonthly trip data\n\n# data dimensions (rows x columns)\ndim_desc(all_trips)\n\n[1] \"[4,913,072 x 13]\"\n\n\nIt’s a large dataset with almost 5 million rows and 13 columns.\n\nglimpse(all_trips)\n\nRows: 4,913,072\nColumns: 13\n$ ride_id            <chr> \"2B22BD5F95FB2629\", \"A7FB70B4AFC6CAF2\", \"86057FA01B…\n$ rideable_type      <chr> \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         <dttm> 2020-09-17 14:27:11, 2020-09-17 15:07:31, 2020-09-…\n$ ended_at           <dttm> 2020-09-17 14:44:24, 2020-09-17 15:07:45, 2020-09-…\n$ start_station_name <chr> \"Michigan Ave & Lake St\", \"W Oakdale Ave & N Broadw…\n$ start_station_id   <chr> \"52\", NA, NA, \"246\", \"24\", \"94\", \"291\", NA, NA, NA,…\n$ end_station_name   <chr> \"Green St & Randolph St\", \"W Oakdale Ave & N Broadw…\n$ end_station_id     <chr> \"112\", NA, NA, \"249\", \"24\", NA, \"256\", NA, NA, NA, …\n$ start_lat          <dbl> 41.88669, 41.94000, 41.94000, 41.95606, 41.89186, 4…\n$ start_lng          <dbl> -87.62356, -87.64000, -87.64000, -87.66892, -87.621…\n$ end_lat            <dbl> 41.88357, 41.94000, 41.94000, 41.96398, 41.89135, 4…\n$ end_lng            <dbl> -87.64873, -87.64000, -87.64000, -87.63822, -87.620…\n$ member_casual      <chr> \"casual\", \"casual\", \"casual\", \"casual\", \"casual\", \"…\n\n\n\nrmarkdown::paged_table(head(all_trips)) \n\n\n\n  \n\n\n\n\nhead(all_trips$started_at)\n\n[1] \"2020-09-17 14:27:11 UTC\" \"2020-09-17 15:07:31 UTC\"\n[3] \"2020-09-17 15:09:04 UTC\" \"2020-09-17 18:10:46 UTC\"\n[5] \"2020-09-17 15:16:13 UTC\" \"2020-09-17 18:37:04 UTC\"\n\n\nDatetimes are stored in the UTC timezone, which is good considering I’m in Europe.\nCurrent Divvy stations with corresponding coordinates\n\ndim_desc(divvy_stations)\n\n[1] \"[785 x 8]\"\n\n\n\nrmarkdown::paged_table(head(divvy_stations))"
  },
  {
    "objectID": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-iii---process",
    "href": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-iii---process",
    "title": "Cyclistic Bike-Share Analysis",
    "section": "✨ PART III - Process",
    "text": "✨ PART III - Process\nIn this step, we are going to explore the data further and finally clean it.\nRight now we can aggregate data only at the ride level, so we’ll create 3 new columns year_month, day_of_week, and hour extracted from the datetime column started_at and convert them to factors where needed.\n\nall_trips <- all_trips %>% \n  mutate(year_month = format(started_at, \"%Y-%m\") %>% as_factor(),\n         day_of_week = wday(started_at, label = TRUE, \n                            week_start = getOption(\"lubridate.week.start\", 1)),\n         hour = hour(started_at) %>% as_factor()) \n\nCalculating and creating a new column ride_length in minutes, then converting it to a numeric data type and rounding to 2 decimal places\n\nall_trips <- all_trips %>% \n  mutate(ride_length = difftime(ended_at, started_at, units=\"mins\") %>% \n           as.numeric() %>% round(2))\n\nChecking if everything is OK.\n\nrmarkdown::paged_table(sample_n(all_trips, 10))\n\n\n\n  \n\n\n\n\nAll is good! Let’s see the summary statistics.\n\nall_trips %>% skim_without_charts()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n4913072\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nride_id\n0\n1.00\n16\n16\n0\n4912863\n0\n\n\nrideable_type\n0\n1.00\n11\n13\n0\n3\n0\n\n\nstart_station_name\n450045\n0.91\n3\n53\n0\n757\n0\n\n\nstart_station_id\n450571\n0.91\n1\n36\n0\n1293\n0\n\n\nend_station_name\n491380\n0.90\n10\n53\n0\n756\n0\n\n\nend_station_id\n491764\n0.90\n1\n36\n0\n1293\n0\n\n\nmember_casual\n0\n1.00\n6\n6\n0\n2\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nyear_month\n0\n1\nFALSE\n12\n202: 822410, 202: 804352, 202: 729595, 202: 532958\n\n\nday_of_week\n0\n1\nTRUE\n7\nSat: 889412, Sun: 758229, Fri: 719239, Wed: 648981\n\n\nhour\n0\n1\nFALSE\n24\n17: 499035, 18: 435037, 16: 415752, 15: 353319\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nstart_lat\n0\n1\n41.90\n0.04\n41.64\n41.88\n41.90\n41.93\n42.08\n\n\nstart_lng\n0\n1\n-87.65\n0.03\n-87.84\n-87.66\n-87.64\n-87.63\n-87.52\n\n\nend_lat\n5015\n1\n41.90\n0.04\n41.51\n41.88\n41.90\n41.93\n42.15\n\n\nend_lng\n5015\n1\n-87.65\n0.03\n-88.07\n-87.66\n-87.64\n-87.63\n-87.44\n\n\nride_length\n0\n1\n21.14\n317.71\n-29049.97\n7.18\n12.80\n23.27\n55944.15\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nstarted_at\n0\n1\n2020-09-01 00:00:07\n2021-08-31 23:59:35\n2021-05-26 16:51:05\n4137017\n\n\nended_at\n0\n1\n2020-09-01 00:04:43\n2021-09-01 17:37:35\n2021-05-26 17:12:45\n4124534\n\n\n\n\n\n\nBy observing the data summary we can notice the following issues that need to be addressed:\n\nThe number of unique ride IDs doesn’t match the number of observations - duplicate rows,\nMissing values (NAs) in the start_station_name, start_station_id, end_station_name, and end_station_id columns,\nNumber of station IDs doesn’t match the number of stations - it is almost twice as big,\nName of the member_casual column is vague, we’ll rename it to user_type,\n5015 missing values (NAs) in the end_lat and end_lng columns,\nThe minimum value for the ride_length column is -20.2 days, the maximum value is 38.9 days.\n\n\n🛠️Removing duplicated records based on the ride_id column\nEach row represents one observation (trip). Based on the previous summary, there are a total of 4,913,072 rows and 4,912,863 unique ride_id values, meaning there are 209 rows of duplicated data.\n\ndupes <- all_trips %>% \n  get_dupes(ride_id) \n\ndupes %>% rmarkdown::paged_table()\n\n\n\n  \n\n\n\nThese are all trips taken with docked bikes. They started on two different dates, 2020-11-25 and 2020-12-15, and ended on 2020-11-25. Since trips started on 2020-12-15 have negative ride lengths, we’re going to remove them and create a new dataframe.\n\nall_trips_cln <- \n  setdiff(all_trips, \n          dupes %>% select(-dupe_count) %>% filter(date(started_at) == \"2020-12-15\")) \n\nNumber of removed records.\n\nnrow(all_trips) - nrow(all_trips_cln)\n\n[1] 209\n\n\n\n\n🛠️ Renaming column member_casual to user_type\n\nall_trips_cln <- all_trips_cln %>% rename(user_type = member_casual)\n\n\n\n🛠️ Removing observations with missing values in the end_lat and end_lng columns\nEvery ride has to end somewhere. The average ride length is unusually high for this subset (you don’t see it here; I want to spare you the details). Traffic accident, malfunction? We’ll remove those trips.\nWe won’t need the columns with coordinates anymore, so we’ll remove them to make the dataframe lighter. Later on, we’ll join the coordinates from the divvy_stations dataframe.\n\nall_trips_cln <- all_trips_cln %>% \n  filter(!is.na(end_lat), !is.na(end_lng)) %>% \n  select(-c(start_lat, start_lng, end_lat, end_lng))\n\n\n\n🛠️ Removing trips\n\nassociated with Divvy test and repair stations\nwith negative ride length (the end of the trip precedes the start)\nwith ride length between 0 and 1 min (potentially false starts or users trying to re-dock a bike to ensure it was secure)\nwith a ride length longer than 180 minutes (3 hours)\n\nLooking for specific values in the start_station_name column associated with Divvy test and repair stations.\n\n\nUnique start station names\n\n\nunique(all_trips_cln$start_station_name)\n\n  [1] \"Michigan Ave & Lake St\"                               \n  [2] \"W Oakdale Ave & N Broadway\"                           \n  [3] \"Ashland Ave & Belle Plaine Ave\"                       \n  [4] \"Fairbanks Ct & Grand Ave\"                             \n  [5] \"Clark St & Armitage Ave\"                              \n  [6] \"Wells St & Evergreen Ave\"                             \n  [7] NA                                                     \n  [8] \"Michigan Ave & 18th St\"                               \n  [9] \"Mies van der Rohe Way & Chestnut St\"                  \n [10] \"Halsted St & Polk St\"                                 \n [11] \"Albany Ave & Bloomingdale Ave\"                        \n [12] \"Lake Shore Dr & Diversey Pkwy\"                        \n [13] \"Elston Ave & Wabansia Ave\"                            \n [14] \"Clybourn Ave & Division St\"                           \n [15] \"Campbell Ave & Fullerton Ave\"                         \n [16] \"Clark St & Lake St\"                                   \n [17] \"Sawyer Ave & Irving Park Rd\"                          \n [18] \"Financial Pl & Ida B Wells Dr\"                        \n [19] \"Wells St & Huron St\"                                  \n [20] \"Shedd Aquarium\"                                       \n [21] \"Streeter Dr & Grand Ave\"                              \n [22] \"Bissell St & Armitage Ave\"                            \n [23] \"Franklin St & Lake St\"                                \n [24] \"Sedgwick St & Huron St\"                               \n [25] \"Dearborn Pkwy & Delaware Pl\"                          \n [26] \"Halsted St & Dickens Ave\"                             \n [27] \"Clark St & Schiller St\"                               \n [28] \"Michigan Ave & Washington St\"                         \n [29] \"Aberdeen St & Jackson Blvd\"                           \n [30] \"Wilton Ave & Diversey Pkwy\"                           \n [31] \"Wells St & Hubbard St\"                                \n [32] \"Archer (Damen) Ave & 37th St\"                         \n [33] \"Franklin St & Monroe St\"                              \n [34] \"Dearborn St & Monroe St\"                              \n [35] \"Morgan St & Lake St\"                                  \n [36] \"Broadway & Granville Ave\"                             \n [37] \"Columbus Dr & Randolph St\"                            \n [38] \"California Ave & Milwaukee Ave\"                       \n [39] \"Milwaukee Ave & Cuyler Ave\"                           \n [40] \"Wells St & Concord Ln\"                                \n [41] \"Ashland Ave & Wellington Ave\"                         \n [42] \"Sheridan Rd & Buena Ave\"                              \n [43] \"Cottage Grove Ave & 83rd St\"                          \n [44] \"LaSalle Dr & Huron St\"                                \n [45] \"Michigan Ave & 14th St\"                               \n [46] \"Damen Ave & Division St\"                              \n [47] \"Museum of Science and Industry\"                       \n [48] \"Wells St & Polk St\"                                   \n [49] \"Clark St & North Ave\"                                 \n [50] \"Milwaukee Ave & Grand Ave\"                            \n [51] \"Wentworth Ave & 104th St\"                             \n [52] \"Rush St & Cedar St\"                                   \n [53] \"Lincoln Park Conservatory\"                            \n [54] \"Lake Shore Dr & Wellington Ave\"                       \n [55] \"Dearborn St & Erie St\"                                \n [56] \"Adler Planetarium\"                                    \n [57] \"Leavitt St & Addison St\"                              \n [58] \"Damen Ave & Madison St\"                               \n [59] \"Sheffield Ave & Willow St\"                            \n [60] \"McClurg Ct & Illinois St\"                             \n [61] \"Indiana Ave & Roosevelt Rd\"                           \n [62] \"Orleans St & Hubbard St\"                              \n [63] \"Seeley Ave & Roscoe St\"                               \n [64] \"Clark St & Grace St\"                                  \n [65] \"Wabash Ave & Grand Ave\"                               \n [66] \"Clark St & Schreiber Ave\"                             \n [67] \"Paulina Ave & North Ave\"                              \n [68] \"Southport Ave & Waveland Ave\"                         \n [69] \"Wentworth Ave & Cermak Rd\"                            \n [70] \"Buckingham Fountain\"                                  \n [71] \"Fort Dearborn Dr & 31st St\"                           \n [72] \"Francisco Ave & Foster Ave\"                           \n [73] \"Sheridan Rd & Loyola Ave\"                             \n [74] \"Clinton St & Madison St\"                              \n [75] \"Halsted St & Clybourn Ave\"                            \n [76] \"Halsted St & Wrightwood Ave\"                          \n [77] \"Peoria St & Jackson Blvd\"                             \n [78] \"Ellis Ave & 60th St\"                                  \n [79] \"Michigan Ave & Madison St\"                            \n [80] \"Lakefront Trail & Bryn Mawr Ave\"                      \n [81] \"Lake Shore Dr & North Blvd\"                           \n [82] \"Clark St & Bryn Mawr Ave\"                             \n [83] \"University Ave & 57th St\"                             \n [84] \"Ashland Ave & Blackhawk St\"                           \n [85] \"Larrabee St & Armitage Ave\"                           \n [86] \"Damen Ave & Pierce Ave\"                               \n [87] \"Damen Ave & Melrose Ave\"                              \n [88] \"Racine Ave & Belmont Ave\"                             \n [89] \"Franklin St & Illinois St\"                            \n [90] \"Broadway & Thorndale Ave\"                             \n [91] \"Ashland Ave & Chicago Ave\"                            \n [92] \"Honore St & Division St\"                              \n [93] \"Southport Ave & Wrightwood Ave\"                       \n [94] \"Clark St & Winnemac Ave\"                              \n [95] \"Eastlake Ter & Rogers Ave\"                            \n [96] \"Clarendon Ave & Junior Ter\"                           \n [97] \"Lake Shore Dr & Monroe St\"                            \n [98] \"Kedzie Ave & Milwaukee Ave\"                           \n [99] \"Michigan Ave & 8th St\"                                \n[100] \"Clinton St & Jackson Blvd\"                            \n[101] \"Lincoln Ave & Sunnyside Ave\"                          \n[102] \"Lake Park Ave & 53rd St\"                              \n[103] \"Desplaines St & Kinzie St\"                            \n[104] \"Lake Park Ave & 35th St\"                              \n[105] \"Kingsbury St & Kinzie St\"                             \n[106] \"Dayton St & North Ave\"                                \n[107] \"Lakeview Ave & Fullerton Pkwy\"                        \n[108] \"Paulina St & Howard St\"                               \n[109] \"Clarendon Ave & Leland Ave\"                           \n[110] \"Chicago Ave & Sheridan Rd\"                            \n[111] \"May St & Cullerton St\"                                \n[112] \"Wood St & Milwaukee Ave\"                              \n[113] \"Green St & Madison St\"                                \n[114] \"Western Ave & Winnebago Ave\"                          \n[115] \"Broadway & Cornelia Ave\"                              \n[116] \"Orleans St & Merchandise Mart Plaza\"                  \n[117] \"McClurg Ct & Erie St\"                                 \n[118] \"Dorchester Ave & 49th St\"                             \n[119] \"Sheffield Ave & Wrightwood Ave\"                       \n[120] \"Shore Dr & 55th St\"                                   \n[121] \"Larrabee St & Kingsbury St\"                           \n[122] \"Damen Ave & Wellington Ave\"                           \n[123] \"Wood St & Chicago Ave\"                                \n[124] \"Desplaines St & Jackson Blvd\"                         \n[125] \"LaSalle St & Illinois St\"                             \n[126] \"State St & Kinzie St\"                                 \n[127] \"Richmond St & Diversey Ave\"                           \n[128] \"Sedgwick St & Schiller St\"                            \n[129] \"Southport Ave & Wellington Ave\"                       \n[130] \"Eberhart Ave & 61st St\"                               \n[131] \"MLK Jr Dr & 29th St\"                                  \n[132] \"Indiana Ave & 26th St\"                                \n[133] \"Humboldt Blvd & Armitage Ave\"                         \n[134] \"Damen Ave & Chicago Ave\"                              \n[135] \"Burling St & Diversey Pkwy\"                           \n[136] \"Franklin St & Adams St (Temp)\"                        \n[137] \"Greenview Ave & Jarvis Ave\"                           \n[138] \"Mies van der Rohe Way & Chicago Ave\"                  \n[139] \"Racine Ave & 18th St\"                                 \n[140] \"Kedzie Ave & Roosevelt Rd\"                            \n[141] \"Ridge Blvd & Howard St\"                               \n[142] \"Canal St & Adams St\"                                  \n[143] \"Lake Shore Dr & Ohio St\"                              \n[144] \"Ritchie Ct & Banks St\"                                \n[145] \"Clinton St & Lake St\"                                 \n[146] \"Noble St & Milwaukee Ave\"                             \n[147] \"Logan Blvd & Elston Ave\"                              \n[148] \"Austin Blvd & Lake St\"                                \n[149] \"Wabash Ave & Roosevelt Rd\"                            \n[150] \"Loomis St & Lexington St\"                             \n[151] \"Wabash Ave & 9th St\"                                  \n[152] \"Larrabee St & Oak St\"                                 \n[153] \"Wells St & Walton St\"                                 \n[154] \"Ashland Ave & Wrightwood Ave\"                         \n[155] \"Orleans St & Chestnut St (NEXT Apts)\"                 \n[156] \"Clark St & Jarvis Ave\"                                \n[157] \"Larrabee St & Menomonee St\"                           \n[158] \"Broadway & Waveland Ave\"                              \n[159] \"Clark St & Elmdale Ave\"                               \n[160] \"Racine Ave & Washington Blvd\"                         \n[161] \"Ellis Ave & 58th St\"                                  \n[162] \"Racine Ave & Fullerton Ave\"                           \n[163] \"Drake Ave & Fullerton Ave\"                            \n[164] \"Indiana Ave & 31st St\"                                \n[165] \"Broadway & Berwyn Ave\"                                \n[166] \"Kingsbury St & Erie St\"                               \n[167] \"Dearborn St & Van Buren St\"                           \n[168] \"Clark St & Newport St\"                                \n[169] \"Ogden Ave & Race Ave\"                                 \n[170] \"Clinton St & Roosevelt Rd\"                            \n[171] \"Sheridan Rd & Lawrence Ave\"                           \n[172] \"Larrabee St & Webster Ave\"                            \n[173] \"Green St & Randolph St\"                               \n[174] \"Leavitt St & Belmont Ave\"                             \n[175] \"Kosciuszko Park\"                                      \n[176] \"Michigan Ave & Jackson Blvd\"                          \n[177] \"Montrose Harbor\"                                      \n[178] \"Lincoln Ave & Winona St\"                              \n[179] \"Aberdeen St & Randolph St\"                            \n[180] \"Clark St & Berwyn Ave\"                                \n[181] \"Ada St & Washington Blvd\"                             \n[182] \"Clark St & Elm St\"                                    \n[183] \"Ashland Ave & Division St\"                            \n[184] \"Ashland Ave & Grand Ave\"                              \n[185] \"Federal St & Polk St\"                                 \n[186] \"Lake Shore Dr & Belmont Ave\"                          \n[187] \"W Armitage Ave & N Sheffield Ave\"                     \n[188] \"Burling St (Halsted) & Diversey Pkwy (Temp)\"          \n[189] \"Lincoln Ave & Diversey Pkwy\"                          \n[190] \"Winchester Ave & Elston Ave\"                          \n[191] \"Field Blvd & South Water St\"                          \n[192] \"Stetson Ave & South Water St\"                         \n[193] \"Cityfront Plaza Dr & Pioneer Ct\"                      \n[194] \"Wabash Ave & Wacker Pl\"                               \n[195] \"Carpenter St & Huron St\"                              \n[196] \"Kimbark Ave & 53rd St\"                                \n[197] \"Larrabee St & North Ave\"                              \n[198] \"Larrabee St & Division St\"                            \n[199] \"Fairfield Ave & Roosevelt Rd\"                         \n[200] \"Sheridan Rd & Noyes St (NU)\"                          \n[201] \"Chicago Ave & Dempster St\"                            \n[202] \"Hermitage Ave & Polk St\"                              \n[203] \"Central Park Ave & Elbridge Ave\"                      \n[204] \"State St & 33rd St\"                                   \n[205] \"Greenwood Ave & 47th St\"                              \n[206] \"Chicago Ave & Washington St\"                          \n[207] \"Broadway & Ridge Ave\"                                 \n[208] \"Sheffield Ave & Kingsbury St\"                         \n[209] \"Pine Grove Ave & Irving Park Rd\"                      \n[210] \"California Ave & Montrose Ave\"                        \n[211] \"Sheridan Rd & Montrose Ave\"                           \n[212] \"Clark St & Randolph St\"                               \n[213] \"Calumet Ave & 18th St\"                                \n[214] \"Spaulding Ave & Armitage Ave\"                         \n[215] \"Damen Ave & Cortland St\"                              \n[216] \"Wells St & Elm St\"                                    \n[217] \"Winthrop Ave & Lawrence Ave\"                          \n[218] \"Clark St & Lincoln Ave\"                               \n[219] \"Western Ave & Division St\"                            \n[220] \"Millennium Park\"                                      \n[221] \"Sheridan Rd & Irving Park Rd\"                         \n[222] \"Milwaukee Ave & Wabansia Ave\"                         \n[223] \"Greenview Ave & Diversey Pkwy\"                        \n[224] \"Rush St & Superior St\"                                \n[225] \"Desplaines St & Randolph St\"                          \n[226] \"Pine Grove Ave & Waveland Ave\"                        \n[227] \"Emerald Ave & 31st St\"                                \n[228] \"Cannon Dr & Fullerton Ave\"                            \n[229] \"Michigan Ave & Ida B Wells Dr\"                        \n[230] \"Halsted St & 47th Pl\"                                 \n[231] \"LaSalle St & Washington St\"                           \n[232] \"Halsted St & 18th St\"                                 \n[233] \"Southport Ave & Roscoe St\"                            \n[234] \"Ogden Ave & Chicago Ave\"                              \n[235] \"Jefferson St & Monroe St\"                             \n[236] \"Walsh Park\"                                           \n[237] \"California Ave & Francis Pl (Temp)\"                   \n[238] \"Michigan Ave & Oak St\"                                \n[239] \"Sheridan Rd & Columbia Ave\"                           \n[240] \"Burnham Harbor\"                                       \n[241] \"Cottage Grove Ave & Oakwood Blvd\"                     \n[242] \"Western Ave & Leland Ave\"                             \n[243] \"Clark St & Drummond Pl\"                               \n[244] \"Theater on the Lake\"                                  \n[245] \"McCormick Place\"                                      \n[246] \"Lincoln Ave & Waveland Ave\"                           \n[247] \"Monticello Ave & Irving Park Rd\"                      \n[248] \"California Ave & Byron St\"                            \n[249] \"Glenwood Ave & Morse Ave\"                             \n[250] \"Lincoln Ave & Fullerton Ave\"                          \n[251] \"State St & 19th St\"                                   \n[252] \"Damen Ave & Leland Ave\"                               \n[253] \"Albany Ave & Montrose Ave\"                            \n[254] \"Wilton Ave & Belmont Ave\"                             \n[255] \"State St & Pearson St\"                                \n[256] \"Field Museum\"                                         \n[257] \"Wacker Dr & Washington St\"                            \n[258] \"Kedzie Ave & Palmer Ct\"                               \n[259] \"Sheffield Ave & Waveland Ave\"                         \n[260] \"Washtenaw Ave & Lawrence Ave\"                         \n[261] \"Broadway & Belmont Ave\"                               \n[262] \"Sheffield Ave & Wellington Ave\"                       \n[263] \"Halsted St & Willow St\"                               \n[264] \"Dauphin Ave & 103rd St\"                               \n[265] \"Throop (Loomis) St & Taylor St\"                       \n[266] \"Western Ave & Lunt Ave\"                               \n[267] \"Broadway & Barry Ave\"                                 \n[268] \"Damen Ave & Grand Ave\"                                \n[269] \"Ashland Ave & 63rd St\"                                \n[270] \"Avers Ave & Belmont Ave\"                              \n[271] \"California Ave & North Ave\"                           \n[272] \"Damen Ave & Thomas St (Augusta Blvd)\"                 \n[273] \"Clifton Ave & Armitage Ave\"                           \n[274] \"Morgan Ave & 14th Pl\"                                 \n[275] \"Constance Ave & 95th St\"                              \n[276] \"Southport Ave & Clybourn Ave\"                         \n[277] \"Greenview Ave & Fullerton Ave\"                        \n[278] \"Ravenswood Ave & Lawrence Ave\"                        \n[279] \"Bernard St & Elston Ave\"                              \n[280] \"Franklin St & Jackson Blvd\"                           \n[281] \"Sheffield Ave & Webster Ave\"                          \n[282] \"South Shore Dr & 67th St\"                             \n[283] \"Vernon Ave & 75th St\"                                 \n[284] \"Ashland Ave & Grace St\"                               \n[285] \"Kedzie Ave & Foster Ave\"                              \n[286] \"State St & Randolph St\"                               \n[287] \"Harper Ave & 59th St\"                                 \n[288] \"Damen Ave & Clybourn Ave\"                             \n[289] \"Sheridan Rd & Greenleaf Ave\"                          \n[290] \"Lincoln Ave & Roscoe St\"                              \n[291] \"State St & Harrison St\"                               \n[292] \"Dusable Harbor\"                                       \n[293] \"Cottage Grove Ave & 43rd St\"                          \n[294] \"Broadway & Wilson Ave\"                                \n[295] \"Christiana Ave & Lawrence Ave\"                        \n[296] \"Clark St & Leland Ave\"                                \n[297] \"Daley Center Plaza\"                                   \n[298] \"St. Clair St & Erie St\"                               \n[299] \"Union Ave & Root St\"                                  \n[300] \"State St & 29th St\"                                   \n[301] \"Paulina St & Montrose Ave\"                            \n[302] \"Halsted St & Roscoe St\"                               \n[303] \"Clark St & Wellington Ave\"                            \n[304] \"Broadway & Argyle St\"                                 \n[305] \"Michigan Ave & Pearson St\"                            \n[306] \"Southport Ave & Belmont Ave\"                          \n[307] \"Blue Island Ave & 18th St\"                            \n[308] \"Franklin St & Chicago Ave\"                            \n[309] \"Southport Ave & Irving Park Rd\"                       \n[310] \"Leavitt St & Armitage Ave\"                            \n[311] \"Clark St & 9th St (AMLI)\"                             \n[312] \"Wentworth Ave & 24th St (Temp)\"                       \n[313] \"Orleans St & Elm St\"                                  \n[314] \"Dodge Ave & Church St\"                                \n[315] \"Budlong Woods Library\"                                \n[316] \"Wabash Ave & Adams St\"                                \n[317] \"Halsted St & 35th St\"                                 \n[318] \"Stockton Dr & Wrightwood Ave\"                         \n[319] \"Artesian Ave & Hubbard St\"                            \n[320] \"Racine Ave & 13th St\"                                 \n[321] \"State St & 123rd St\"                                  \n[322] \"Talman Ave & Addison St\"                              \n[323] \"Albany Ave & 26th St\"                                 \n[324] \"Damen Ave & Cullerton St\"                             \n[325] \"Eberhart Ave & 131st St\"                              \n[326] \"Clark St & Wrightwood Ave\"                            \n[327] \"Bennett Ave & 79th St\"                                \n[328] \"Dorchester Ave & 63rd St\"                             \n[329] \"State St & 35th St\"                                   \n[330] \"Leavitt St & North Ave\"                               \n[331] \"Damen Ave & Charleston St\"                            \n[332] \"Wolcott (Ravenswood) Ave & Montrose Ave\"              \n[333] \"Western Ave & Congress Pkwy\"                          \n[334] \"Racine Ave & Randolph St\"                             \n[335] \"Lincoln Ave & Belmont Ave\"                            \n[336] \"Rush St & Hubbard St\"                                 \n[337] \"Drake Ave & Montrose Ave\"                             \n[338] \"Damen Ave & Sunnyside Ave\"                            \n[339] \"Big Marsh Park\"                                       \n[340] \"Sedgwick St & North Ave\"                              \n[341] \"Wallace St & 35th St\"                                 \n[342] \"Leavitt St & Archer Ave\"                              \n[343] \"Marine Dr & Ainslie St\"                               \n[344] \"Halsted St & 37th St\"                                 \n[345] \"Central Park Ave & Ogden Ave\"                         \n[346] \"Clinton St & Tilden St\"                               \n[347] \"Warren Park West\"                                     \n[348] \"Lake Park Ave & 47th St\"                              \n[349] \"Clark St & Chicago Ave\"                               \n[350] \"Avondale Ave & Irving Park Rd\"                        \n[351] \"Wabash Ave & 16th St\"                                 \n[352] \"Western Ave & Fillmore St\"                            \n[353] \"Kilbourn Ave & Milwaukee Ave\"                         \n[354] \"Wood St & Hubbard St\"                                 \n[355] \"California Ave & Division St\"                         \n[356] \"Cornell Ave & Hyde Park Blvd\"                         \n[357] \"Ellis Ave & 55th St\"                                  \n[358] \"California Ave & Altgeld St\"                          \n[359] \"Benson Ave & Church St\"                               \n[360] \"May St & Taylor St\"                                   \n[361] \"Delano Ct & Roosevelt Rd\"                             \n[362] \"Morgan St & 18th St\"                                  \n[363] \"Wolcott Ave & Polk St\"                                \n[364] \"Jeffery Blvd & 71st St\"                               \n[365] \"Sedgwick St & Webster Ave\"                            \n[366] \"Lakefront Trail & Wilson Ave\"                         \n[367] \"Canal St & Jackson Blvd\"                              \n[368] \"Glenwood Ave & Touhy Ave\"                             \n[369] \"2112 W Peterson Ave\"                                  \n[370] \"Loomis St & Archer Ave\"                               \n[371] \"Phillips Ave & 79th St\"                               \n[372] \"Pulaski Rd & Lake St\"                                 \n[373] \"Clarendon Ave & Gordon Ter\"                           \n[374] \"Central Ave & Chicago Ave\"                            \n[375] \"LaSalle St & Adams St\"                                \n[376] \"Rhodes Ave & 32nd St\"                                 \n[377] \"Western Ave & Roscoe St\"                              \n[378] \"Knox Ave & Montrose Ave\"                              \n[379] \"LaSalle St & Jackson Blvd\"                            \n[380] \"Dearborn St & Adams St\"                               \n[381] \"Broadway & Sheridan Rd\"                               \n[382] \"Calumet Ave & 21st St\"                                \n[383] \"Marshfield Ave & Cortland St\"                         \n[384] \"Wells St & 19th St\"                                   \n[385] \"Clark St & Lunt Ave\"                                  \n[386] \"Western Ave & Monroe St\"                              \n[387] \"Milwaukee Ave & Rockwell St\"                          \n[388] \"Stony Island Ave & 90th St\"                           \n[389] \"Cottage Grove Ave & 111th Pl\"                         \n[390] \"Campbell Ave & North Ave\"                             \n[391] \"Pulaski Rd & Congress Pkwy\"                           \n[392] \"Elizabeth (May) St & Fulton St\"                       \n[393] \"Halsted St & Archer Ave\"                              \n[394] \"Eckhart Park\"                                         \n[395] \"Dauphin Ave & 87th St\"                                \n[396] \"Prairie Ave & 43rd St\"                                \n[397] \"Woodlawn Ave & 55th St\"                               \n[398] \"Western Ave & Walton St\"                              \n[399] \"Leavitt St & Lawrence Ave\"                            \n[400] \"Racine Ave & 35th St\"                                 \n[401] \"Wabash Ave & Cermak Rd\"                               \n[402] \"California Ave & 23rd Pl\"                             \n[403] \"Stave St & Armitage Ave\"                              \n[404] \"Sheffield Ave & Fullerton Ave\"                        \n[405] \"California Ave & Fletcher St\"                         \n[406] \"Laflin St & Cullerton St\"                             \n[407] \"Shields Ave & 28th Pl\"                                \n[408] \"Racine Ave & Congress Pkwy\"                           \n[409] \"Princeton Ave & Garfield Blvd\"                        \n[410] \"State St & Van Buren St\"                              \n[411] \"Clinton St & Washington Blvd\"                         \n[412] \"Damen Ave & Coulter St\"                               \n[413] \"Wood St & Augusta Blvd\"                               \n[414] \"Fairbanks St & Superior St\"                           \n[415] \"Pulaski Rd & Eddy St (Temp)\"                          \n[416] \"Clark St & Montrose Ave\"                              \n[417] \"Loomis St & Jackson Blvd\"                             \n[418] \"Western Ave & 24th St\"                                \n[419] \"Canal St & Madison St\"                                \n[420] \"Damen Ave & Pershing Rd\"                              \n[421] \"California Ave & Lake St\"                             \n[422] \"Jeffery Blvd & 67th St\"                               \n[423] \"Ravenswood Ave & Irving Park Rd\"                      \n[424] \"Spaulding Ave & Division St\"                          \n[425] \"South Shore Dr & 71st St\"                             \n[426] \"Morgan St & Polk St\"                                  \n[427] \"Lincoln Ave & Belle Plaine Ave\"                       \n[428] \"Leavitt St & Chicago Ave\"                             \n[429] \"State St & 95th St\"                                   \n[430] \"Elmwood Ave & Austin St\"                              \n[431] \"Oglesby Ave & 100th St\"                               \n[432] \"Dodge Ave & Mulford St\"                               \n[433] \"St. Louis Ave & Balmoral Ave\"                         \n[434] \"Ellis Ave & 53rd St\"                                  \n[435] \"Halsted St & 111th St\"                                \n[436] \"Lake Park Ave & 56th St\"                              \n[437] \"MLK Jr Dr & 56th St\"                                  \n[438] \"Halsted St & North Branch St\"                         \n[439] \"Major Taylor Trail & 115th St\"                        \n[440] \"Racine Ave & Wrightwood Ave\"                          \n[441] \"Calumet Ave & 51st St\"                                \n[442] \"Wood St & Taylor St\"                                  \n[443] \"Racine Ave & 15th St\"                                 \n[444] \"900 W Harrison St\"                                    \n[445] \"Bosworth Ave & Howard St\"                             \n[446] \"Ashland Ave & Augusta Blvd\"                           \n[447] \"Aberdeen St & Monroe St\"                              \n[448] \"Western Ave & Granville Ave\"                          \n[449] \"Greenwood Ave & 91st St\"                              \n[450] \"Damen Ave & Foster Ave\"                               \n[451] \"Michigan Ave & 71st St\"                               \n[452] \"Prairie Ave & Garfield Blvd\"                          \n[453] \"Stony Island Ave & 71st St\"                           \n[454] \"Southport Ave & Clark St\"                             \n[455] \"Damen Ave & 51st St\"                                  \n[456] \"Canal St & Monroe St\"                                 \n[457] \"Smith Park\"                                           \n[458] \"Manor Ave & Leland Ave\"                               \n[459] \"MLK Jr Dr & Pershing Rd\"                              \n[460] \"Canal St & Taylor St\"                                 \n[461] \"Ridge Blvd & Touhy Ave\"                               \n[462] \"California Ave & Cortez St\"                           \n[463] \"Central St & Girard Ave\"                              \n[464] \"Central Park Ave & Bloomingdale Ave\"                  \n[465] \"Jeffery Blvd & 91st St\"                               \n[466] \"Ashland Ave & 13th St\"                                \n[467] \"Baltimore Ave & 87th St\"                              \n[468] \"WATSON TESTING - DIVVY\"                               \n[469] \"Halsted St & 96th St\"                                 \n[470] \"Clark St & Columbia Ave\"                              \n[471] \"Blackstone Ave & Hyde Park Blvd\"                      \n[472] \"Damen Ave & Walnut (Lake) St\"                         \n[473] \"Calumet Ave & 33rd St\"                                \n[474] \"Kildare Ave & Montrose Ave\"                           \n[475] \"Paulina St & Flournoy St\"                             \n[476] \"Kimball Ave & Belmont Ave\"                            \n[477] \"Cottage Grove Ave & 51st St\"                          \n[478] \"Campbell Ave & Montrose Ave\"                          \n[479] \"Wood St & Taylor St (Temp)\"                           \n[480] \"Paulina St & 18th St\"                                 \n[481] \"Troy St & Elston Ave\"                                 \n[482] \"Halsted St & 104th St\"                                \n[483] \"Evanston Civic Center\"                                \n[484] \"Shields Ave & 43rd St\"                                \n[485] \"Indiana Ave & 103rd St\"                               \n[486] \"Kedzie Ave & Lake St\"                                 \n[487] \"Kedzie Ave & Chicago Ave\"                             \n[488] \"Halsted St & Maxwell St\"                              \n[489] \"Kostner Ave & Lake St\"                                \n[490] \"Keystone Ave & Montrose Ave\"                          \n[491] \"California Ave & 21st St\"                             \n[492] \"Morgan St & 31st St\"                                  \n[493] \"Keystone Ave & Fullerton Ave\"                         \n[494] \"Kedzie Ave & Bryn Mawr Ave\"                           \n[495] \"Kedzie Ave & 24th St\"                                 \n[496] \"Elizabeth St & 47th St\"                               \n[497] \"MLK Jr Dr & 47th St\"                                  \n[498] \"Austin Blvd & Chicago Ave\"                            \n[499] \"Throop St & Taylor St\"                                \n[500] \"Ashland Ave & Lake St\"                                \n[501] \"Ravenswood Ave & Berteau Ave\"                         \n[502] \"Cherry Ave & Blackhawk St\"                            \n[503] \"Clark St & Ida B Wells Dr\"                            \n[504] \"Lincoln Ave & Addison St\"                             \n[505] \"Western Ave & Howard St\"                              \n[506] \"Claremont Ave & Hirsch St\"                            \n[507] \"Canal St & Harrison St\"                               \n[508] \"Hoyne Ave & 47th St\"                                  \n[509] \"MLK Jr Dr & 83rd St\"                                  \n[510] \"Ashland Ave & 69th St\"                                \n[511] \"Ashland Ave & Archer Ave\"                             \n[512] \"Rockwell St & Eastwood Ave\"                           \n[513] \"Leavitt St & Division St\"                             \n[514] \"Laramie Ave & Madison St\"                             \n[515] \"Hoyne Ave & Balmoral Ave\"                             \n[516] \"Racine Ave & Garfield Blvd\"                           \n[517] \"Woodlawn Ave & 75th St\"                               \n[518] \"State St & Pershing Rd\"                               \n[519] \"Western Ave & 21st St\"                                \n[520] \"Central Ave & Lake St\"                                \n[521] \"University Library (NU)\"                              \n[522] \"Woodlawn Ave & Lake Park Ave\"                         \n[523] \"Troy St & North Ave\"                                  \n[524] \"DuSable Museum\"                                       \n[525] \"Ogden Ave & Roosevelt Rd\"                             \n[526] \"Elizabeth St & 92nd St\"                               \n[527] \"Sangamon St & Washington Blvd\"                        \n[528] \"Clinton St & 18th St\"                                 \n[529] \"Stewart Ave & 63rd St\"                                \n[530] \"Kedzie Ave & Leland Ave\"                              \n[531] \"Millard Ave & 26th St\"                                \n[532] \"Wentworth Ave & 63rd St\"                              \n[533] \"Prospect Sq & 91st St\"                                \n[534] \"Latrobe Ave & Chicago Ave\"                            \n[535] \"Valli Produce - Evanston Plaza\"                       \n[536] \"Kedzie Ave & Harrison St\"                             \n[537] \"Wentworth Ave & 35th St\"                              \n[538] \"Central Park Ave & North Ave\"                         \n[539] \"Ashland Ave & Pershing Rd\"                            \n[540] \"Central Park Blvd & 5th Ave\"                          \n[541] \"Conservatory Dr & Lake St\"                            \n[542] \"Austin Blvd & Madison St\"                             \n[543] \"Halsted St & 21st St\"                                 \n[544] \"Stony Island Ave & 64th St\"                           \n[545] \"Ogden Ave & Congress Pkwy\"                            \n[546] \"Kilbourn Ave & Irving Park Rd\"                        \n[547] \"Oakley Ave & Irving Park Rd\"                          \n[548] \"Cottage Grove Ave & 47th St\"                          \n[549] \"Central St Metra\"                                     \n[550] \"Lincolnwood Dr & Central St\"                          \n[551] \"South Shore Dr & 74th St\"                             \n[552] \"Clinton St & Polk St\"                                 \n[553] \"Indiana Ave & 40th St\"                                \n[554] \"Wolcott Ave & Fargo Ave\"                              \n[555] \"Vincennes Ave & 75th St\"                              \n[556] \"Warren Park East\"                                     \n[557] \"Washtenaw Ave & Ogden Ave\"                            \n[558] \"Wentworth Ave & 33rd St\"                              \n[559] \"Western Blvd & 48th Pl\"                               \n[560] \"Cottage Grove Ave & 67th St\"                          \n[561] \"Winchester (Ravenswood) Ave & Balmoral Ave\"           \n[562] \"Emerald Ave & 28th St\"                                \n[563] \"Greenwood Ave & 97th St\"                              \n[564] \"Vernon Ave & 107th St\"                                \n[565] \"Ashland Ave & 78th St\"                                \n[566] \"Wood St & 35th St\"                                    \n[567] \"Halsted St & Roosevelt Rd\"                            \n[568] \"Greenwood Ave & 79th St\"                              \n[569] \"Shields Ave & 31st St\"                                \n[570] \"East End Ave & 87th St\"                               \n[571] \"Calumet Ave & 35th St\"                                \n[572] \"Drake Ave & Addison St\"                               \n[573] \"Wabash Ave & 87th St\"                                 \n[574] \"Stony Island Ave & 82nd St\"                           \n[575] \"State St & 54th St\"                                   \n[576] \"Cottage Grove Ave & 78th St\"                          \n[577] \"Oakley Ave & Touhy Ave\"                               \n[578] \"Marshfield Ave & 44th St\"                             \n[579] \"Racine Ave & 61st St\"                                 \n[580] \"Avenue O & 134th St\"                                  \n[581] \"Malcolm X College\"                                    \n[582] \"Cornell Dr & Hayes Dr\"                                \n[583] \"Cicero Ave & Lake St\"                                 \n[584] \"Ashland Ave & McDowell Ave\"                           \n[585] \"Torrence Ave & 106th St\"                              \n[586] \"Eggleston Ave & 92nd St\"                              \n[587] \"Calumet Ave & 71st St\"                                \n[588] \"Normal Ave & Archer Ave\"                              \n[589] \"Karlov Ave & Madison St\"                              \n[590] \"State St & 79th St\"                                   \n[591] \"Ashland Ave & 50th St\"                                \n[592] \"Maplewood Ave & Peterson Ave\"                         \n[593] \"Clark St & Touhy Ave\"                                 \n[594] \"Yates Blvd & 75th St\"                                 \n[595] \"Stewart Ave & 83rd St\"                                \n[596] \"Marquette Ave & 89th St\"                              \n[597] \"Central Park Ave & 24th St\"                           \n[598] \"Evans Ave & 75th St\"                                  \n[599] \"Sacramento Blvd & Franklin Blvd\"                      \n[600] \"Halsted St & 63rd St\"                                 \n[601] \"63rd St Beach\"                                        \n[602] \"Kostner Ave & Adams St\"                               \n[603] \"Ashland Ave & 66th St\"                                \n[604] \"State St & 76th St\"                                   \n[605] \"Summit Ave & 86th St\"                                 \n[606] \"Stony Island Ave & 67th St\"                           \n[607] \"Vernon Ave & 79th St\"                                 \n[608] \"Laramie Ave & Kinzie St\"                              \n[609] \"Central Ave & Madison St\"                             \n[610] \"Stony Island Ave & South Chicago Ave\"                 \n[611] \"Cottage Grove Ave & 63rd St\"                          \n[612] \"Halsted St & 69th St\"                                 \n[613] \"Kedzie Ave & 21st St\"                                 \n[614] \"Halsted St & 78th St\"                                 \n[615] \"Houston Ave & 92nd St\"                                \n[616] \"Michigan Ave & 114th St\"                              \n[617] \"Eberhart Ave & 91st St\"                               \n[618] \"Rainbow Beach\"                                        \n[619] \"Throop St & 52nd St\"                                  \n[620] \"MLK Jr Dr & 63rd St\"                                  \n[621] \"Stony Island Ave & 75th St\"                           \n[622] \"Exchange Ave & 79th St\"                               \n[623] \"Marshfield Ave & 59th St\"                             \n[624] \"Princeton Ave & 47th St\"                              \n[625] \"Perry Ave & 69th St\"                                  \n[626] \"Racine Ave & 65th St\"                                 \n[627] \"Morgan St & Pershing Rd\"                              \n[628] \"Commercial Ave & 83rd St\"                             \n[629] \"Halsted St & 51st St\"                                 \n[630] \"Ewing Ave & Burnham Greenway\"                         \n[631] \"Phillips Ave & 83rd St\"                               \n[632] \"Clyde Ave & 87th St\"                                  \n[633] \"May St & 69th St\"                                     \n[634] \"Damen Ave & 59th St\"                                  \n[635] \"Halsted St & 56th St\"                                 \n[636] \"Ashland Ave & Garfield Blvd\"                          \n[637] \"Western Ave & 28th St\"                                \n[638] \"Jeffery Blvd & 76th St\"                               \n[639] \"Loomis Blvd & 84th St\"                                \n[640] \"Central Ave & Harrison St\"                            \n[641] \"Cicero Ave & Flournoy St\"                             \n[642] \"Halsted St & 73rd St\"                                 \n[643] \"Cicero Ave & Quincy St\"                               \n[644] \"Laramie Ave & Gladys Ave\"                             \n[645] \"Ellis Ave & 83rd St\"                                  \n[646] \"Kenton Ave & Madison St\"                              \n[647] \"California Ave & 26th St\"                             \n[648] \"South Chicago Ave & Elliot Ave\"                       \n[649] \"Halsted St & 59th St\"                                 \n[650] \"Wabash Ave & 83rd St\"                                 \n[651] \"Ashland Ave & 74th St\"                                \n[652] \"Torrence Ave & 126th Pl\"                              \n[653] \"Seeley Ave & Garfield Blvd\"                           \n[654] \"Eggleston Ave & 69th St\"                              \n[655] \"Rhodes Ave & 71st St\"                                 \n[656] \"Carpenter St & 63rd St\"                               \n[657] \"Cottage Grove Ave & 71st St\"                          \n[658] \"South Chicago Ave & 83rd St\"                          \n[659] \"HUBBARD ST BIKE CHECKING (LBS-WH-TEST)\"               \n[660] \"Walden Pkwy & 100th St\"                               \n[661] \"Burnham Greenway & 105th St\"                          \n[662] \"Elizabeth St & 59th St\"                               \n[663] \"Major Taylor Trail & 124th St\"                        \n[664] \"Avenue O & 118th St\"                                  \n[665] \"N Clark St & W Elm St\"                                \n[666] \"St. Louis Ave & Fullerton Ave\"                        \n[667] \"New St & Illinois St\"                                 \n[668] \"Hale Ave & 107th St\"                                  \n[669] \"Bradley Park\"                                         \n[670] \"Commercial Ave & 130th St\"                            \n[671] \"Vincennes Ave & 104th St\"                             \n[672] \"Western Ave & 111th St\"                               \n[673] \"Ada St & 113th St\"                                    \n[674] \"Lawndale Ave & 111th St\"                              \n[675] \"Homewood Ave & 115th St\"                              \n[676] \"Western Ave & 104th St\"                               \n[677] \"Hegewisch Metra Station\"                              \n[678] \"S Michigan Ave & E 118th St\"                          \n[679] \"Dodge Ave & Main St\"                                  \n[680] \"Base - 2132 W Hubbard Warehouse\"                      \n[681] \"N Green St & W Lake St\"                               \n[682] \"Commercial Ave & 100th St\"                            \n[683] \"N Paulina St & Lincoln Ave\"                           \n[684] \"N Southport Ave & W Newport Ave\"                      \n[685] \"Malcolm X College Vaccination Site\"                   \n[686] \"Broadway & Wilson - Truman College Vaccination Site\"  \n[687] \"W Washington Blvd & N Peoria St\"                      \n[688] \"Kedzie Ave & 110th St\"                                \n[689] \"N Sheffield Ave & W Wellington Ave\"                   \n[690] \"N Carpenter St & W Lake St\"                           \n[691] \"Halsted & 63rd - Kennedy-King Vaccination Site\"       \n[692] \"Western & 28th - Velasquez Institute Vaccination Site\"\n[693] \"Chicago State University\"                             \n[694] \"Damen Ave & Wabansia Ave\"                             \n[695] \"N Damen Ave & W Wabansia St\"                          \n[696] \"S Wentworth Ave & W 111th St\"                         \n[697] \"N Hampden Ct & W Diversey Ave\"                        \n[698] \"Altgeld Gardens\"                                      \n[699] \"Hampden Ct & Diversey Ave\"                            \n[700] \"Kedzie Ave & 104th St\"                                \n[701] \"Avenue L & 114th St\"                                  \n[702] \"Calumet Park\"                                         \n[703] \"Elston Ave & Cortland St\"                             \n[704] \"Loomis St & 89th St\"                                  \n[705] \"Woodlawn & 103rd - Olive Harvey Vaccination Site\"     \n[706] \"W 103rd St & S Avers Ave\"                             \n[707] \"WEST CHI-WATSON\"                                      \n[708] \"Yates Blvd & 93rd St\"                                 \n[709] \"Maryland Ave & 104th St\"                              \n[710] \"S Aberdeen St & W 106th St\"                           \n[711] \"Lyft Driver Center Private Rack\"                      \n[712] \"Cicero Ave & Grace St\"                                \n[713] \"DuSable Lake Shore Dr & Belmont Ave\"                  \n[714] \"DuSable Lake Shore Dr & Monroe St\"                    \n[715] \"DuSable Lake Shore Dr & North Blvd\"                   \n[716] \"DuSable Lake Shore Dr & Ohio St\"                      \n[717] \"DuSable Lake Shore Dr & Wellington Ave\"               \n[718] \"DuSable Lake Shore Dr & Diversey Pkwy\"                \n[719] \"Kildare Ave & 26th St\"                                \n[720] \"Tripp Ave & 31st St\"                                  \n[721] \"Halsted St & 18th St (Temp)\"                          \n[722] \"Lawndale Ave & 16th St\"                               \n[723] \"Spaulding Ave & 16th St\"                              \n[724] \"Homan Ave & Fillmore St\"                              \n[725] \"Lavergne & Fullerton\"                                 \n[726] \"DIVVY CASSETTE REPAIR MOBILE STATION\"                 \n[727] \"Kilpatrick Ave & Parker Ave\"                          \n[728] \"Central Park Ave & Douglas Blvd\"                      \n[729] \"Lamon Ave & Belmont Ave\"                              \n[730] \"Long Ave & Belmont Ave\"                               \n[731] \"Meade Ave & Diversey Ave\"                             \n[732] \"Long Ave & Belden Ave\"                                \n[733] \"Tripp Ave & 15th St\"                                  \n[734] \"Mason Ave & Belmont Ave\"                              \n[735] \"Parkside Ave & Armitage Ave\"                          \n[736] \"Keeler Ave & Roosevelt Rd\"                            \n[737] \"Pulaski Rd & 21st St\"                                 \n[738] \"Central Ave & Parker Ave\"                             \n[739] \"Lawndale Ave & 30th St\"                               \n[740] \"Meade Ave & Addison St\"                               \n[741] \"Kilbourn & Roscoe\"                                    \n[742] \"Mulligan Ave & Wellington Ave\"                        \n[743] \"Lockwood Ave & Wrightwood Ave\"                        \n[744] \"Plainfield & Irving Park\"                             \n[745] \"Narragansett Ave & School St\"                         \n[746] \"Central Ave & Roscoe St\"                              \n[747] \"351\"                                                  \n[748] \"Long & Irving Park\"                                   \n[749] \"Komensky Ave & 31st St\"                               \n[750] \"Kilbourn & Belden\"                                    \n[751] \"Kostner Ave & Wrightwood Ave\"                         \n[752] \"North Ave & New England Ave\"                          \n[753] \"Harding Ave & 26th St\"                                \n[754] \"Harlem Ave & Grace St\"                                \n[755] \"Oketo Ave & Addison St\"                               \n[756] \"Roscoe & Harlem\"                                      \n[757] \"Lamon Ave & Armitage Ave\"                             \n[758] \"Sayre & Diversey\"                                     \n\n\n\nCreating a vector of station names identified as Divvy test and repair stations.\n\ntrs_name <- c(\"Base – 2132 W Hubbard Warehouse\",\n              \"DIVVY CASSETTE REPAIR MOBILE STATION\",\n              \"HUBBARD ST BIKE CHECKING (LBS-WH-TEST)\",\n              \"WEST CHI-WATSON\")\n\nLooking for specific values in the start_station_id column associated with Divvy test and repair stations.\n\n\nUnique start station IDs\n\n\nunique(all_trips_cln$start_station_id)\n\n   [1] \"52\"                                  \n   [2] NA                                    \n   [3] \"246\"                                 \n   [4] \"24\"                                  \n   [5] \"94\"                                  \n   [6] \"291\"                                 \n   [7] \"273\"                                 \n   [8] \"145\"                                 \n   [9] \"108\"                                 \n  [10] \"511\"                                 \n  [11] \"329\"                                 \n  [12] \"315\"                                 \n  [13] \"138\"                                 \n  [14] \"504\"                                 \n  [15] \"38\"                                  \n  [16] \"485\"                                 \n  [17] \"89\"                                  \n  [18] \"53\"                                  \n  [19] \"3\"                                   \n  [20] \"35\"                                  \n  [21] \"113\"                                 \n  [22] \"164\"                                 \n  [23] \"111\"                                 \n  [24] \"140\"                                 \n  [25] \"225\"                                 \n  [26] \"301\"                                 \n  [27] \"43\"                                  \n  [28] \"21\"                                  \n  [29] \"13\"                                  \n  [30] \"212\"                                 \n  [31] \"645\"                                 \n  [32] \"287\"                                 \n  [33] \"49\"                                  \n  [34] \"71\"                                  \n  [35] \"454\"                                 \n  [36] \"195\"                                 \n  [37] \"123\"                                 \n  [38] \"589\"                                 \n  [39] \"289\"                                 \n  [40] \"250\"                                 \n  [41] \"306\"                                 \n  [42] \"585\"                                 \n  [43] \"627\"                                 \n  [44] \"168\"                                 \n  [45] \"130\"                                 \n  [46] \"424\"                                 \n  [47] \"175\"                                 \n  [48] \"126\"                                 \n  [49] \"84\"                                  \n  [50] \"712\"                                 \n  [51] \"172\"                                 \n  [52] \"673\"                                 \n  [53] \"157\"                                 \n  [54] \"110\"                                 \n  [55] \"341\"                                 \n  [56] \"492\"                                 \n  [57] \"215\"                                 \n  [58] \"93\"                                  \n  [59] \"26\"                                  \n  [60] \"255\"                                 \n  [61] \"636\"                                 \n  [62] \"308\"                                 \n  [63] \"165\"                                 \n  [64] \"199\"                                 \n  [65] \"453\"                                 \n  [66] \"16\"                                  \n  [67] \"227\"                                 \n  [68] \"120\"                                 \n  [69] \"2\"                                   \n  [70] \"150\"                                 \n  [71] \"471\"                                 \n  [72] \"451\"                                 \n  [73] \"77\"                                  \n  [74] \"331\"                                 \n  [75] \"349\"                                 \n  [76] \"134\"                                 \n  [77] \"426\"                                 \n  [78] \"197\"                                 \n  [79] \"459\"                                 \n  [80] \"268\"                                 \n  [81] \"460\"                                 \n  [82] \"423\"                                 \n  [83] \"333\"                                 \n  [84] \"288\"                                 \n  [85] \"69\"                                  \n  [86] \"228\"                                 \n  [87] \"226\"                                 \n  [88] \"672\"                                 \n  [89] \"458\"                                 \n  [90] \"350\"                                 \n  [91] \"17\"                                  \n  [92] \"190\"                                 \n  [93] \"325\"                                 \n  [94] \"523\"                                 \n  [95] \"245\"                                 \n  [96] \"76\"                                  \n  [97] \"260\"                                 \n  [98] \"623\"                                 \n  [99] \"638\"                                 \n [100] \"243\"                                 \n [101] \"419\"                                 \n [102] \"56\"                                  \n [103] \"406\"                                 \n [104] \"133\"                                 \n [105] \"60\"                                  \n [106] \"313\"                                 \n [107] \"515\"                                 \n [108] \"251\"                                 \n [109] \"603\"                                 \n [110] \"171\"                                 \n [111] \"61\"                                  \n [112] \"198\"                                 \n [113] \"116\"                                 \n [114] \"303\"                                 \n [115] \"100\"                                 \n [116] \"142\"                                 \n [117] \"416\"                                 \n [118] \"302\"                                 \n [119] \"247\"                                 \n [120] \"48\"                                  \n [121] \"162\"                                 \n [122] \"637\"                                 \n [123] \"107\"                                 \n [124] \"181\"                                 \n [125] \"47\"                                  \n [126] \"501\"                                 \n [127] \"236\"                                 \n [128] \"153\"                                 \n [129] \"431\"                                 \n [130] \"237\"                                 \n [131] \"147\"                                 \n [132] \"507\"                                 \n [133] \"128\"                                 \n [134] \"332\"                                 \n [135] \"286\"                                 \n [136] \"520\"                                 \n [137] \"173\"                                 \n [138] \"15\"                                  \n [139] \"435\"                                 \n [140] \"514\"                                 \n [141] \"192\"                                 \n [142] \"99\"                                  \n [143] \"180\"                                 \n [144] \"66\"                                  \n [145] \"29\"                                  \n [146] \"258\"                                 \n [147] \"532\"                                 \n [148] \"59\"                                  \n [149] \"320\"                                 \n [150] \"321\"                                 \n [151] \"364\"                                 \n [152] \"46\"                                  \n [153] \"166\"                                 \n [154] \"620\"                                 \n [155] \"517\"                                 \n [156] \"28\"                                  \n [157] \"304\"                                 \n [158] \"457\"                                 \n [159] \"654\"                                 \n [160] \"328\"                                 \n [161] \"87\"                                  \n [162] \"503\"                                 \n [163] \"272\"                                 \n [164] \"294\"                                 \n [165] \"74\"                                  \n [166] \"624\"                                 \n [167] \"632\"                                 \n [168] \"186\"                                 \n [169] \"57\"                                  \n [170] \"323\"                                 \n [171] \"144\"                                 \n [172] \"112\"                                 \n [173] \"664\"                                 \n [174] \"499\"                                 \n [175] \"284\"                                 \n [176] \"249\"                                 \n [177] \"472\"                                 \n [178] \"621\"                                 \n [179] \"463\"                                 \n [180] \"346\"                                 \n [181] \"176\"                                 \n [182] \"210\"                                 \n [183] \"277\"                                 \n [184] \"41\"                                  \n [185] \"334\"                                 \n [186] \"152\"                                 \n [187] \"505\"                                 \n [188] \"7\"                                   \n [189] \"264\"                                 \n [190] \"196\"                                 \n [191] \"194\"                                 \n [192] \"92\"                                  \n [193] \"322\"                                 \n [194] \"27\"                                  \n [195] \"359\"                                 \n [196] \"436\"                                 \n [197] \"604\"                                 \n [198] \"625\"                                 \n [199] \"261\"                                 \n [200] \"500\"                                 \n [201] \"148\"                                 \n [202] \"252\"                                 \n [203] \"597\"                                 \n [204] \"461\"                                 \n [205] \"20\"                                  \n [206] \"254\"                                 \n [207] \"481\"                                 \n [208] \"231\"                                 \n [209] \"51\"                                  \n [210] \"338\"                                 \n [211] \"506\"                                 \n [212] \"219\"                                 \n [213] \"182\"                                 \n [214] \"253\"                                 \n [215] \"141\"                                 \n [216] \"305\"                                 \n [217] \"90\"                                  \n [218] \"240\"                                 \n [219] \"158\"                                 \n [220] \"319\"                                 \n [221] \"161\"                                 \n [222] \"96\"                                  \n [223] \"232\"                                 \n [224] \"339\"                                 \n [225] \"34\"                                  \n [226] \"45\"                                  \n [227] \"411\"                                 \n [228] \"98\"                                  \n [229] \"202\"                                 \n [230] \"229\"                                 \n [231] \"54\"                                  \n [232] \"73\"                                  \n [233] \"628\"                                 \n [234] \"259\"                                 \n [235] \"85\"                                  \n [236] \"660\"                                 \n [237] \"4\"                                   \n [238] \"265\"                                 \n [239] \"239\"                                 \n [240] \"220\"                                 \n [241] \"177\"                                 \n [242] \"62\"                                  \n [243] \"257\"                                 \n [244] \"484\"                                 \n [245] \"487\"                                 \n [246] \"447\"                                 \n [247] \"127\"                                 \n [248] \"178\"                                 \n [249] \"242\"                                 \n [250] \"480\"                                 \n [251] \"117\"                                 \n [252] \"106\"                                 \n [253] \"97\"                                  \n [254] \"18\"                                  \n [255] \"290\"                                 \n [256] \"114\"                                 \n [257] \"475\"                                 \n [258] \"296\"                                 \n [259] \"115\"                                 \n [260] \"224\"                                 \n [261] \"695\"                                 \n [262] \"19\"                                  \n [263] \"467\"                                 \n [264] \"300\"                                 \n [265] \"214\"                                 \n [266] \"563\"                                 \n [267] \"496\"                                 \n [268] \"276\"                                 \n [269] \"183\"                                 \n [270] \"223\"                                 \n [271] \"137\"                                 \n [272] \"701\"                                 \n [273] \"307\"                                 \n [274] \"188\"                                 \n [275] \"344\"                                 \n [276] \"640\"                                 \n [277] \"36\"                                  \n [278] \"327\"                                 \n [279] \"355\"                                 \n [280] \"571\"                                 \n [281] \"347\"                                 \n [282] \"470\"                                 \n [283] \"44\"                                  \n [284] \"425\"                                 \n [285] \"163\"                                 \n [286] \"354\"                                 \n [287] \"230\"                                 \n [288] \"5\"                                   \n [289] \"6\"                                   \n [290] \"271\"                                 \n [291] \"293\"                                 \n [292] \"474\"                                 \n [293] \"326\"                                 \n [294] \"81\"                                  \n [295] \"211\"                                 \n [296] \"408\"                                 \n [297] \"193\"                                 \n [298] \"297\"                                 \n [299] \"299\"                                 \n [300] \"156\"                                 \n [301] \"295\"                                 \n [302] \"25\"                                  \n [303] \"154\"                                 \n [304] \"129\"                                 \n [305] \"31\"                                  \n [306] \"318\"                                 \n [307] \"309\"                                 \n [308] \"394\"                                 \n [309] \"132\"                                 \n [310] \"23\"                                  \n [311] \"600\"                                 \n [312] \"468\"                                 \n [313] \"39\"                                  \n [314] \"279\"                                 \n [315] \"324\"                                 \n [316] \"376\"                                 \n [317] \"136\"                                 \n [318] \"714\"                                 \n [319] \"491\"                                 \n [320] \"444\"                                 \n [321] \"124\"                                 \n [322] \"716\"                                 \n [323] \"340\"                                 \n [324] \"578\"                                 \n [325] \"428\"                                 \n [326] \"184\"                                 \n [327] \"213\"                                 \n [328] \"310\"                                 \n [329] \"238\"                                 \n [330] \"382\"                                 \n [331] \"88\"                                  \n [332] \"131\"                                 \n [333] \"125\"                                 \n [334] \"479\"                                 \n [335] \"316\"                                 \n [336] \"717\"                                 \n [337] \"118\"                                 \n [338] \"278\"                                 \n [339] \"9\"                                   \n [340] \"465\"                                 \n [341] \"262\"                                 \n [342] \"438\"                                 \n [343] \"68\"                                  \n [344] \"450\"                                 \n [345] \"267\"                                 \n [346] \"337\"                                 \n [347] \"483\"                                 \n [348] \"72\"                                  \n [349] \"644\"                                 \n [350] \"591\"                                 \n [351] \"285\"                                 \n [352] \"216\"                                 \n [353] \"417\"                                 \n [354] \"420\"                                 \n [355] \"502\"                                 \n [356] \"596\"                                 \n [357] \"22\"                                  \n [358] \"626\"                                 \n [359] \"14\"                                  \n [360] \"342\"                                 \n [361] \"11\"                                  \n [362] \"143\"                                 \n [363] \"639\"                                 \n [364] \"75\"                                  \n [365] \"525\"                                 \n [366] \"456\"                                 \n [367] \"366\"                                 \n [368] \"579\"                                 \n [369] \"528\"                                 \n [370] \"312\"                                 \n [371] \"550\"                                 \n [372] \"40\"                                  \n [373] \"263\"                                 \n [374] \"493\"                                 \n [375] \"592\"                                 \n [376] \"283\"                                 \n [377] \"37\"                                  \n [378] \"256\"                                 \n [379] \"370\"                                 \n [380] \"58\"                                  \n [381] \"218\"                                 \n [382] \"432\"                                 \n [383] \"381\"                                 \n [384] \"222\"                                 \n [385] \"705\"                                 \n [386] \"698\"                                 \n [387] \"160\"                                 \n [388] \"535\"                                 \n [389] \"217\"                                 \n [390] \"206\"                                 \n [391] \"86\"                                  \n [392] \"706\"                                 \n [393] \"410\"                                 \n [394] \"248\"                                 \n [395] \"374\"                                 \n [396] \"311\"                                 \n [397] \"367\"                                 \n [398] \"42\"                                  \n [399] \"442\"                                 \n [400] \"185\"                                 \n [401] \"67\"                                  \n [402] \"498\"                                 \n [403] \"208\"                                 \n [404] \"401\"                                 \n [405] \"32\"                                  \n [406] \"385\"                                 \n [407] \"33\"                                  \n [408] \"91\"                                  \n [409] \"167\"                                 \n [410] \"657\"                                 \n [411] \"635\"                                 \n [412] \"488\"                                 \n [413] \"234\"                                 \n [414] \"146\"                                 \n [415] \"281\"                                 \n [416] \"174\"                                 \n [417] \"546\"                                 \n [418] \"378\"                                 \n [419] \"352\"                                 \n [420] \"244\"                                 \n [421] \"510\"                                 \n [422] \"12\"                                  \n [423] \"241\"                                 \n [424] \"298\"                                 \n [425] \"659\"                                 \n [426] \"690\"                                 \n [427] \"598\"                                 \n [428] \"697\"                                 \n [429] \"662\"                                 \n [430] \"469\"                                 \n [431] \"418\"                                 \n [432] \"711\"                                 \n [433] \"345\"                                 \n [434] \"421\"                                 \n [435] \"365\"                                 \n [436] \"713\"                                 \n [437] \"343\"                                 \n [438] \"415\"                                 \n [439] \"317\"                                 \n [440] \"274\"                                 \n [441] \"109\"                                 \n [442] \"522\"                                 \n [443] \"30\"                                  \n [444] \"80\"                                  \n [445] \"452\"                                 \n [446] \"702\"                                 \n [447] \"464\"                                 \n [448] \"674\"                                 \n [449] \"204\"                                 \n [450] \"356\"                                 \n [451] \"292\"                                 \n [452] \"554\"                                 \n [453] \"191\"                                 \n [454] \"643\"                                 \n [455] \"477\"                                 \n [456] \"179\"                                 \n [457] \"414\"                                 \n [458] \"466\"                                 \n [459] \"622\"                                 \n [460] \"602\"                                 \n [461] \"641\"                                 \n [462] \"704\"                                 \n [463] \"275\"                                 \n [464] \"693\"                                 \n [465] \"676\"                                 \n [466] \"700\"                                 \n [467] \"449\"                                 \n [468] \"121\"                                 \n [469] \"656\"                                 \n [470] \"149\"                                 \n [471] \"630\"                                 \n [472] \"383\"                                 \n [473] \"497\"                                 \n [474] \"351\"                                 \n [475] \"482\"                                 \n [476] \"205\"                                 \n [477] \"490\"                                 \n [478] \"709\"                                 \n [479] \"661\"                                 \n [480] \"409\"                                 \n [481] \"696\"                                 \n [482] \"377\"                                 \n [483] \"373\"                                 \n [484] \"282\"                                 \n [485] \"536\"                                 \n [486] \"495\"                                 \n [487] \"348\"                                 \n [488] \"280\"                                 \n [489] \"619\"                                 \n [490] \"494\"                                 \n [491] \"441\"                                 \n [492] \"553\"                                 \n [493] \"200\"                                 \n [494] \"524\"                                 \n [495] \"119\"                                 \n [496] \"314\"                                 \n [497] \"666\"                                 \n [498] \"50\"                                  \n [499] \"330\"                                 \n [500] \"527\"                                 \n [501] \"159\"                                 \n [502] \"169\"                                 \n [503] \"551\"                                 \n [504] \"586\"                                 \n [505] \"566\"                                 \n [506] \"368\"                                 \n [507] \"478\"                                 \n [508] \"658\"                                 \n [509] \"540\"                                 \n [510] \"655\"                                 \n [511] \"559\"                                 \n [512] \"569\"                                 \n [513] \"407\"                                 \n [514] \"203\"                                 \n [515] \"531\"                                 \n [516] \"605\"                                 \n [517] \"413\"                                 \n [518] \"509\"                                 \n [519] \"422\"                                 \n [520] \"434\"                                 \n [521] \"689\"                                 \n [522] \"233\"                                 \n [523] \"170\"                                 \n [524] \"649\"                                 \n [525] \"476\"                                 \n [526] \"443\"                                 \n [527] \"390\"                                 \n [528] \"684\"                                 \n [529] \"642\"                                 \n [530] \"599\"                                 \n [531] \"433\"                                 \n [532] \"405\"                                 \n [533] \"508\"                                 \n [534] \"547\"                                 \n [535] \"533\"                                 \n [536] \"518\"                                 \n [537] \"544\"                                 \n [538] \"135\"                                 \n [539] \"95\"                                  \n [540] \"122\"                                 \n [541] \"590\"                                 \n [542] \"486\"                                 \n [543] \"336\"                                 \n [544] \"601\"                                 \n [545] \"663\"                                 \n [546] \"399\"                                 \n [547] \"103\"                                 \n [548] \"201\"                                 \n [549] \"519\"                                 \n [550] \"568\"                                 \n [551] \"448\"                                 \n [552] \"437\"                                 \n [553] \"403\"                                 \n [554] \"594\"                                 \n [555] \"429\"                                 \n [556] \"462\"                                 \n [557] \"207\"                                 \n [558] \"703\"                                 \n [559] \"699\"                                 \n [560] \"683\"                                 \n [561] \"369\"                                 \n [562] \"55\"                                  \n [563] \"576\"                                 \n [564] \"402\"                                 \n [565] \"686\"                                 \n [566] \"335\"                                 \n [567] \"489\"                                 \n [568] \"595\"                                 \n [569] \"583\"                                 \n [570] \"646\"                                 \n [571] \"575\"                                 \n [572] \"526\"                                 \n [573] \"549\"                                 \n [574] \"562\"                                 \n [575] \"631\"                                 \n [576] \"653\"                                 \n [577] \"529\"                                 \n [578] \"552\"                                 \n [579] \"707\"                                 \n [580] \"691\"                                 \n [581] \"393\"                                 \n [582] \"209\"                                 \n [583] \"534\"                                 \n [584] \"573\"                                 \n [585] \"555\"                                 \n [586] \"455\"                                 \n [587] \"353\"                                 \n [588] \"396\"                                 \n [589] \"677\"                                 \n [590] \"692\"                                 \n [591] \"440\"                                 \n [592] \"570\"                                 \n [593] \"375\"                                 \n [594] \"388\"                                 \n [595] \"101\"                                 \n [596] \"545\"                                 \n [597] \"565\"                                 \n [598] \"572\"                                 \n [599] \"685\"                                 \n [600] \"102\"                                 \n [601] \"574\"                                 \n [602] \"530\"                                 \n [603] \"542\"                                 \n [604] \"577\"                                 \n [605] \"427\"                                 \n [606] \"391\"                                 \n [607] \"439\"                                 \n [608] \"681\"                                 \n [609] \"694\"                                 \n [610] \"682\"                                 \n [611] \"398\"                                 \n [612] \"556\"                                 \n [613] \"430\"                                 \n [614] \"270\"                                 \n [615] \"580\"                                 \n [616] \"560\"                                 \n [617] \"412\"                                 \n [618] \"392\"                                 \n [619] \"564\"                                 \n [620] \"548\"                                 \n [621] \"581\"                                 \n [622] \"384\"                                 \n [623] \"718\"                                 \n [624] \"582\"                                 \n [625] \"708\"                                 \n [626] \"567\"                                 \n [627] \"561\"                                 \n [628] \"386\"                                 \n [629] \"558\"                                 \n [630] \"446\"                                 \n [631] \"395\"                                 \n [632] \"687\"                                 \n [633] \"541\"                                 \n [634] \"538\"                                 \n [635] \"678\"                                 \n [636] \"539\"                                 \n [637] \"543\"                                 \n [638] \"584\"                                 \n [639] \"537\"                                 \n [640] \"445\"                                 \n [641] \"665\"                                 \n [642] \"593\"                                 \n [643] \"587\"                                 \n [644] \"679\"                                 \n [645] \"720\"                                 \n [646] \"557\"                                 \n [647] \"650\"                                 \n [648] \"652\"                                 \n [649] \"648\"                                 \n [650] \"400\"                                 \n [651] \"588\"                                 \n [652] \"671\"                                 \n [653] \"721\"                                 \n [654] \"710\"                                 \n [655] \"647\"                                 \n [656] \"715\"                                 \n [657] \"719\"                                 \n [658] \"723\"                                 \n [659] \"731\"                                 \n [660] \"722\"                                 \n [661] \"726\"                                 \n [662] \"728\"                                 \n [663] \"727\"                                 \n [664] \"730\"                                 \n [665] \"725\"                                 \n [666] \"724\"                                 \n [667] \"732\"                                 \n [668] \"13157\"                               \n [669] \"TA1309000006\"                        \n [670] \"KA1503000043\"                        \n [671] \"TA1309000014\"                        \n [672] \"TA1305000006\"                        \n [673] \"13042\"                               \n [674] \"TA1307000127\"                        \n [675] \"TA1306000003\"                        \n [676] \"TA1306000006\"                        \n [677] \"TA1306000011\"                        \n [678] \"SL-013\"                              \n [679] \"13124\"                               \n [680] \"13137\"                               \n [681] \"13074\"                               \n [682] \"TA1309000066\"                        \n [683] \"TA1307000138\"                        \n [684] \"TA1307000052\"                        \n [685] \"TA1309000059\"                        \n [686] \"13268\"                               \n [687] \"KA1503000002\"                        \n [688] \"TA1307000140\"                        \n [689] \"13294\"                               \n [690] \"TA1307000062\"                        \n [691] \"15655\"                               \n [692] \"TA1307000064\"                        \n [693] \"TA1306000010\"                        \n [694] \"13058\"                               \n [695] \"13179\"                               \n [696] \"KA1504000076\"                        \n [697] \"13431\"                               \n [698] \"KA1503000070\"                        \n [699] \"13197\"                               \n [700] \"13006\"                               \n [701] \"KA1503000040\"                        \n [702] \"TA1307000129\"                        \n [703] \"TA1307000159\"                        \n [704] \"13326\"                               \n [705] \"SL-009\"                              \n [706] \"TA1307000120\"                        \n [707] \"13332\"                               \n [708] \"TA1305000011\"                        \n [709] \"TA1307000070\"                        \n [710] \"TA1305000030\"                        \n [711] \"TA1309000008\"                        \n [712] \"15545\"                               \n [713] \"13162\"                               \n [714] \"TA1307000039\"                        \n [715] \"13164\"                               \n [716] \"KA1504000104\"                        \n [717] \"13353\"                               \n [718] \"13022\"                               \n [719] \"13036\"                               \n [720] \"13139\"                               \n [721] \"KA1503000064\"                        \n [722] \"16920\"                               \n [723] \"13243\"                               \n [724] \"13017\"                               \n [725] \"TA1305000001\"                        \n [726] \"13028\"                               \n [727] \"TA1307000121\"                        \n [728] \"TA1307000130\"                        \n [729] \"SL-005\"                              \n [730] \"13165\"                               \n [731] \"TA1308000019\"                        \n [732] \"KA1503000041\"                        \n [733] \"TA1305000009\"                        \n [734] \"KA1503000072\"                        \n [735] \"TA1309000004\"                        \n [736] \"TA1309000035\"                        \n [737] \"KA1504000129\"                        \n [738] \"TA1306000007\"                        \n [739] \"TA1309000043\"                        \n [740] \"KA1503000034\"                        \n [741] \"KA1504000140\"                        \n [742] \"TA1307000126\"                        \n [743] \"KA1503000012\"                        \n [744] \"TA1307000166\"                        \n [745] \"TA1305000041\"                        \n [746] \"TA1309000019\"                        \n [747] \"15628\"                               \n [748] \"TA1308000049\"                        \n [749] \"TA1305000035\"                        \n [750] \"KA1504000116\"                        \n [751] \"13224\"                               \n [752] \"KA1504000149\"                        \n [753] \"TA1307000156\"                        \n [754] \"13276\"                               \n [755] \"TA1306000032\"                        \n [756] \"13136\"                               \n [757] \"15664\"                               \n [758] \"15529\"                               \n [759] \"13134\"                               \n [760] \"TA1307000041\"                        \n [761] \"KA1503000022\"                        \n [762] \"KA1504000096\"                        \n [763] \"TA1308000005\"                        \n [764] \"13409\"                               \n [765] \"13229\"                               \n [766] \"13109\"                               \n [767] \"15578\"                               \n [768] \"17660\"                               \n [769] \"TA1309000018\"                        \n [770] \"KA1504000103\"                        \n [771] \"KA1504000106\"                        \n [772] \"TA1309000007\"                        \n [773] \"TA1309000001\"                        \n [774] \"13389\"                               \n [775] \"13253\"                               \n [776] \"20252.0\"                             \n [777] \"13292\"                               \n [778] \"KA1504000159\"                        \n [779] \"20110\"                               \n [780] \"KA1504000164\"                        \n [781] \"15491\"                               \n [782] \"13245\"                               \n [783] \"15546\"                               \n [784] \"TA1307000128\"                        \n [785] \"TA1309000037\"                        \n [786] \"13258\"                               \n [787] \"RP-004\"                              \n [788] \"13156\"                               \n [789] \"TA1305000022\"                        \n [790] \"TA1307000005\"                        \n [791] \"13263\"                               \n [792] \"20116\"                               \n [793] \"15640\"                               \n [794] \"TA1309000049\"                        \n [795] \"TA1309000039\"                        \n [796] \"20207\"                               \n [797] \"TA1308000002\"                        \n [798] \"13277\"                               \n [799] \"13241\"                               \n [800] \"TA1305000010\"                        \n [801] \"KA1504000135\"                        \n [802] \"18016\"                               \n [803] \"13303\"                               \n [804] \"KA1504000082\"                        \n [805] \"KA1503000047\"                        \n [806] \"KA1503000033\"                        \n [807] \"KA1504000148\"                        \n [808] \"13133\"                               \n [809] \"TA1309000011\"                        \n [810] \"15692\"                               \n [811] \"15645\"                               \n [812] \"TA1309000055\"                        \n [813] \"16940\"                               \n [814] \"20238\"                               \n [815] \"20120\"                               \n [816] \"TA1307000163\"                        \n [817] \"13071\"                               \n [818] \"13213\"                               \n [819] \"20205\"                               \n [820] \"16903\"                               \n [821] \"20109\"                               \n [822] \"TA1307000161\"                        \n [823] \"20236\"                               \n [824] \"15682\"                               \n [825] \"KA1503000013\"                        \n [826] \"KA1503000018\"                        \n [827] \"SL-011\"                              \n [828] \"TA1308000050\"                        \n [829] \"TA1307000131\"                        \n [830] \"13296\"                               \n [831] \"13073\"                               \n [832] \"TA1308000021\"                        \n [833] \"13146\"                               \n [834] \"TA1305000029\"                        \n [835] \"TA1305000014\"                        \n [836] \"13323\"                               \n [837] \"TA1305000005\"                        \n [838] \"13269\"                               \n [839] \"TA1309000029\"                        \n [840] \"13008\"                               \n [841] \"15691\"                               \n [842] \"KA1503000053\"                        \n [843] \"13206\"                               \n [844] \"TA1305000037\"                        \n [845] \"KA1503000038\"                        \n [846] \"13290\"                               \n [847] \"16943\"                               \n [848] \"15631\"                               \n [849] \"20231\"                               \n [850] \"KA1504000161\"                        \n [851] \"TA1309000063\"                        \n [852] \"TA1308000038\"                        \n [853] \"15470\"                               \n [854] \"13091\"                               \n [855] \"15644\"                               \n [856] \"20233\"                               \n [857] \"KA1503000054\"                        \n [858] \"TA1307000119\"                        \n [859] \"20227\"                               \n [860] \"13341\"                               \n [861] \"TA1308000001\"                        \n [862] \"TA1309000061\"                        \n [863] \"18069\"                               \n [864] \"KA1504000093\"                        \n [865] \"15535\"                               \n [866] \"TA1305000039\"                        \n [867] \"TA1307000111\"                        \n [868] \"TA1308000029\"                        \n [869] \"KA1503000020\"                        \n [870] \"KA1503000075\"                        \n [871] \"KA1504000167\"                        \n [872] \"18062\"                               \n [873] \"15623\"                               \n [874] \"KA1503000044\"                        \n [875] \"15634\"                               \n [876] \"20221\"                               \n [877] \"KA1504000168\"                        \n [878] \"15654\"                               \n [879] \"20222\"                               \n [880] \"13053\"                               \n [881] \"15534\"                               \n [882] \"16950\"                               \n [883] \"651\"                                 \n [884] \"Hubbard Bike-checking (LBS-WH-TEST)\" \n [885] \"KA1706005015\"                        \n [886] \"18022\"                               \n [887] \"TA1309000053\"                        \n [888] \"KA1706005007\"                        \n [889] \"15550\"                               \n [890] \"15632\"                               \n [891] \"13056\"                               \n [892] \"15689\"                               \n [893] \"13247\"                               \n [894] \"TA1306000008\"                        \n [895] \"KA1504000141\"                        \n [896] \"KA1503000068\"                        \n [897] \"KA1503000052\"                        \n [898] \"15667\"                               \n [899] \"KA1504000156\"                        \n [900] \"15575\"                               \n [901] \"15446\"                               \n [902] \"20226\"                               \n [903] \"E007\"                                \n [904] \"KA1503000045\"                        \n [905] \"E002\"                                \n [906] \"KA1503000003\"                        \n [907] \"13307\"                               \n [908] \"15442\"                               \n [909] \"13045\"                               \n [910] \"TA1307000001\"                        \n [911] \"13379\"                               \n [912] \"15642\"                               \n [913] \"15539\"                               \n [914] \"TA1308000036\"                        \n [915] \"15530\"                               \n [916] \"TA1305000025\"                        \n [917] \"TA1309000002\"                        \n [918] \"KA1504000142\"                        \n [919] \"TA1308000026\"                        \n [920] \"16937\"                               \n [921] \"E008\"                                \n [922] \"15449\"                               \n [923] \"20229\"                               \n [924] \"16913\"                               \n [925] \"15622\"                               \n [926] \"KA1504000151\"                        \n [927] \"KA1504000113\"                        \n [928] \"15646\"                               \n [929] \"TA1308000014\"                        \n [930] \"TA1309000021\"                        \n [931] \"TA1308000007\"                        \n [932] \"13345\"                               \n [933] \"13331\"                               \n [934] \"TA1308000013\"                        \n [935] \"16806\"                               \n [936] \"KA1504000134\"                        \n [937] \"15666\"                               \n [938] \"KA1504000109\"                        \n [939] \"TA1309000003\"                        \n [940] \"TA1309000025\"                        \n [941] \"TA1307000158\"                        \n [942] \"15615\"                               \n [943] \"TA1309000032\"                        \n [944] \"KA1504000091\"                        \n [945] \"13191\"                               \n [946] \"20105\"                               \n [947] \"16010\"                               \n [948] \"TA1305000004\"                        \n [949] \"E011\"                                \n [950] \"KA150400009X\"                        \n [951] \"KA1504000097\"                        \n [952] \"13285\"                               \n [953] \"TA1308000006\"                        \n [954] \"KA1504000155\"                        \n [955] \"KA1504000130\"                        \n [956] \"LF-005\"                              \n [957] \"13300\"                               \n [958] \"KA1504000152\"                        \n [959] \"13193\"                               \n [960] \"13029\"                               \n [961] \"KA1503000059\"                        \n [962] \"20242\"                               \n [963] \"KA1503000028\"                        \n [964] \"TA1308000045\"                        \n [965] \"TA1309000030\"                        \n [966] \"KA1503000065\"                        \n [967] \"KA1504000086\"                        \n [968] \"20244\"                               \n [969] \"KA1503000025\"                        \n [970] \"TA1308000035\"                        \n [971] \"13061\"                               \n [972] \"TA1309000041\"                        \n [973] \"13143\"                               \n [974] \"KA1504000175\"                        \n [975] \"TA1307000149\"                        \n [976] \"RN-\"                                 \n [977] \"13216\"                               \n [978] \"TA1306000002\"                        \n [979] \"KA1504000090\"                        \n [980] \"13256\"                               \n [981] \"TA1307000160\"                        \n [982] \"WL-008\"                              \n [983] \"13288\"                               \n [984] \"KA1504000147\"                        \n [985] \"KA1503000021\"                        \n [986] \"KA1503000023\"                        \n [987] \"KA1503000049\"                        \n [988] \"13196\"                               \n [989] \"15445\"                               \n [990] \"13221\"                               \n [991] \"KA17018068\"                          \n [992] \"15621\"                               \n [993] \"13398\"                               \n [994] \"TA1307000107\"                        \n [995] \"13128\"                               \n [996] \"TA1307000048\"                        \n [997] \"TA1309000010\"                        \n [998] \"13084\"                               \n [999] \"TA1306000009\"                        \n[1000] \"SL-006\"                              \n[1001] \"KA1504000102\"                        \n[1002] \"TA1305000017\"                        \n[1003] \"KA1504000078\"                        \n[1004] \"18025\"                               \n[1005] \"KA1504000110\"                        \n[1006] \"16991\"                               \n[1007] \"RP-005\"                              \n[1008] \"16994\"                               \n[1009] \"E014\"                                \n[1010] \"KA1503000007\"                        \n[1011] \"TA1308000009\"                        \n[1012] \"TA1309000026\"                        \n[1013] \"13257\"                               \n[1014] \"15686\"                               \n[1015] \"13021\"                               \n[1016] \"KA1503000001\"                        \n[1017] \"13235\"                               \n[1018] \"20246.0\"                             \n[1019] \"TA1306000026\"                        \n[1020] \"KA1503000019\"                        \n[1021] \"13434\"                               \n[1022] \"TA1307000139\"                        \n[1023] \"13194\"                               \n[1024] \"20104\"                               \n[1025] \"KA1503000031\"                        \n[1026] \"13108\"                               \n[1027] \"13430\"                               \n[1028] \"KA1503000011\"                        \n[1029] \"KA1503000030\"                        \n[1030] \"18003\"                               \n[1031] \"KP1705001026\"                        \n[1032] \"KA1504000146\"                        \n[1033] \"13016\"                               \n[1034] \"KA1504000079\"                        \n[1035] \"13001\"                               \n[1036] \"TA1306000013\"                        \n[1037] \"TA1306000025\"                        \n[1038] \"13034\"                               \n[1039] \"13427\"                               \n[1040] \"TA1307000115\"                        \n[1041] \"KA1504000101\"                        \n[1042] \"TA1309000064\"                        \n[1043] \"TA1309000027\"                        \n[1044] \"TA1309000036\"                        \n[1045] \"13063\"                               \n[1046] \"15542\"                               \n[1047] \"TA1308000012\"                        \n[1048] \"TA1306000015\"                        \n[1049] \"WL-011\"                              \n[1050] \"KA1503000029\"                        \n[1051] \"KA1503000074\"                        \n[1052] \"13192\"                               \n[1053] \"13289\"                               \n[1054] \"TA1307000136\"                        \n[1055] \"TA1307000143\"                        \n[1056] \"13249\"                               \n[1057] \"13432\"                               \n[1058] \"TA1307000142\"                        \n[1059] \"TA1306000014\"                        \n[1060] \"13319\"                               \n[1061] \"KA1504000133\"                        \n[1062] \"13068\"                               \n[1063] \"15571\"                               \n[1064] \"TA1307000150\"                        \n[1065] \"KA1503000015\"                        \n[1066] \"13150\"                               \n[1067] \"13135\"                               \n[1068] \"13158\"                               \n[1069] \"13265\"                               \n[1070] \"TA1307000151\"                        \n[1071] \"13278\"                               \n[1072] \"13099\"                               \n[1073] \"13037\"                               \n[1074] \"13075\"                               \n[1075] \"TA1309000023\"                        \n[1076] \"TA1305000003\"                        \n[1077] \"TA1309000024\"                        \n[1078] \"13138\"                               \n[1079] \"13160\"                               \n[1080] \"TA1309000015\"                        \n[1081] \"TA1307000144\"                        \n[1082] \"TA1307000006\"                        \n[1083] \"TA1308000031\"                        \n[1084] \"TA1305000020\"                        \n[1085] \"KA1504000127\"                        \n[1086] \"TA1308000047\"                        \n[1087] \"RP-009\"                              \n[1088] \"RP-002\"                              \n[1089] \"13325\"                               \n[1090] \"TA1307000134\"                        \n[1091] \"TA1309000050\"                        \n[1092] \"TA1306000012\"                        \n[1093] \"TA1309000012\"                        \n[1094] \"LP-\"                                 \n[1095] \"KA1504000158\"                        \n[1096] \"KA1504000080\"                        \n[1097] \"TA1306000016\"                        \n[1098] \"TA1307000038\"                        \n[1099] \"18067\"                               \n[1100] \"13085\"                               \n[1101] \"13242\"                               \n[1102] \"RP-001\"                              \n[1103] \"SL-010\"                              \n[1104] \"15585\"                               \n[1105] \"KA1504000114\"                        \n[1106] \"13155\"                               \n[1107] \"WL-012\"                              \n[1108] \"18058\"                               \n[1109] \"SL-007\"                              \n[1110] \"13266\"                               \n[1111] \"KA1504000143\"                        \n[1112] \"13089\"                               \n[1113] \"KA1503000066\"                        \n[1114] \"13081\"                               \n[1115] \"TA1308000043\"                        \n[1116] \"KA1504000160\"                        \n[1117] \"KA1503000046\"                        \n[1118] \"KA1503000051\"                        \n[1119] \"KA17018054\"                          \n[1120] \"20119\"                               \n[1121] \"16906\"                               \n[1122] \"TA1307000124\"                        \n[1123] \"13304\"                               \n[1124] \"20243\"                               \n[1125] \"TA1309000042\"                        \n[1126] \"20234\"                               \n[1127] \"20254.0\"                             \n[1128] \"18017\"                               \n[1129] \"KA1503000073\"                        \n[1130] \"KA1503000027\"                        \n[1131] \"13080\"                               \n[1132] \"KA1504000128\"                        \n[1133] \"13033\"                               \n[1134] \"13259\"                               \n[1135] \"KA1504000117\"                        \n[1136] \"16933\"                               \n[1137] \"16948\"                               \n[1138] \"16912\"                               \n[1139] \"KA1503000069\"                        \n[1140] \"15651\"                               \n[1141] \"TA1309000058\"                        \n[1142] \"20239\"                               \n[1143] \"TA1308000023\"                        \n[1144] \"15687\"                               \n[1145] \"15597\"                               \n[1146] \"KA1503000055\"                        \n[1147] \"TA1307000066\"                        \n[1148] \"KA1503000071\"                        \n[1149] \"RP-006\"                              \n[1150] \"15544\"                               \n[1151] \"16921\"                               \n[1152] \"13154\"                               \n[1153] \"KA1503000032\"                        \n[1154] \"13354\"                               \n[1155] \"KA1503000004\"                        \n[1156] \"13050\"                               \n[1157] \"20232\"                               \n[1158] \"20206\"                               \n[1159] \"16932\"                               \n[1160] \"13338\"                               \n[1161] \"KA1503000010\"                        \n[1162] \"20121\"                               \n[1163] \"20107\"                               \n[1164] \"20210\"                               \n[1165] \"KA1503000009\"                        \n[1166] \"13420\"                               \n[1167] \"13011\"                               \n[1168] \"15648\"                               \n[1169] \"16953\"                               \n[1170] \"13083\"                               \n[1171] \"13102\"                               \n[1172] \"16905\"                               \n[1173] \"13144\"                               \n[1174] \"RP-008\"                              \n[1175] \"SL-008\"                              \n[1176] \"15685\"                               \n[1177] \"20127\"                               \n[1178] \"20223\"                               \n[1179] \"TA1307000113\"                        \n[1180] \"13217\"                               \n[1181] \"TA1307000117\"                        \n[1182] \"KA1504000139\"                        \n[1183] \"TA1307000153\"                        \n[1184] \"20108\"                               \n[1185] \"TA1305000032\"                        \n[1186] \"TA1305000002\"                        \n[1187] \"13132\"                               \n[1188] \"KA1503000024\"                        \n[1189] \"20235\"                               \n[1190] \"20131\"                               \n[1191] \"15541\"                               \n[1192] \"15443\"                               \n[1193] \"KA1504000126\"                        \n[1194] \"20218\"                               \n[1195] \"RP-007\"                              \n[1196] \"KA1503000005\"                        \n[1197] \"TA1308000046\"                        \n[1198] \"15652\"                               \n[1199] \"15668\"                               \n[1200] \"SL-012\"                              \n[1201] \"15624\"                               \n[1202] \"E006\"                                \n[1203] \"TA1309000067\"                        \n[1204] \"20230\"                               \n[1205] \"TA1309000051\"                        \n[1206] \"TA1305000034\"                        \n[1207] \"TA1307000061\"                        \n[1208] \"16918\"                               \n[1209] \"20118\"                               \n[1210] \"20129\"                               \n[1211] \"TA1307000044\"                        \n[1212] \"13215\"                               \n[1213] \"KA1504000171\"                        \n[1214] \"13059\"                               \n[1215] \"13271\"                               \n[1216] \"TA1308000022\"                        \n[1217] \"TA1309000033\"                        \n[1218] \"TA1307000164\"                        \n[1219] \"15653\"                               \n[1220] \"15643\"                               \n[1221] \"TA1306000029\"                        \n[1222] \"13163\"                               \n[1223] \"KA1503000014\"                        \n[1224] \"13248\"                               \n[1225] \"15650\"                               \n[1226] \"13096\"                               \n[1227] \"KA1504000162\"                        \n[1228] \"20103\"                               \n[1229] \"16907\"                               \n[1230] \"20203\"                               \n[1231] \"16916\"                               \n[1232] \"20215\"                               \n[1233] \"20225\"                               \n[1234] \"20253.0\"                             \n[1235] \"16915\"                               \n[1236] \"20257.0\"                             \n[1237] \"20247.0\"                             \n[1238] \"20204\"                               \n[1239] \"20113\"                               \n[1240] \"20130\"                               \n[1241] \"20124\"                               \n[1242] \"20256.0\"                             \n[1243] \"16970\"                               \n[1244] \"20112\"                               \n[1245] \"20251.0\"                             \n[1246] \"20214\"                               \n[1247] \"15599\"                               \n[1248] \"20245\"                               \n[1249] \"20228\"                               \n[1250] \"20123\"                               \n[1251] \"20101\"                               \n[1252] \"20208\"                               \n[1253] \"20106\"                               \n[1254] \"20.0\"                                \n[1255] \"20211\"                               \n[1256] \"20258.0\"                             \n[1257] \"20128\"                               \n[1258] \"20217\"                               \n[1259] \"20248.0\"                             \n[1260] \"20213\"                               \n[1261] \"20212\"                               \n[1262] \"20125\"                               \n[1263] \"202480.0\"                            \n[1264] \"20201\"                               \n[1265] \"20220\"                               \n[1266] \"20224\"                               \n[1267] \"201022\"                              \n[1268] \"20133\"                               \n[1269] \"20202\"                               \n[1270] \"20249.0\"                             \n[1271] \"DIVVY 001\"                           \n[1272] \"20237\"                               \n[1273] \"20134\"                               \n[1274] \"20126\"                               \n[1275] \"20999\"                               \n[1276] \"365.0\"                               \n[1277] \"368.0\"                               \n[1278] \"362.0\"                               \n[1279] \"366.0\"                               \n[1280] \"364.0\"                               \n[1281] \"DIVVY CASSETTE REPAIR MOBILE STATION\"\n[1282] \"358\"                                 \n[1283] \"329.0\"                               \n[1284] \"363.0\"                               \n[1285] \"473\"                                 \n[1286] \"20209\"                               \n[1287] \"330.0\"                               \n[1288] \"331.0\"                               \n[1289] \"334.0\"                               \n[1290] \"360\"                                 \n[1291] \"397\"                                 \n[1292] \"335.0\"                               \n[1293] \"332.0\"                               \n[1294] \"357\"                                 \n\n\n\nCreating a vector of station ids identified as ids associated with Divvy test and repair stations.\n\ntrs_id <- c(\"DIVVY 001\",\n            \"DIVVY CASSETTE REPAIR MOBILE STATION\",\n            \"Hubbard Bike-checking (LBS-WH-TEST)\")\n\nKeeping only trips that are relevant for the analysis.\n\nall_trips_cln <- all_trips_cln %>%\n  filter(!start_station_name %in% trs_name &\n         !end_station_name %in% trs_name &\n         !start_station_id %in% trs_id &\n         !end_station_id %in% trs_id &\n         between(ride_length, 1, 180))\n\n\ndim_desc(all_trips_cln)\n\n[1] \"[4,804,481 x 13]\"\n\n\n\n\n🔍 Inspecting the difference between the number of station IDs and the number of station names\n\nall_trips_cln %>% \n  group_by(start_station_name, start_station_id) %>% \n  summarise(min_datetime = min(started_at), \n            max_datetime = max(started_at), \n            count = n(),.groups = 'drop') %>%\n  arrange(start_station_name, min_datetime) %>% \n  head(20) %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nstart_station_name\nstart_station_id\nmin_datetime\nmax_datetime\ncount\n\n\n\n\n2112 W Peterson Ave\n456\n2020-09-01 10:20:07\n2020-11-30 17:57:38\n295\n\n\n2112 W Peterson Ave\nKA1504000155\n2020-12-01 20:57:23\n2021-08-31 19:04:10\n766\n\n\n351\n351\n2021-08-04 16:31:29\n2021-08-07 23:59:38\n2\n\n\n63rd St Beach\n101\n2020-09-01 06:32:10\n2020-11-29 09:15:38\n645\n\n\n63rd St Beach\n15491\n2020-12-01 12:13:32\n2021-08-31 18:59:18\n1716\n\n\n900 W Harrison St\n109\n2020-09-01 00:10:07\n2020-11-30 21:37:27\n1540\n\n\n900 W Harrison St\n13028\n2020-12-01 05:57:08\n2021-08-31 23:32:29\n4379\n\n\nAberdeen St & Jackson Blvd\n21\n2020-09-01 05:21:41\n2020-11-30 18:00:59\n2762\n\n\nAberdeen St & Jackson Blvd\n13157\n2020-12-01 05:47:45\n2021-08-31 21:53:55\n7855\n\n\nAberdeen St & Monroe St\n80\n2020-09-01 06:26:54\n2020-11-30 16:31:57\n2126\n\n\nAberdeen St & Monroe St\n13156\n2020-12-01 00:36:33\n2021-08-31 23:57:23\n7547\n\n\nAberdeen St & Randolph St\n621\n2020-09-01 05:48:49\n2020-11-30 21:05:51\n1934\n\n\nAberdeen St & Randolph St\n18062\n2020-12-01 07:23:39\n2021-08-31 22:02:47\n6648\n\n\nAda St & 113th St\n727\n2020-10-07 19:34:30\n2020-11-26 22:29:45\n20\n\n\nAda St & 113th St\n20129\n2020-12-13 13:21:13\n2021-08-26 14:36:13\n65\n\n\nAda St & Washington Blvd\n346\n2020-09-01 04:34:53\n2020-11-30 20:44:42\n1928\n\n\nAda St & Washington Blvd\n13353\n2020-12-01 07:19:09\n2021-08-31 23:42:08\n6191\n\n\nAdler Planetarium\n341\n2020-09-01 07:04:27\n2020-11-30 15:22:52\n3961\n\n\nAdler Planetarium\n13431\n2020-12-01 15:08:23\n2021-08-31 22:11:50\n11668\n\n\nAlbany Ave & 26th St\n444\n2020-09-02 17:49:47\n2020-11-29 13:42:10\n104\n\n\n\n\n\nConclusion: Divvy changed station IDs for most stations at the beginning of December 2020. This will not affect the analysis, so we’ll leave it as is. What is important is that at the time of a ride, each station has been assigned a unique ID. Let’s see why this change happened.\nCreating a 3-dimensional frequency table.\n\nftable(all_trips_cln$user_type, all_trips_cln$year_month, all_trips_cln$rideable_type)\n\n                classic_bike docked_bike electric_bike\n                                                      \ncasual 2020-09             0      166860         57264\n       2020-10             0       77464         63239\n       2020-11             0       44729         41296\n       2020-12         11129        4801         13526\n       2021-01          8141        2026          7552\n       2021-02          5491        1196          3077\n       2021-03         44734       15100         22385\n       2021-04         69391       23795         40221\n       2021-05        121322       41444         87520\n       2021-06        183950       49440        127428\n       2021-07        236632       55529        139456\n       2021-08        225733       43517        134755\nmember 2020-09             0      226706         68014\n       2020-10             0      152039         85355\n       2020-11             0      103106         65012\n       2020-12         58556        7660         33450\n       2021-01         52683           1         24681\n       2021-02         28540           0          9846\n       2021-03        105476           0         36713\n       2021-04        141562           0         55563\n       2021-05        182005           0         87425\n       2021-06        242527           0        109475\n       2021-07        260782           0        112381\n       2021-08        268693           0        116087\n\n\nFrom September to November 2020, there were only two rideable types, docked_bike and electric_bike. At the beginning of December 2020, a new rideable type is introduced, the classic_bike, and a distinction is made between the docked_bike and the classic_bike. Since the beginning of 2021, casual riders can use classic and docked bikes, while annual members can only use classic bikes. Therefore, we cannot conclude a preference for one rideable type over another between casuals and members. Only when it comes to electric bikes.\n\n\n🔍 🛠️ Checking if any station has changed its name at some point\n\nall_trips_cln %>% \n  group_by(start_station_id, year_month) %>%\n  summarise(n_distinct_start_station_name = n_distinct(start_station_name), \n            .groups = 'drop') %>% \n  filter(n_distinct_start_station_name > 1) %>% \n  knitr::kable()\n\n\n\n\nstart_station_id\nyear_month\nn_distinct_start_station_name\n\n\n\n\n13074\n2021-01\n2\n\n\n13099\n2021-07\n2\n\n\n13221\n2021-07\n2\n\n\n13300\n2021-07\n2\n\n\n19\n2020-09\n2\n\n\n26\n2020-10\n2\n\n\n317\n2020-09\n2\n\n\n332\n2020-09\n2\n\n\n351\n2021-08\n2\n\n\n503\n2020-10\n2\n\n\n625\n2020-11\n2\n\n\n631\n2021-01\n2\n\n\n704\n2020-09\n2\n\n\n709\n2020-09\n2\n\n\n725\n2020-10\n2\n\n\nE011\n2020-12\n2\n\n\nLF-005\n2021-07\n2\n\n\nTA1305000039\n2021-05\n2\n\n\nTA1306000029\n2021-07\n2\n\n\nTA1307000041\n2021-07\n2\n\n\nTA1309000039\n2021-07\n2\n\n\nTA1309000049\n2021-07\n2\n\n\nNA\n2020-09\n4\n\n\nNA\n2020-10\n3\n\n\nNA\n2020-11\n4\n\n\n\n\n\n\nstart_id <- c(\"13074\", \"13099\", \"13300\", \"19\", \"26\", \"317\", \"332\", \"351\", \"503\",\n              \"625\", \"631\", \"704\", \"709\", \"725\", \"E011\", \"LF-005\", \"TA1305000039\",\n              \"TA1306000029\", \"TA1307000041\", \"TA1309000039\", \"TA1309000049\")\n\n\nall_trips_cln %>%\n  filter(start_station_id %in% start_id) %>% \n  group_by(start_station_id, start_station_name) %>%\n  summarise(min_datetime = min(started_at), \n            max_datetime = max(started_at), \n            count = n(), .groups = 'drop' ) %>% \n  arrange(start_station_id, min_datetime) %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nstart_station_id\nstart_station_name\nmin_datetime\nmax_datetime\ncount\n\n\n\n\n13074\nBroadway & Wilson Ave\n2020-12-01 09:29:54\n2021-01-26 19:50:38\n425\n\n\n13074\nBroadway & Wilson - Truman College Vaccination Site\n2021-01-27 14:35:48\n2021-08-31 22:38:18\n6036\n\n\n13099\nHalsted St & 18th St\n2020-12-01 06:10:22\n2021-07-28 12:51:03\n1616\n\n\n13099\nHalsted St & 18th St (Temp)\n2021-07-26 16:35:28\n2021-08-31 23:54:30\n509\n\n\n13300\nLake Shore Dr & Monroe St\n2020-12-01 08:26:56\n2021-07-21 10:45:56\n25133\n\n\n13300\nDuSable Lake Shore Dr & Monroe St\n2021-07-20 19:20:12\n2021-08-31 23:38:38\n9220\n\n\n19\nThroop (Loomis) St & Taylor St\n2020-09-01 00:06:50\n2020-09-30 07:08:15\n811\n\n\n19\nThroop St & Taylor St\n2020-09-30 06:11:29\n2020-11-30 17:30:36\n1048\n\n\n26\nMcClurg Ct & Illinois St\n2020-09-01 05:05:44\n2020-10-21 09:26:26\n3877\n\n\n26\nNew St & Illinois St\n2020-10-18 17:00:24\n2020-11-30 21:27:44\n1234\n\n\n317\nWood St & Taylor St\n2020-09-01 09:42:40\n2020-09-22 09:09:28\n417\n\n\n317\nWood St & Taylor St (Temp)\n2020-09-02 14:44:03\n2020-11-30 22:32:47\n1241\n\n\n317\nLong Ave & Belmont Ave\n2021-07-27 21:37:39\n2021-08-30 11:23:10\n34\n\n\n332\nBurling St (Halsted) & Diversey Pkwy (Temp)\n2020-09-01 07:32:26\n2020-09-23 15:16:31\n1204\n\n\n332\nBurling St & Diversey Pkwy\n2020-09-23 16:14:35\n2020-11-30 23:05:39\n2286\n\n\n351\nCottage Grove Ave & 51st St\n2020-09-01 12:43:12\n2020-11-30 23:21:30\n385\n\n\n351\nMulligan Ave & Wellington Ave\n2021-08-03 16:01:18\n2021-08-30 17:06:17\n41\n\n\n351\n351\n2021-08-04 16:31:29\n2021-08-07 23:59:38\n2\n\n\n503\nDrake Ave & Fullerton Ave\n2020-09-01 04:40:18\n2020-10-01 13:50:08\n447\n\n\n503\nSt. Louis Ave & Fullerton Ave\n2020-10-01 19:53:17\n2020-11-30 20:20:55\n392\n\n\n625\nChicago Ave & Dempster St\n2020-09-01 09:51:04\n2020-11-30 21:35:09\n731\n\n\n625\nDodge Ave & Main St\n2020-11-29 14:43:14\n2020-11-29 15:25:01\n3\n\n\n631\nMalcolm X College\n2020-09-01 14:36:58\n2021-01-23 18:39:19\n311\n\n\n631\nMalcolm X College Vaccination Site\n2021-01-27 19:13:29\n2021-08-31 18:31:05\n979\n\n\n704\nJeffery Blvd & 91st St\n2020-09-03 06:59:22\n2020-09-20 15:05:01\n19\n\n\n704\nAvenue O & 134th St\n2020-09-23 18:40:05\n2020-11-11 15:42:28\n34\n\n\n709\nHalsted St & 104th St\n2020-09-15 18:26:12\n2020-09-16 08:01:57\n8\n\n\n709\nMichigan Ave & 114th St\n2020-09-17 18:27:01\n2020-11-28 19:38:12\n20\n\n\n725\nWestern Ave & 104th St\n2020-10-06 14:38:17\n2020-10-06 14:38:17\n1\n\n\n725\nHalsted St & 104th St\n2020-10-13 13:05:17\n2020-11-12 11:20:12\n23\n\n\nE011\nChicago Ave & Dempster St\n2020-12-01 18:39:42\n2020-12-01 18:39:42\n1\n\n\nE011\nDodge Ave & Main St\n2020-12-20 18:35:35\n2021-08-31 16:08:19\n309\n\n\nLF-005\nLake Shore Dr & North Blvd\n2020-12-01 07:21:59\n2021-07-21 10:39:07\n23613\n\n\nLF-005\nDuSable Lake Shore Dr & North Blvd\n2021-07-01 17:21:59\n2021-08-31 21:30:07\n12941\n\n\nTA1305000039\nMarshfield Ave & Cortland St\n2020-12-01 06:57:11\n2021-05-26 09:06:13\n2396\n\n\nTA1305000039\nElston Ave & Cortland St\n2021-05-23 14:50:25\n2021-08-31 22:54:49\n3587\n\n\nTA1306000029\nLake Shore Dr & Ohio St\n2020-12-01 05:10:32\n2021-07-21 09:34:21\n11047\n\n\nTA1306000029\nDuSable Lake Shore Dr & Ohio St\n2021-07-21 10:59:27\n2021-08-31 23:47:54\n4458\n\n\nTA1307000041\nLake Shore Dr & Wellington Ave\n2020-12-01 08:32:58\n2021-07-21 10:31:29\n11320\n\n\nTA1307000041\nDuSable Lake Shore Dr & Wellington Ave\n2021-07-21 08:38:05\n2021-08-31 21:16:41\n5318\n\n\nTA1309000039\nLake Shore Dr & Diversey Pkwy\n2020-12-01 06:19:39\n2021-07-21 10:50:30\n11543\n\n\nTA1309000039\nDuSable Lake Shore Dr & Diversey Pkwy\n2021-07-21 10:55:37\n2021-08-31 22:12:24\n5376\n\n\nTA1309000049\nLake Shore Dr & Belmont Ave\n2020-12-01 01:39:00\n2021-07-21 10:03:25\n11101\n\n\nTA1309000049\nDuSable Lake Shore Dr & Belmont Ave\n2021-07-18 14:31:44\n2021-08-31 22:52:17\n5449\n\n\n\n\n\nAmong other changes, in July 2021, Chicago renamed the iconic Lake Shore Drive to honor its city’s ‘founder’ Jean Baptiste Point DuSable. It is now known as DuSable Lake Shore Drive. You can read the story here.\nWe’ll now replace old station names in the start_station_name and end_station_name columns with new ones. This step is necessary if we want to get an accurate list of the most popular stations.\n\nall_trips_cln <- all_trips_cln %>% \n  mutate(start_station_name = recode(start_station_name, \n        \"Broadway & Wilson Ave\" = \"Broadway & Wilson - Truman College Vaccination Site\",\n        \"Halsted St & 18th St\" = \"Halsted St & 18th St (Temp)\",\n        \"Lake Shore Dr & Monroe St\" = \"DuSable Lake Shore Dr & Monroe St\",\n        \"Throop (Loomis) St & Taylor St\" = \"Throop St & Taylor St\",\n        \"McClurg Ct & Illinois St\" = \"New St & Illinois St\",\n        \"Burling St (Halsted) & Diversey Pkwy (Temp)\" = \"Burling St & Diversey Pkwy\",\n        \"Drake Ave & Fullerton Ave\" = \"St. Louis Ave & Fullerton Ave\",\n        \"Malcolm X College\" = \"Malcolm X College Vaccination Site\",\n        \"Lake Shore Dr & North Blvd\" = \"DuSable Lake Shore Dr & North Blvd\",\n        \"Marshfield Ave & Cortland St\" = \"Elston Ave & Cortland St\",\n        \"Lake Shore Dr & Ohio St\" = \"DuSable Lake Shore Dr & Ohio St\",\n        \"Lake Shore Dr & Wellington Ave\" = \"DuSable Lake Shore Dr & Wellington Ave\",\n        \"Lake Shore Dr & Diversey Pkwy\" = \"DuSable Lake Shore Dr & Diversey Pkwy\",\n        \"Lake Shore Dr & Belmont Ave\" = \"DuSable Lake Shore Dr & Belmont Ave\")) %>% \n  mutate(end_station_name = recode(end_station_name, \n        \"Broadway & Wilson Ave\" = \"Broadway & Wilson - Truman College Vaccination Site\",\n        \"Halsted St & 18th St\" = \"Halsted St & 18th St (Temp)\",\n        \"Lake Shore Dr & Monroe St\" = \"DuSable Lake Shore Dr & Monroe St\",\n        \"Throop (Loomis) St & Taylor St\" = \"Throop St & Taylor St\",\n        \"McClurg Ct & Illinois St\" = \"New St & Illinois St\",\n        \"Burling St (Halsted) & Diversey Pkwy (Temp)\" = \"Burling St & Diversey Pkwy\",\n        \"Drake Ave & Fullerton Ave\" = \"St. Louis Ave & Fullerton Ave\",\n        \"Malcolm X College\" = \"Malcolm X College Vaccination Site\",\n        \"Lake Shore Dr & North Blvd\" = \"DuSable Lake Shore Dr & North Blvd\",\n        \"Marshfield Ave & Cortland St\" = \"Elston Ave & Cortland St\",\n        \"Lake Shore Dr & Ohio St\" = \"DuSable Lake Shore Dr & Ohio St\",\n        \"Lake Shore Dr & Wellington Ave\" = \"DuSable Lake Shore Dr & Wellington Ave\",\n        \"Lake Shore Dr & Diversey Pkwy\" = \"DuSable Lake Shore Dr & Diversey Pkwy\",\n        \"Lake Shore Dr & Belmont Ave\" = \"DuSable Lake Shore Dr & Belmont Ave\"))\n\n\n\n🔍 Inspecting observations with missing values (NAs) in the start_station_name, start_station_id, end_station_name, and end_station_id columns\n\nall_trips_NA <- all_trips_cln %>% \n  filter(is.na(start_station_name) | \n         is.na(end_station_name) |\n         is.na(start_station_id) | \n         is.na(end_station_id))\n\n# number of NAs in each column\ncolSums(is.na(all_trips_NA))\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            433066             433555             465223             465570 \n         user_type         year_month        day_of_week               hour \n                 0                  0                  0                  0 \n       ride_length \n                 0 \n\n\n\n# contingency table for the subset with NAs\ntable(all_trips_NA$year_month, all_trips_NA$rideable_type)\n\n         \n          classic_bike electric_bike\n  2020-09            0         30374\n  2020-10            0         47054\n  2020-11            0         35629\n  2020-12          108         16863\n  2021-01          135         12485\n  2021-02          107          5956\n  2021-03          182         21691\n  2021-04          161         37194\n  2021-05          197         77127\n  2021-06          346        115639\n  2021-07          473        124787\n  2021-08          378        125277\n\n\n\n# contingency table for the whole dataset\ntable(all_trips_cln$year_month, all_trips_cln$rideable_type)\n\n         \n          classic_bike docked_bike electric_bike\n  2020-09            0      393566        125278\n  2020-10            0      229503        148594\n  2020-11            0      147835        106308\n  2020-12        69685       12461         46976\n  2021-01        60824        2027         32233\n  2021-02        34031        1196         12923\n  2021-03       150210       15100         59098\n  2021-04       210953       23795         95784\n  2021-05       303327       41444        174945\n  2021-06       426477       49440        236903\n  2021-07       497414       55529        251837\n  2021-08       494426       43517        250842\n\n\n\nall_trips_NA %>%   \n  count(user_type)\n\n# A tibble: 2 × 2\n  user_type      n\n  <chr>      <int>\n1 casual    322707\n2 member    329456\n\n\n\nsummary(all_trips_NA$ride_length)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    6.82   12.38   17.82   22.13  180.00 \n\n\nDeleting the observations with missing values can reduce the statistical power of the analysis. We must understand why the data is missing. The reason for missing values here is the fact that electric bikes can be parked outside of the stations within a service area. You can find more information here. In Zone 1, an out-of-station parking fee ($2) is charged. In Zone 2, the out-of-station parking fee is waived to account for the lower density of stations.\nThere is no way to substitute missing values, and nothing indicates that something is wrong with this subset. If we delete it, we will lose a small percentage of rides taken with classic bikes (0.1%) and a lot of rides taken with electric bikes (42%). We’ll keep these observations.\n\n\n✅ Final dataset ready for analysis\n\n# selecting columns needed for analysis\nal_trips_cln <- all_trips_cln %>% \n  select(start_station_name, end_station_name, user_type, year_month, day_of_week, \n         hour, ride_length, rideable_type)\n\n\n# final dataset dimensions\ndim_desc(al_trips_cln)\n\n[1] \"[4,804,481 x 8]\"\n\n\n\n# proportion of trips removed\n(nrow(all_trips) - nrow(all_trips_cln)) / nrow(all_trips) \n\n[1] 0.02210246"
  },
  {
    "objectID": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-iv---analyze-share",
    "href": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-iv---analyze-share",
    "title": "Cyclistic Bike-Share Analysis",
    "section": "✨ PART IV - Analyze & Share",
    "text": "✨ PART IV - Analyze & Share\nIn this step, we’ll try to find an answer to the question, “How do annual members and casual riders use Cyclistic bikes differently?”, and share key findings by analyzing the following:\n\nnumber of rides\naverage ride length\nbike-type usage\nmost popular stations\n\n\n\nCode for ggplot theme customization\n# theme customization\nmy_theme <- theme(plot.title=element_text(size=14),\n                  plot.subtitle=element_text(size=10),\n                  axis.text.x=element_text(size=10),\n                  axis.text.y=element_text(size=10),\n                  axis.title.x=element_text(size=10),\n                  axis.title.y=element_text(size=10),\n                  strip.text = element_text(size=12),\n                  legend.title=element_text(size=11),\n                  legend.text=element_text(size=11))\n\nmy_colors <- c(\"#355273\", \"#FF3030\") \n\n\n\n\n💡 Number of rides\n\n📊 Total Rides by User Type\n\n\nShow code\nall_trips_cln %>% \n  select(user_type) %>% \n  group_by(user_type) %>% \n  summarise(total_rides = n(), .groups = 'drop') %>% \n  webr::PieDonut(aes(user_type, count = total_rides), r0 = 0.7, r1 = 0.9, \n                 labelpositionThreshold = 1, showPieName = FALSE, pieAlpha = 1) + \n  scale_fill_manual(values = my_colors) +\n  annotate(geom = 'text', x = 0, y = 0, \n           label = str_c(\"Totak Rides: \", round(nrow(all_trips_cln)/1e6, 1), \"M\"),\n           size = 4.7) +\n  theme_void()\n\n\n\n\n\n\n\n📊 Total Rides by Month\n\n\nShow code and table output\n\n\ntotal_rides_by_month <- all_trips_cln %>% \n  select(user_type, year_month) %>% \n  group_by(user_type, year_month) %>% \n  summarise(total_rides = n()) %>% \n  mutate(percentage = scales::percent(total_rides/sum(total_rides), accuracy = 0.1)) %>% \n  ungroup()\n  \ntotal_rides_by_month %>% knitr::kable()\n\n\n\n\nuser_type\nyear_month\ntotal_rides\npercentage\n\n\n\n\ncasual\n2020-09\n224124\n10.3%\n\n\ncasual\n2020-10\n140703\n6.5%\n\n\ncasual\n2020-11\n86025\n4.0%\n\n\ncasual\n2020-12\n29456\n1.4%\n\n\ncasual\n2021-01\n17719\n0.8%\n\n\ncasual\n2021-02\n9764\n0.4%\n\n\ncasual\n2021-03\n82219\n3.8%\n\n\ncasual\n2021-04\n133407\n6.1%\n\n\ncasual\n2021-05\n250286\n11.5%\n\n\ncasual\n2021-06\n360818\n16.6%\n\n\ncasual\n2021-07\n431617\n19.9%\n\n\ncasual\n2021-08\n404005\n18.6%\n\n\nmember\n2020-09\n294720\n11.2%\n\n\nmember\n2020-10\n237394\n9.0%\n\n\nmember\n2020-11\n168118\n6.4%\n\n\nmember\n2020-12\n99666\n3.8%\n\n\nmember\n2021-01\n77365\n2.9%\n\n\nmember\n2021-02\n38386\n1.5%\n\n\nmember\n2021-03\n142189\n5.4%\n\n\nmember\n2021-04\n197125\n7.5%\n\n\nmember\n2021-05\n269430\n10.2%\n\n\nmember\n2021-06\n352002\n13.4%\n\n\nmember\n2021-07\n373163\n14.2%\n\n\nmember\n2021-08\n384780\n14.6%\n\n\n\n\n\n\n\n\nShow code\ntotal_rides_by_month %>% \n  ggplot(aes(x = year_month, y = total_rides, fill = user_type)) + \n  geom_col(width = 0.65, position = position_dodge(0.75)) + \n  scale_y_continuous(labels = scales::comma) +\n  scale_fill_manual(values = my_colors) +\n  labs(title = \"Total Rides by Month\", subtitle = \"September 2020 - August 2021\",\n       x = \"\", y = \"\", fill = \"User Type\") + \n  my_theme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \"top\")\n\n\n\n\n\nIt is evident that the number of rides through the months directly correlates with the weather. Weather history in Chicago (Weather Spark) for the period September 2020 - August 2021 reveals common temperatures throughout the year with the exception of February 2021 (freezing, snow) and the summer months June, July, and August 2021 (above-average temperatures).\nThe number of rides taken by casuals exceeded the number of rides taken by members only in June, July, and August 2021, during which time casual rides accounted for 55.1% of total casual rides.\nThe number of rides by casuals in relation to members is much lower in the period from October 2020 to January 2021. The possible cause is the COVID-19 pandemic. We can see the number of new cases per capita in the following animation.\n\n\n\n📊 Total Rides by Day of the Week\n\n\nShow code and table output\n\n\ntotal_rides_by_dow <- all_trips_cln %>% \n  select(user_type, day_of_week) %>% \n  group_by(user_type, day_of_week) %>% \n  summarise(total_rides = n()) %>% \n  mutate(percentage = scales::percent(total_rides/sum(total_rides), accuracy = 0.1)) %>% \n  ungroup()\n\ntotal_rides_by_dow %>% knitr::kable()\n\n\n\n\nuser_type\nday_of_week\ntotal_rides\npercentage\n\n\n\n\ncasual\nMon\n244486\n11.3%\n\n\ncasual\nTue\n236602\n10.9%\n\n\ncasual\nWed\n236318\n10.9%\n\n\ncasual\nThu\n242197\n11.2%\n\n\ncasual\nFri\n314450\n14.5%\n\n\ncasual\nSat\n486516\n22.4%\n\n\ncasual\nSun\n409574\n18.9%\n\n\nmember\nMon\n357796\n13.6%\n\n\nmember\nTue\n393073\n14.9%\n\n\nmember\nWed\n399560\n15.2%\n\n\nmember\nThu\n383457\n14.6%\n\n\nmember\nFri\n388557\n14.7%\n\n\nmember\nSat\n381707\n14.5%\n\n\nmember\nSun\n330188\n12.5%\n\n\n\n\n\n\n\n\nShow code\ntotal_rides_by_dow %>% \n  ggplot(aes(x = day_of_week, y = total_rides, fill = user_type)) + \n  geom_col(width = 0.65, position = position_dodge(0.75)) + \n  scale_y_continuous(labels = scales::comma) +\n  scale_fill_manual(values = my_colors) +\n  labs(title = \"Total Rides by Day of the Week\", \n       subtitle = \"September 2020 - August 2021\",\n       x = \"\", y = \"\", fill = \"User Type\") +\n  my_theme +\n  theme(legend.position = \"top\")\n\n\n\n\n\nCasual riders prefer Saturday rides (22.4% of total casual rides). Sunday is the second day with the highest number of rides (18.9%), followed by Friday (14.5%). Rides from Monday to Thursday are equally split (11%).\nMembers’ rides are much more evenly split (13–15%). Members took the fewest rides on Sundays (12.5%).\n\n\n\n📊 Total Rides by Hour\n\n\nShow code and table output\n\n\ntotal_rides_by_hour <- all_trips_cln %>%\n  select(user_type, hour) %>% \n  group_by(user_type, hour) %>% \n  summarise(total_rides = n()) %>% \n  mutate(percentage = scales::percent(total_rides/sum(total_rides), accuracy = 0.1)) %>% \n  ungroup()\n\ntotal_rides_by_hour %>% knitr::kable() \n\n\n\n\nuser_type\nhour\ntotal_rides\npercentage\n\n\n\n\ncasual\n0\n43483\n2.0%\n\n\ncasual\n1\n30951\n1.4%\n\n\ncasual\n2\n19563\n0.9%\n\n\ncasual\n3\n10690\n0.5%\n\n\ncasual\n4\n7551\n0.3%\n\n\ncasual\n5\n9465\n0.4%\n\n\ncasual\n6\n20186\n0.9%\n\n\ncasual\n7\n35908\n1.7%\n\n\ncasual\n8\n50059\n2.3%\n\n\ncasual\n9\n61550\n2.8%\n\n\ncasual\n10\n87268\n4.0%\n\n\ncasual\n11\n116386\n5.4%\n\n\ncasual\n12\n141729\n6.5%\n\n\ncasual\n13\n152619\n7.0%\n\n\ncasual\n14\n159014\n7.3%\n\n\ncasual\n15\n167521\n7.7%\n\n\ncasual\n16\n182255\n8.4%\n\n\ncasual\n17\n208923\n9.6%\n\n\ncasual\n18\n187132\n8.6%\n\n\ncasual\n19\n143626\n6.6%\n\n\ncasual\n20\n105533\n4.9%\n\n\ncasual\n21\n86832\n4.0%\n\n\ncasual\n22\n79799\n3.7%\n\n\ncasual\n23\n62100\n2.9%\n\n\nmember\n0\n24795\n0.9%\n\n\nmember\n1\n15890\n0.6%\n\n\nmember\n2\n9038\n0.3%\n\n\nmember\n3\n5144\n0.2%\n\n\nmember\n4\n6306\n0.2%\n\n\nmember\n5\n24432\n0.9%\n\n\nmember\n6\n71187\n2.7%\n\n\nmember\n7\n126402\n4.8%\n\n\nmember\n8\n139994\n5.3%\n\n\nmember\n9\n109507\n4.2%\n\n\nmember\n10\n111689\n4.2%\n\n\nmember\n11\n137262\n5.2%\n\n\nmember\n12\n160348\n6.1%\n\n\nmember\n13\n158130\n6.0%\n\n\nmember\n14\n157059\n6.0%\n\n\nmember\n15\n177986\n6.8%\n\n\nmember\n16\n224585\n8.5%\n\n\nmember\n17\n279870\n10.6%\n\n\nmember\n18\n239079\n9.1%\n\n\nmember\n19\n169026\n6.4%\n\n\nmember\n20\n110341\n4.2%\n\n\nmember\n21\n77320\n2.9%\n\n\nmember\n22\n58519\n2.2%\n\n\nmember\n23\n40429\n1.5%\n\n\n\n\n\n\n\n\nShow code\ntotal_rides_by_hour %>% \n  ggplot(aes(x = hour, y = total_rides, color = user_type)) +\n  geom_line(aes(group = user_type), size = 1) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_color_manual(values = my_colors) +\n  labs(title = \"Total Rides by Hour\", subtitle = \"September 2020 - August 2021\",\n       x = \"\", y = \"\", color = \"User Type\") +\n  expand_limits(y = 3e5) +\n  my_theme +\n  theme(legend.position = \"top\")\n\n\n\n\n\nAmong members, we see spikes in use at 8 a.m. and 5 p.m., with another small bump at noon. Annual members are locals and frequent riders. They use bikes to commute to work or school, run errands, or get to appointments.\nAmong casual riders, we see a spike in use at 5 p.m. Casual riders are one-way commuters. Overall, they tend to ride in the late morning and afternoon.\n\n\n\n💡 Average ride length\nRide Length summary statistics by user type in minutes.\n\nall_trips_cln %>%   \n  select(user_type, ride_length) %>% \n  group_by(user_type) %>% \n  summarize(min = min(ride_length),\n            q1 = quantile(ride_length, 0.25),\n            median = median(ride_length),\n            mean = mean(ride_length),\n            q3 = quantile(ride_length, 0.75),\n            max = max(ride_length)) %>% \n  knitr::kable()\n\n\n\n\nuser_type\nmin\nq1\nmedian\nmean\nq3\nmax\n\n\n\n\ncasual\n1\n9.87\n17.22\n26.10541\n31.15\n180.00\n\n\nmember\n1\n6.12\n10.40\n13.83993\n17.82\n179.98\n\n\n\n\n\nThe average ride length for casuals (26 min) is almost twice as long as the average ride length for members (14 min). On average, rides by casuals last 12 minutes longer than rides by annual members.\n\n\n📊 Average Ride Length by Day of the Week\n\n\nShow code and table output\n\n\navg_ride_length_by_dow <- all_trips_cln %>% \n  select(user_type, day_of_week, ride_length) %>% \n  group_by(user_type, day_of_week) %>% \n  summarise(avg_ride_length = mean(ride_length) %>% round(2)) %>% \n  ungroup()\n\navg_ride_length_by_dow %>% knitr::kable() \n\n\n\n\nuser_type\nday_of_week\navg_ride_length\n\n\n\n\ncasual\nMon\n26.39\n\n\ncasual\nTue\n23.88\n\n\ncasual\nWed\n23.25\n\n\ncasual\nThu\n22.63\n\n\ncasual\nFri\n24.25\n\n\ncasual\nSat\n28.29\n\n\ncasual\nSun\n29.76\n\n\nmember\nMon\n13.34\n\n\nmember\nTue\n13.11\n\n\nmember\nWed\n13.21\n\n\nmember\nThu\n13.04\n\n\nmember\nFri\n13.48\n\n\nmember\nSat\n15.35\n\n\nmember\nSun\n15.62\n\n\n\n\n\n\n\n\nShow code\navg_ride_length_by_dow %>% \n  ggplot(aes(x = day_of_week, y = avg_ride_length, fill = user_type)) + \n  geom_col(width = 0.665, position = position_dodge(0.75)) + \n  scale_y_continuous(labels = scales::comma) +\n  scale_fill_manual(values = my_colors) +\n  labs(title = \"Average Ride Length (minute) by Day of the Week\",\n       subtitle = \"September 2020 - August 2021\",\n       x = \"\", y = \"\", fill = \"User Type\") +\n  my_theme +\n  theme(legend.position = \"top\") \n\n\n\n\n\nThe average ride length for members is constant from Monday to Friday (13 min), with a slight increase on Saturdays and Sundays (15 min).\nThe average ride length for casuals varies during the week (from 22 to 30 min), The highest average ride length is on Sundays (30 min), followed by Saturdays (28 min).\n\n\n\n📊 Average Ride Length by Hour\n\n\nShow code and table output\n\n\navg_ride_length_by_hour <- all_trips_cln %>% \n  select(user_type, hour, ride_length) %>% \n  group_by(user_type, hour) %>% \n  summarise(avg_ride_length = mean(ride_length) %>% round(2)) %>% \n  ungroup()\n\navg_ride_length_by_hour %>% knitr::kable()\n\n\n\n\nuser_type\nhour\navg_ride_length\n\n\n\n\ncasual\n0\n23.48\n\n\ncasual\n1\n22.59\n\n\ncasual\n2\n21.83\n\n\ncasual\n3\n21.65\n\n\ncasual\n4\n19.90\n\n\ncasual\n5\n18.87\n\n\ncasual\n6\n17.03\n\n\ncasual\n7\n18.05\n\n\ncasual\n8\n20.33\n\n\ncasual\n9\n25.39\n\n\ncasual\n10\n28.84\n\n\ncasual\n11\n29.63\n\n\ncasual\n12\n29.30\n\n\ncasual\n13\n29.98\n\n\ncasual\n14\n29.97\n\n\ncasual\n15\n28.87\n\n\ncasual\n16\n26.94\n\n\ncasual\n17\n24.95\n\n\ncasual\n18\n24.19\n\n\ncasual\n19\n24.35\n\n\ncasual\n20\n24.48\n\n\ncasual\n21\n23.80\n\n\ncasual\n22\n23.28\n\n\ncasual\n23\n22.90\n\n\nmember\n0\n12.72\n\n\nmember\n1\n13.00\n\n\nmember\n2\n12.92\n\n\nmember\n3\n13.16\n\n\nmember\n4\n12.21\n\n\nmember\n5\n11.47\n\n\nmember\n6\n12.26\n\n\nmember\n7\n12.44\n\n\nmember\n8\n12.51\n\n\nmember\n9\n13.02\n\n\nmember\n10\n13.78\n\n\nmember\n11\n14.09\n\n\nmember\n12\n13.93\n\n\nmember\n13\n14.31\n\n\nmember\n14\n14.73\n\n\nmember\n15\n14.56\n\n\nmember\n16\n14.44\n\n\nmember\n17\n14.51\n\n\nmember\n18\n14.28\n\n\nmember\n19\n13.97\n\n\nmember\n20\n13.65\n\n\nmember\n21\n13.37\n\n\nmember\n22\n13.07\n\n\nmember\n23\n12.80\n\n\n\n\n\n\n\n\nShow code\navg_ride_length_by_hour %>% \n  ggplot(aes(x = hour, y = avg_ride_length, color = user_type)) +\n  geom_line(aes(group = user_type), size = 1) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_color_manual(values = my_colors) +\n  labs(title = \"Average Ride Length (minute) by Hour\", \n       subtitle = \"September 2020 - August 2021\",\n       x = \"\", y = \"\", color = \"User Type\") +\n  my_theme +\n  theme(legend.position = \"top\")\n\n\n\n\n\nThe longest rides for casuals start in the late mornings and afternoons. The average ride length for members is pretty much constant during the day.\n\n\n\n📊 Average Ride Length by Hour and Day of the Week\n\n\nShow code\nall_trips_cln %>%\n  group_by(user_type, day_of_week, hour) %>%\n  summarise(avg_ride_length = mean(ride_length), .groups = 'drop') %>% \n  ggplot(aes(x = hour, y = avg_ride_length, color = user_type)) +\n  geom_line(aes(group = user_type), size=1) +\n  facet_wrap(~day_of_week) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_discrete(breaks = seq(1, 23, 2)) +\n  scale_color_manual(values = my_colors) +\n  labs(title = \"Average Ride Length (minute) by Hour and Day of the Week\", \n       subtitle = \"September 2020 - August 2021\",\n       x = \"\", y = \"\", color = \"User Type\") +\n  my_theme +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n💡 Bike-type usage\n\nall_trips_cln %>% \n  select(user_type, rideable_type) %>% \n  group_by(user_type, rideable_type) %>% \n  summarise(total_rides = n()) %>% \n  mutate(percentage = scales::percent(total_rides/sum(total_rides), accuracy = 0.1)) %>% \n  ungroup() %>% \n  pivot_wider(-total_rides, names_from = rideable_type, values_from = percentage) %>% \n  knitr::kable()\n\n\n\n\nuser_type\nclassic_bike\ndocked_bike\nelectric_bike\n\n\n\n\ncasual\n41.8%\n24.2%\n34.0%\n\n\nmember\n50.9%\n18.6%\n30.5%\n\n\n\n\n\nNote: Both classic_bike and docked_bike are classic bikes. classic_bike is a classic docless bike, while docked_bike is a classic docked bike.\nOn average, casual riders took 66% of total casual rides with classic bikes and 34% with electric. Members took slightly fewer rides with electric bikes (30.5%) than casuals.\n\n\n📊 Rideable Type by Month\n\n\nShow code\nall_trips_cln %>%\n  group_by(user_type, year_month, rideable_type) %>% \n  summarise(total_rides = n()) %>% \n  ggplot(aes(x = year_month, y = total_rides, fill = rideable_type)) +\n  geom_col(width = 0.7, position = \"fill\") +\n  facet_wrap(~user_type) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Rideable Type Usage by Month\", \n       subtitle = \"September 2020 - August 2021\",\n       x = \"\", y = \"\") + \n  my_theme + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \"top\")\n\n\n\n\n\nWe can see that from October 2020 to January 2021, more rides were taken with electric bikes than on average. Since fewer rides were taken in those months, both groups had more electric bikes at their disposal.\n\n\n\n💡 Most popular stations\nNow we’ll find out the top 20 most popular stations for casuals and members. The most popular starting and destination stations don’t differ much, so I chose to work with starting stations.\n\n🛠️ Data wrangling\n\ntop_start_stations <- all_trips_cln %>% \n  select(user_type, start_station_name) %>% \n  filter(!is.na(start_station_name)) %>% \n  group_by(user_type, start_station_name) %>% \n  summarise(total_rides = n()) %>% \n  slice_max(total_rides, n = 20) %>% \n  mutate(rank = row_number()) %>% \n  ungroup() %>% \n  # join coordinates\n  left_join(divvy_stations %>% select(station_name, latitude, longitude), \n            by = c(\"start_station_name\" = \"station_name\"))\n\ntop_start_stations\n\n# A tibble: 40 × 6\n   user_type start_station_name                 total_ri…¹  rank latit…² longi…³\n   <chr>     <chr>                                   <int> <int>   <dbl>   <dbl>\n 1 casual    Streeter Dr & Grand Ave                 55539     1    41.9   -87.6\n 2 casual    DuSable Lake Shore Dr & Monroe St       33604     2    41.9   -87.6\n 3 casual    Millennium Park                         30505     3    41.9   -87.6\n 4 casual    Michigan Ave & Oak St                   27212     4    41.9   -87.6\n 5 casual    DuSable Lake Shore Dr & North Blvd      26677     5    41.9   -87.6\n 6 casual    Theater on the Lake                     21516     6    41.9   -87.6\n 7 casual    Shedd Aquarium                          20644     7    41.9   -87.6\n 8 casual    Wells St & Concord Ln                   17111     8    41.9   -87.6\n 9 casual    Indiana Ave & Roosevelt Rd              16393     9    41.9   -87.6\n10 casual    Clark St & Lincoln Ave                  15450    10    41.9   -87.6\n# … with 30 more rows, and abbreviated variable names ¹​total_rides, ²​latitude,\n#   ³​longitude\n\n\n\n\n📄 Interactive data table with the most popular stations\n💡 The table below is sortable. You can sort a column by clicking on its header. I find this table very useful. It provides so much information. See different results by clicking on “User type”, “Rank”, or “Total rides”.\n\n\nShow code\n# Render a bar chart with a label on the left\nbar_chart <- function(label, width = \"100%\", height = \"0.875rem\", fill = \"#00bfc4\", background = NULL) {\n  bar <- div(style = list(background = fill, width = width, height = height))\n  chart <- div(style = list(flexGrow = 1, marginLeft = \"0.375rem\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\nreactable(\n  top_start_stations %>% select(user_type, rank, start_station_name, total_rides),\n  showPageSizeOptions = TRUE,\n  compact = TRUE,\n  columns = list(\n    user_type = colDef(\n      name = \"User type\",\n      width = 120\n    ),\n    rank = colDef(\n      name = \"Rank\",\n      width = 100\n    ),\n    start_station_name = colDef(\n      name = \"Start station name\",\n      minWidth = 320\n    ),\n    total_rides = colDef(\n      name = \"Total rides\",\n      defaultSortOrder = \"desc\",\n      format = colFormat(separators = TRUE),\n      cell = function(value, index) {\n        width <- paste0(value * 100 / max(top_start_stations$total_rides), \"%\")\n        # Add thousands separators\n        value <- format(value, big.mark = \",\")\n        color_fill <- if(index <= (nrow(top_start_stations)/2)) {\n            \"#355273\"\n          } else {\n            \"#FF3030\"\n          }\n        bar_chart(value, width = width, fill = color_fill)    \n      },\n      minWidth = 250,\n      # And left-align the columns\n      align = \"left\"\n    )\n  ),\n  defaultSorted = c(\"total_rides\", \"user_type\")\n)\n\n\n\n\n\n\n\n\n\n📊 Interactive leaflet map with the most popular stations\n\n\nShow code\n# subsetting top stations for casuals and members\ncasual <- top_start_stations %>% \n  filter(user_type == \"casual\")\n\nmember <- top_start_stations %>% \n  filter(user_type == \"member\")\n\n# Creating a color palette\nmypalette <- colorFactor(my_colors, domain = top_start_stations$user_type)\n\n# Preparing the text for the tooltip\nmytext_cas <- paste(\n   \"Station: \", casual$start_station_name, \"<br/>\", \n   \"User type: \", casual$user_type, \"<br/>\", \n   \"Total rides: \", scales::comma(casual$total_rides), \"<br/>\", \n   \"Rank: \", casual$rank, sep=\"\") %>%\n  lapply(htmltools::HTML)\n\nmytext_mem <- paste(\n   \"Station: \", member$start_station_name, \"<br/>\", \n   \"User type: \", member$user_type, \"<br/>\", \n   \"Total rides: \", scales::comma(member$total_rides),  \"<br/>\", \n   \"Rank: \", casual$rank, sep=\"\") %>%\n  lapply(htmltools::HTML)\n\n# final map\nleaflet(top_start_stations) %>% \n  addProviderTiles(\"CartoDB\") %>%  \n  addCircleMarkers(data = casual,\n                   fillColor = ~mypalette(user_type), \n                   fillOpacity = 0.7, \n                   color = \"white\", \n                   radius = ~total_rides/3000,\n                   stroke = FALSE,\n                   label = mytext_cas,\n                   labelOptions = labelOptions(style = list(\"font-weight\"=\"normal\", \n                padding = \"3px 8px\"), textsize = \"12px\", direction = \"auto\"),\n                   group = \"casual\") %>%\n  addCircleMarkers(data = member,\n                   fillColor = ~mypalette(user_type), \n                   fillOpacity = 0.7, \n                   color = \"white\", \n                   radius = ~total_rides/3000,\n                   stroke = FALSE,\n                   label = mytext_mem,\n                   labelOptions = labelOptions(style = list(\"font-weight\"=\"normal\", \n                padding = \"3px 8px\"), textsize = \"12px\", direction = \"auto\"),\n                   group = \"member\") %>%\n  addLegend(pal = mypalette, \n            values = ~user_type, \n            opacity = 0.7, \n            title = \"User type\", \n            position = \"bottomright\" ) %>% \n  addLayersControl(overlayGroups = c(\"casual\", \"member\"),\n                   options = layersControlOptions(collapsed = FALSE)) %>% \n  leaflet.extras::addResetMapButton()\n\n\n\n\n\n\n\nThe most popular stations for casual riders are Millennium Park and those located along DuSable Lake Shore Drive. These are all localities for entertainment and leisure activities, which is indicative of the main purpose of these rides.\nMost popular stations for members are located nearby or inside the business district, which is indicative of the main purpose of these rides (commuting to work or school, running errands, getting to appointments, and similar)."
  },
  {
    "objectID": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-v-act",
    "href": "posts/cyclistic-bike-share-analysis/cyclistic.html#part-v-act",
    "title": "Cyclistic Bike-Share Analysis",
    "section": "✨ PART V: Act",
    "text": "✨ PART V: Act\nCasual riders can be locals or visitors (tourists). With our new marketing campaign, we should target only locals since there is little chance that visitors will buy an annual membership. Unfortunately, there is no way to distinguish locals from visitors in the group of casual riders. We’ll have to settle for the data we currently have.\n\nMy top three recommendations for the new marketing strategy aimed at converting casual riders into annual members are::1. Conduct the marketing campaign during the summer months (June, July, August), mostly on weekends in the afternoon but also on weekdays around 5 p.m.2. Use the fact that casual riders tend to ride 12+ minutes longer than annual members.3. Act in the areas of stations that are most popular with casual riders.\n\n\nYou’ve reached the end of this project. Thank you for reading! Dream big. Bye! 🖐"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html",
    "href": "posts/shiny-mass-shootings/mass-shootings.html",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "",
    "text": "This post will be regularly updated with each new case.\nLast update on November 2, 2023."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#introduction",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#introduction",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Introduction",
    "text": "Introduction\nMass shootings have been a topic of intense discussion in the United States. A public “database” of mass shootings since 1982 has been made available by the Mother Jones, a non-profit organization. This “database” is stored in a Google spreadsheet. You can access it here and download as a CSV file.\nThere are many definitions of mass shooting. Here is what Britannica has to say:\n\nMass shooting, also called active shooter incident, as defined by the U.S. Federal Bureau of Investigation (FBI), an event in which one or more individuals are “actively engaged in killing or attempting to kill people in a populated area. Implicit in this definition is the shooter’s use of a firearm.” The FBI has not set a minimum number of casualties to qualify an event as a mass shooting, but U.S. statute (the Investigative Assistance for Violent Crimes Act of 2012) defines a “mass killing” as “3 or more killings in a single incident”."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-overview",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data overview",
    "text": "Data overview\n\nlibrary(tidyverse)\nlibrary(tidygeocoder)\nlibrary(plotly)\ntheme_set(theme_classic())\n\nmass_shootings <- read_csv(\"data/mass_shootings_usa_1982-2023.csv\")\n\nmass_shootings %>% glimpse()\n\nRows: 148\nColumns: 24\n$ case                             <chr> \"Maine bowling alley and restaurant s…\n$ location...2                     <chr> \"Lewiston, Maine\", \"Jacksonville, Flo…\n$ date                             <chr> \"10/25/2023\", \"8/26/2023\", \"8/23/2023…\n$ summary                          <chr> \"Robert Card, 40, an Army reservist a…\n$ fatalities                       <dbl> 18, 3, 3, 5, 3, 8, 5, 6, 3, 7, 11, 6,…\n$ injured                          <dbl> 13, 0, 6, 2, 6, 7, 8, 6, 5, 1, 10, 6,…\n$ total_victims                    <dbl> 21, 3, 9, 7, 9, 15, 13, 12, 8, 8, 21,…\n$ location...8                     <chr> \"Other\", \"workplace\", \"Other\", \"Other…\n$ age_of_shooter                   <chr> \"40\", \"21\", \"59\", \"40\", \"18\", \"33\", \"…\n$ prior_signs_mental_health_issues <chr> \"yes\", \"yes\", \"-\", \"yes\", \"yes\", \"yes…\n$ mental_health_details            <chr> \"Card reportedly spoke of \\\"hearing v…\n$ weapons_obtained_legally         <chr> \"-\", \"yes\", \"-\", \"yes\", \"yes\", \"yes\",…\n$ where_obtained                   <chr> \"Yes\", \"local gun stores\", \"-\", \"-\", …\n$ weapon_type                      <chr> \"semiautomatic rifle\", \"semiautomatic…\n$ weapon_details                   <chr> \"AR-15-style rifle (Rugar SFAR)\", \"AR…\n$ race                             <chr> \"White\", \"White\", \"White\", \"Black\", \"…\n$ gender                           <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"F…\n$ sources                          <chr> \"https://www.washingtonpost.com/natio…\n$ mental_health_sources            <chr> \"https://www.nytimes.com/article/lewi…\n$ sources_additional_age           <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ latitude                         <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ longitude                        <chr> \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-…\n$ type                             <chr> \"Spree\", \"mass\", \"mass\", \"mass\", \"mas…\n$ year                             <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2…\n\n\nWe have 148 cases, described with 24 variables. At first glance, this dataset clearly needs extensive cleaning."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#data-cleaning",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n🧹 Step 1. Initial cleaning\nThe first cleaning step includes:\n\nselecting columns of interest,\nreplacing the character value \"-\" with NA in all columns with character data type,\nconverting date column from character to date data type,\nrenaming location columns,\nconverting character data type to numeric for specific columns.\n\n\nmass_shootings_cln <- mass_shootings %>% \n  select(1:6, 8:10, 12, 16, 17, 21:24) %>% \n  mutate(across(where(is.character), ~na_if(., \"-\"))) %>% \n  mutate(date = lubridate::mdy(date)) %>% \n  rename(location = location...2, location_2 = location...8) %>% \n  mutate_at(c(\"injured\", \"age_of_shooter\", \"latitude\", \"longitude\"), as.numeric)\n  \nmass_shootings_cln %>% glimpse()\n\nRows: 148\nColumns: 16\n$ case                             <chr> \"Maine bowling alley and restaurant s…\n$ location                         <chr> \"Lewiston, Maine\", \"Jacksonville, Flo…\n$ date                             <date> 2023-10-25, 2023-08-26, 2023-08-23, …\n$ summary                          <chr> \"Robert Card, 40, an Army reservist a…\n$ fatalities                       <dbl> 18, 3, 3, 5, 3, 8, 5, 6, 3, 7, 11, 6,…\n$ injured                          <dbl> 13, 0, 6, 2, 6, 7, 8, 6, 5, 1, 10, 6,…\n$ location_2                       <chr> \"Other\", \"workplace\", \"Other\", \"Other…\n$ age_of_shooter                   <dbl> 40, 21, 59, 40, 18, 33, 25, 28, 43, 6…\n$ prior_signs_mental_health_issues <chr> \"yes\", \"yes\", NA, \"yes\", \"yes\", \"yes\"…\n$ weapons_obtained_legally         <chr> NA, \"yes\", NA, \"yes\", \"yes\", \"yes\", \"…\n$ race                             <chr> \"White\", \"White\", \"White\", \"Black\", \"…\n$ gender                           <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"F…\n$ latitude                         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ longitude                        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ type                             <chr> \"Spree\", \"mass\", \"mass\", \"mass\", \"mas…\n$ year                             <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2…\n\n\n🔎 Are there any duplicates? No.\n\nsum(duplicated(mass_shootings))\n\n[1] 0\n\n\n🔎 Number of missing values, NA, per column.\n\nmass_shootings_cln %>% \n  summarise_all(~sum(is.na(.))) %>% \n  # transposing for better visibility\n  pivot_longer(cols = everything(), names_to = \"column\", values_to = \"n_missing\")\n\n# A tibble: 16 × 2\n   column                           n_missing\n   <chr>                                <int>\n 1 case                                     0\n 2 location                                 0\n 3 date                                     0\n 4 summary                                  0\n 5 fatalities                               0\n 6 injured                                  0\n 7 location_2                               0\n 8 age_of_shooter                           2\n 9 prior_signs_mental_health_issues        29\n10 weapons_obtained_legally                18\n11 race                                    12\n12 gender                                   0\n13 latitude                                22\n14 longitude                               22\n15 type                                     0\n16 year                                     0\n\n\n22 of the most recent cases don’t have location coordinates at all. We’ll address this in the final cleanup step.\n\n\n🧹 Step 2. Fixing unique values of categorical variables\n🔎 Let’s take a look at the unique values of the gender column.\n\nmass_shootings_cln %>% \n  count(gender, sort = TRUE) \n\n# A tibble: 6 × 2\n  gender                                                                       n\n  <chr>                                                                    <int>\n1 \"M\"                                                                         72\n2 \"Male\"                                                                      70\n3 \"Female\"                                                                     2\n4 \"Male & Female\"                                                              2\n5 \"F\"                                                                          1\n6 \"F (\\\"identifies as transgender\\\" and \\\"Audrey Hale is a biological wom…     1\n\n\nAlmost all categorical variables need unique values correction.\nTo make a long story short, I’ll correct them all in one step using case_when function, and we’ll look at them later during the analysis.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  mutate(gender = case_when(gender == \"F\" ~ \"Female\",\n                            gender == \"M\" ~ \"Male\", \n                            gender == \"(see summary)\" ~ \"Male\",\n                            gender %>% str_detect(\"transgender\")~\"Female (transgender)\",\n                            TRUE ~ gender),\n         race = case_when(race == \"white\" ~ \"White\",\n                          race == \"black\" ~ \"Black\",\n                          race == \"unclear\" ~ \"Unclear\",\n                          TRUE ~ race),\n         location_2 = \n           case_when(location_2 %in% c(\"workplace\", \"\\nWorkplace\") ~ \"Workplace\",\n                                location_2 == \"Other\\n\" ~ \"Other\",\n                                location_2 == \"religious\" ~ \"Religious\",\n                                TRUE ~ location_2),\n         prior_signs_mental_health_issues = \n           case_when(prior_signs_mental_health_issues == \"yes\" ~ \"Yes\",\n                     prior_signs_mental_health_issues == \"TBD\" ~ \"To be determined\",\n                     TRUE ~ prior_signs_mental_health_issues),\n         weapons_obtained_legally = \n           case_when(weapons_obtained_legally %in% c(\"yes\", \"\\nYes\") ~ \"Yes\",\n                     weapons_obtained_legally == \"TBD\" ~ \"To be determined\",\n                     weapons_obtained_legally %>% str_detect(\"Kelley\") ~ \"Unknown\",\n                     weapons_obtained_legally %>% str_detect(\"some\") ~ \"Partially\",\n                     TRUE ~ weapons_obtained_legally), \n         type = case_when(type == \"mass\" ~ \"Mass\",\n                          TRUE ~ type))\n\n\n\n🧹 Step 3. Geocoding locations with missing coordinates\nThere are 22 cases with missing location coordinates. In this step we’ll convert locations to coordinates with geocoding. and use them later to create a leaflet map for a shiny app.\nThe tidygeocoder package provides geocoding services. It’s designed to work easily with the tidyverse. It also provides access to several different geocoding services, including LocationIQ which I’m going to use here. LocationIQ is a freemium service that provides a free tier, which doesn’t require you to give them your billing details. When you sign up to LocationIQ, they’ll take you to the Manage Your API Access Tokens page, which is where we obtain our API token. Next, you need to provide the tidygeocoder package with your API key.\nYou can also use the Nominatim (“osm”) geocoding service (OpenStreetMap) which can be specified with the method argument (method = \"osm\"). I found LocationIQ to be faster.\nThe first step is to select only locations with missing coordinates and geocode them.\n\ngeocoded_locations <- mass_shootings_cln %>% \n  filter(is.na(latitude) | is.na(longitude)) %>% \n  select(location) %>% \n  geocode(location, method = \"iq\")\n\ngeocoded_locations %>% \n  mutate(across(where(is.numeric), ~ num(., digits = 6)))\n\n# A tibble: 22 × 3\n   location                         lat        long\n   <chr>                      <num:.6!>   <num:.6!>\n 1 Lewiston, Maine            44.095108  -70.215545\n 2 Jacksonville, Florida      30.332184  -81.655651\n 3 Trabuco Canyon, California 33.662623 -117.589380\n 4 Philadelphia, Pennsylvania 39.952724  -75.163526\n 5 Farmington, New Mexico     36.729115 -108.205445\n 6 Allen, Texas               33.103174  -96.670550\n 7 Louisville, Kentucky       38.254238  -85.759407\n 8 Nashville, Tennessee       36.162277  -86.774298\n 9 East Lansing, Michigan     42.732031  -84.472168\n10 Half Moon Bay, California  37.463552 -122.428586\n# ℹ 12 more rows\n\n\nThe next step is to join mass shootings table with geocoded locations and replace missing latitudes and longitudes with geocoded.\n\nmass_shootings_cln <- mass_shootings_cln %>% \n  left_join(geocoded_locations, by = \"location\") %>% \n  mutate(latitude = ifelse(is.na(latitude), lat, latitude),\n         longitude = ifelse(is.na(longitude), long, longitude))\n\n🔎 Checking for null values.\n\nsum(is.na(mass_shootings_cln$latitude))\n\n[1] 0\n\nsum(is.na(mass_shootings_cln$longitude))\n\n[1] 0\n\n\nOK, this looks fine."
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#exploratory-data-analysis-eda",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\n❕ Writing a function\nTo count unique values for all categorical variables separately, I’ll write a function, count_unique, to avoid copying and pasting a block of code several times.\nHere we have a special case where we have to pass a dataframe column name (variable) to a function argument. The solution is to embrace the argument by surrounding it in doubled braces, like group_by({{ var }}).\n\ncount_unique <- function(data, var) {\n  \n  data %>%\n    group_by({{ var }}) %>%    \n    summarise(count = n(), .groups = \"drop\") %>% \n    mutate(percent = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n    arrange(desc(count))\n\n}\n\n\n\n📄 Breakdown by categorical variables\n\nGender\n\ncount_unique(mass_shootings_cln, gender)\n\n# A tibble: 4 × 3\n  gender               count percent\n  <chr>                <int> <chr>  \n1 Male                   142 95.9%  \n2 Female                   3 2.0%   \n3 Male & Female            2 1.4%   \n4 Female (transgender)     1 0.7%   \n\n\n\n\nRace\n\ncount_unique(mass_shootings_cln, race) \n\n# A tibble: 8 × 3\n  race            count percent\n  <chr>           <int> <chr>  \n1 White              79 53.4%  \n2 Black              26 17.6%  \n3 Latino             12 8.1%   \n4 <NA>               12 8.1%   \n5 Asian              10 6.8%   \n6 Other               5 3.4%   \n7 Native American     3 2.0%   \n8 Unclear             1 0.7%   \n\n\n\n\nSpecific location\n\ncount_unique(mass_shootings_cln, location_2)\n\n# A tibble: 6 × 3\n  location_2 count percent\n  <chr>      <int> <chr>  \n1 Other         58 39.2%  \n2 Workplace     53 35.8%  \n3 School        22 14.9%  \n4 Religious      8 5.4%   \n5 Military       6 4.1%   \n6 Airport        1 0.7%   \n\n\n\n\nPrior signs of mental health issues\n\ncount_unique(mass_shootings_cln, prior_signs_mental_health_issues)\n\n# A tibble: 6 × 3\n  prior_signs_mental_health_issues count percent\n  <chr>                            <int> <chr>  \n1 Yes                                 72 48.6%  \n2 <NA>                                29 19.6%  \n3 Unclear                             24 16.2%  \n4 No                                  17 11.5%  \n5 To be determined                     5 3.4%   \n6 Unknown                              1 0.7%   \n\n\n\n\nWeapons obtained legally\n\ncount_unique(mass_shootings_cln, weapons_obtained_legally)\n\n# A tibble: 6 × 3\n  weapons_obtained_legally count percent\n  <chr>                    <int> <chr>  \n1 Yes                         99 66.9%  \n2 <NA>                        18 12.2%  \n3 No                          16 10.8%  \n4 To be determined             7 4.7%   \n5 Unknown                      7 4.7%   \n6 Partially                    1 0.7%   \n\n\n\n\nType\n\ncount_unique(mass_shootings_cln, type)\n\n# A tibble: 2 × 3\n  type  count percent\n  <chr> <int> <chr>  \n1 Mass    126 85.1%  \n2 Spree    22 14.9%  \n\n\nNote: Spree shootings have three or more victims in a short time in multiple locations.\n\n\n\n📊 Age of shooter distribution\n\n\nCode for creating the age_group column\n# create \"age group\" column\nmass_shootings_cln <-  mass_shootings_cln %>% \n  mutate(age_group = case_when(\n    age_of_shooter >= 10 & age_of_shooter <= 14 ~ \"10-14\",\n    age_of_shooter <= 19 ~ \"15-19\",\n    age_of_shooter <= 24 ~ \"20-24\",\n    age_of_shooter <= 29 ~ \"25-29\",\n    age_of_shooter <= 34 ~ \"30-34\",\n    age_of_shooter <= 39 ~ \"35-39\",\n    age_of_shooter <= 44 ~ \"40-44\",\n    age_of_shooter <= 49 ~ \"45-49\",\n    age_of_shooter <= 54 ~ \"50-54\",\n    age_of_shooter <= 59 ~ \"55-59\",\n    age_of_shooter <= 64 ~ \"60-64\",\n    age_of_shooter <= 69 ~ \"65-69\",\n    age_of_shooter <= 74 ~ \"70-74\"))\n\n\n\np1 <- mass_shootings_cln %>% \n  filter(!is.na(age_group)) %>% \n  group_by(age_group) %>% \n  summarise(count = n(), .groups = \"drop\") %>% \n  mutate(percent = scales::percent(count/sum(count), accuracy = 0.1)) %>% \n  mutate(label_text = str_glue(\"Age group: {age_group}\n                               Count: {count}\n                               Percent: {percent}\")) %>%\n  ggplot(aes(x = age_group, y = count, text = label_text)) +\n  geom_col(width = 0.7, fill = \"indianred\") +\n  labs(title = \"Age Distribution\", x = \"age group\") \n\nggplotly(p1, tooltip = \"text\")\n\n\n\n\n\n\n\nThe vast majority of shooters were between 15 and 50 years old.\nThe age distribution is bimodal, with one mode around 23 years of age and a second mode around 41 years of age.\nMost shooters were in the 20-24 age group (18.5 %), followed by 40-44 (17.1 %) and 25-29 (15.8 %).\n\n🔎 Who was the youngest shooter?\n\nmass_shootings_cln %>% \n  slice_min(age_of_shooter, n = 1) %>% \n  select(case, date, summary, fatalities) %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\ncase\ndate\nsummary\nfatalities\n\n\n\n\nWestside Middle School killings\n1998-03-24\nMitchell Scott Johnson, 13, and Andrew Douglas Golden, 11, two juveniles, ambushed students and teachers as they left the school; they were apprehended by police at the scene.\n5\n\n\n\n\n\n\n\n📊 Number of cases per year\n\np2 <- mass_shootings_cln %>%\n  group_by(year) %>%\n  summarise(count = n()) %>% \n  ggplot(aes(year, count)) +\n  geom_col(fill = \"steelblue\") + \n  geom_smooth(method = \"loess\", se = FALSE, color = \"indianred\", size = 0.7) +\n  labs(title = \"Number of Cases per Year\") \n\nggplotly(p2)\n\n\n\n\n\n\n\nWe can see an increase in mass shootings in the last 12 years.\n2020 has a smaller number of cases probably due to Covid restrictions.\nThe data for 2023 is incomplete, with 11 cases in the first ten months.\n\n\n\n📊 Fatalities-Injured relationship\n\np3 <- mass_shootings_cln %>%\n  ggplot(aes(x = fatalities, y = injured)) +\n  geom_jitter() +\n  scale_y_sqrt() +\n  labs(title = \"Fatalities-Injured Relationship\")\n  \nggplotly(p3)\n\n\n\n\n\n\nPlease note that the Injured values are square root scaled for better visibility, but you can see the actual values by hovering over the points.\nSummary of fatalities\n\nsummary(mass_shootings_cln$fatalities)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   4.000   6.000   7.743   8.000  58.000 \n\n\nSummary of injured people\n\nsummary(mass_shootings_cln$injured)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    3.00   10.99   10.00  546.00 \n\n\n\n\n📊 Total fatalities by state\n\n🛠️ Data manipulation\n\n# create us states with abbreviations tibble\nstates_with_abbr <- \n  tibble(state = state.name, abbr = state.abb) %>% \n  bind_rows(tibble(state = \"District of Columbia\", abbr = \"DC\"))\n\n# data manipulation\nby_state <- mass_shootings_cln %>% \n  # recode D.d. to District of Columbia\n  mutate(location = ifelse(location == \"Washington, D.C.\", \n                           \"Washington, District of Columbia\", \n                           location)) %>% \n  # separate location into city and state\n  separate(location, c(\"city\", \"state\"), sep = \", \") %>% \n  # group and summarize\n  group_by(state) %>% \n  summarise(total_cases = n(),\n            total_fatalities = sum(fatalities), .groups = \"drop\") %>% \n  # add us states abbreviations\n  left_join(states_with_abbr, by = \"state\") %>% \n  # rearrange columns\n  select(state, abbr, everything())\n\n\n\n📈 Top ten states regarding number of cases and fatalities\n\nby_state %>% \n  arrange(-total_cases, -total_fatalities) %>% \n  head(10)\n\n# A tibble: 10 × 4\n   state        abbr  total_cases total_fatalities\n   <chr>        <chr>       <int>            <dbl>\n 1 California   CA             26              178\n 2 Texas        TX             13              159\n 3 Florida      FL             13              129\n 4 Colorado     CO              8               53\n 5 Washington   WA              7               37\n 6 Pennsylvania PA              6               32\n 7 New York     NY              5               40\n 8 Wisconsin    WI              5               28\n 9 Illinois     IL              5               25\n10 Virginia     VA              4               53\n\n\n\n\n📊 Total fatalities by state visualization\n\nby_state %>% \n  plot_geo(locationmode = 'USA-states') %>% \n  add_trace(z = ~total_fatalities,\n            locations = ~abbr,\n            color = ~total_fatalities,\n            colors = ~\"Reds\") %>% \n  layout(\n    geo = list(\n      scope = \"usa\",\n      projection = list(type = \"albers usa\"),\n      lakecolor = toRGB(\"white\")\n    )\n  )"
  },
  {
    "objectID": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "href": "posts/shiny-mass-shootings/mass-shootings.html#shiny-app",
    "title": "Shiny App - Mass Shootings in the USA",
    "section": "Shiny app",
    "text": "Shiny app\nThe app you can see below is embedded in this quarto document since my website is static. It was originally published on shinyapps.io, where you can also interact with it.\nNote: If you don’t see the application, I’ve run out of my 25 active hours (when my applications are not idle). Sorry, this is a free account, and my app will not be available again until the following month cycle. Hope you get lucky! 😊\n📢 By clicking on each circle, you can read a summary of the mass shooting case.\n\n\n\n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html",
    "title": "Titanic Survival Exercises",
    "section": "",
    "text": "After auditing the HarvardX’s Data Science: Visualization course I’ve found this assessment way too interesting and fun. So I decided to put all my new skills together to perform exploratory data analysis on a classic machine learning dataset: Titanic survival! My goal is to provide answers entirely through visualizations."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#background",
    "title": "Titanic Survival Exercises",
    "section": "Background",
    "text": "Background\nThe Titanic was a British ocean liner that struck an iceberg and sunk on its maiden voyage in 1912 from the United Kingdom to New York. More than 1,500 of the estimated 2,224 passengers and crew died in the accident, making this one of the largest maritime disasters ever outside of war. The ship carried a wide range of passengers of all ages and both genders, from luxury travelers in first-class to immigrants in the lower classes. However, not all passengers were equally likely to survive the accident. We use real data about a selection of 891 passengers to learn who was on the Titanic and which passengers were more likely to survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#libraries-customizations-and-data",
    "title": "Titanic Survival Exercises",
    "section": "Libraries, Customizations, and Data",
    "text": "Libraries, Customizations, and Data\n\nlibrary(tidyverse)\nlibrary(titanic)\n\noptions(digits = 3)  \ntheme_set(theme_classic())\ncolors_sex <- c(\"mediumorchid1\", \"dodgerblue\")\ncolors_survived <- c(\"gray65\", \"lightgreen\")\n\nDefining the titanic dataset.\n\ntitanic <- titanic_train %>%\n  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare) %>%\n  mutate(Survived = factor(Survived),\n         Pclass = factor(Pclass),\n         Sex = factor(Sex))\n\nhead(titanic)\n\n  Survived Pclass    Sex Age SibSp Parch  Fare\n1        0      3   male  22     1     0  7.25\n2        1      1 female  38     1     0 71.28\n3        1      3 female  26     0     0  7.92\n4        1      1 female  35     1     0 53.10\n5        0      3   male  35     0     0  8.05\n6        0      3   male  NA     0     0  8.46\n\nstr(titanic)\n\n'data.frame':   891 obs. of  7 variables:\n $ Survived: Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass  : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Sex     : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age     : num  22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp   : int  1 1 0 1 0 0 0 3 0 1 ...\n $ Parch   : int  0 0 0 0 0 0 0 1 2 0 ...\n $ Fare    : num  7.25 71.28 7.92 53.1 8.05 ..."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-1-variable-types",
    "title": "Titanic Survival Exercises",
    "section": "Question 1: Variable Types",
    "text": "Question 1: Variable Types\nInstructions: Inspect the data and also use ?titanic_train to learn more about the variables in the dataset. Match these variables from the dataset to their variable type. There is at least one variable of each type (ordinal categorical, non-ordinal (nominal) categorical, continuous, discrete).\nChecking if Age variable is discrete or continuous…\n\nunique(titanic$Age)\n\n [1] 22.00 38.00 26.00 35.00    NA 54.00  2.00 27.00 14.00  4.00 58.00 20.00\n[13] 39.00 55.00 31.00 34.00 15.00 28.00  8.00 19.00 40.00 66.00 42.00 21.00\n[25] 18.00  3.00  7.00 49.00 29.00 65.00 28.50  5.00 11.00 45.00 17.00 32.00\n[37] 16.00 25.00  0.83 30.00 33.00 23.00 24.00 46.00 59.00 71.00 37.00 47.00\n[49] 14.50 70.50 32.50 12.00  9.00 36.50 51.00 55.50 40.50 44.00  1.00 61.00\n[61] 56.00 50.00 36.00 45.50 20.50 62.00 41.00 52.00 63.00 23.50  0.92 43.00\n[73] 60.00 10.00 64.00 13.00 48.00  0.75 53.00 57.00 80.00 70.00 24.50  6.00\n[85]  0.67 30.50  0.42 34.50 74.00\n\n\nAge is a continuous variable.\n\n\n\nVariable\nDescription\nVariable Type\n\n\n\n\nSurvived\nPassenger Survival Indicator\nnominal categorical\n\n\nPclass\nPassenger Class\nordinal categorical\n\n\nSex\nSex\nnominal categorical\n\n\nAge\nAge\ncontinuous\n\n\nSibSp\nNumber of Siblings/Spouses Aboard\ndiscrete\n\n\nParch\nNumber of Parents/Children Aboard\ndiscrete\n\n\nFare\nPassenger Fare\ncontinuous"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-2-demographics-of-titanic-passengers",
    "title": "Titanic Survival Exercises",
    "section": "Question 2: Demographics of Titanic Passengers",
    "text": "Question 2: Demographics of Titanic Passengers\nInstructions: Make density plots of age grouped by sex. Try experimenting with combinations of faceting, alpha blending, stacking and using variable counts on the y-axis to answer the following questions. Some questions may be easier to answer with different versions of the density plot.\n\ntitanic %>% \n  ggplot(aes(Age)) +\n  geom_density(aes(color = Sex), size = 0.7) +\n  scale_color_manual(values = colors_sex) +\n  geom_vline(xintercept = c(18, 35), linetype = 2) +\n  geom_text(aes(x = 18, y = 0.031, label= \"18\", hjust = 1.5)) +\n  geom_text(aes(x = 35, y = 0.031, label= \"35\", hjust = -0.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, fill = Sex)) +\n  geom_density(alpha = 0.3) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 17, linetype = 2) +\n  geom_text(aes(x = 17, y = 0.031, label= \"17\", hjust = 1.5)) +\n  theme(legend.position = \"top\") +\n  ylab(\"density\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Sex)) +\n  geom_density(alpha = 0.7) +\n  facet_grid(Sex ~ .) +\n  scale_fill_manual(values = colors_sex) +\n  geom_vline(xintercept = 40, linetype = 2) + \n  geom_text(aes(x = 40, y = 14, label= \"40\", hjust = -0.5)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers\n\n✅ Females and males had the same general shape of age distribution.\n✅ } The age distribution was bimodal, with one mode around 25 years of age and a second - - smaller mode around 5 years of age.\n❌ There were more females than males.\n✅ The count of males of age 40 was higher than the count of females of age 40.\n✅ The proportion of males age 18-35 was higher than the proportion of females age 18-35.\n✅ The proportion of females under age 17 was higher than the proportion of males under age 17.\n❌ The oldest passengers were female."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-3-qq-plot-of-age-distribution",
    "title": "Titanic Survival Exercises",
    "section": "Question 3: QQ-plot of Age Distribution",
    "text": "Question 3: QQ-plot of Age Distribution\nInstructions: Use geom_qq() to make a QQ-plot of passenger age and add an identity line with geom_abline(). Filter out any individuals with an age of NA first.\n\nparams <- titanic %>%\n  filter(!is.na(Age)) %>%\n  summarize(mean = mean(Age), sd = sd(Age))\n\nparams\n\n  mean   sd\n1 29.7 14.5\n\ntitanic %>% ggplot(aes(sample = Age)) + \n  geom_qq(dparams = params) +\n  geom_abline()\n\n\n\n\nWhich of the following is the correct plot according to the instructions above?\n\n✅ The plot above."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-4-survival-by-sex",
    "title": "Titanic Survival Exercises",
    "section": "Question 4: Survival by Sex",
    "text": "Question 4: Survival by Sex\nInstructions: To answer the following questions, make barplots of the Survived and Sex variables using geom_bar(). Try plotting one variable and filling by the other variable. You may want to try the default plot, then try adding position = position_dodge() to geom_bar() to make separate bars for each group.\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Sex)) +\n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_sex)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(width =  0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Less than half of passengers survived.\n✅ Most of the survivors were female.\n❌ Most of the males survived.\n✅ Most of the females survived."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-5-survival-by-age",
    "title": "Titanic Survival Exercises",
    "section": "Question 5: Survival by Age",
    "text": "Question 5: Survival by Age\nInstructions: Make a density plot of age filled by survival status. Change the y-axis to count and set alpha = 0.2.\nThe following answers were offered for all three questions:\n\n0-8\n10-18\n18-30\n30-50\n50-70\n70-80\n\n\nWhich age group is the only group more likely to survive than die?\n\ntitanic %>% \n  ggplot(aes(Age, y = ..count.., fill = Survived)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = colors_survived) +\n  geom_vline(xintercept = 8, linetype = 2) +\n  geom_text(aes(x = 8, y = 14, label= \"8\", hjust = -0.5))\n\n\n\n\n\n✅ Age group 0-8.\n\n\n\nWhich age group had the most deaths?\nIt’s hard to tell from the previews plot I’ll have to make a new column Age group based on the offered answers.\n\ntitanic2 <- titanic %>%\n  filter(!is.na(Age)) %>% \n  mutate(`Age group` = case_when(\n    Age > 0 & Age <= 8 ~ \"0-8\",\n    Age > 10 & Age <= 18 ~ \"10-18\",\n    Age > 18 & Age <= 30 ~ \"18-30\",\n    Age > 30 & Age <= 50 ~ \"30-50\",\n    Age > 50 & Age <= 70 ~ \"50-70\",\n    Age > 70 & Age <= 80 ~ \"70-80\"\n    )\n  )\n\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 18-30.\n\n\n\nWhich age group had the highest proportion of deaths?\n\ntitanic2 %>% \n  filter(!is.na(`Age group`)) %>% \n  ggplot(aes(`Age group`, fill = Survived)) + \n  geom_bar(position = \"fill\", width = 0.7, color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\n✅ Age group 70-80"
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-6-survival-by-fare",
    "title": "Titanic Survival Exercises",
    "section": "Question 6: Survival by Fare",
    "text": "Question 6: Survival by Fare\nInstructions: Filter the data to remove individuals who paid a fare of 0. Make a boxplot of fare grouped by survival status. Try a log2 transformation of fares. Add the data points with jitter and alpha blending.\n\nset.seed(123)\n\ntitanic %>% filter(Fare != 0) %>% \n  ggplot(aes(Survived, Fare)) +\n  geom_boxplot(fill = colors_survived, width = 0.5, alpha = 0.5) + \n  geom_jitter(width = 0.1, alpha = 0.2) +\n  scale_y_continuous(trans = \"log2\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ Passengers who survived generally payed higher fares than those who did not survive.\n❌ The interquartile range for fares was smaller for passengers who survived.\n✅ The median fare was lower for passengers who did not survive.\n❌ Only one individual paid a fare around $500. That individual survived. (3 individuals survived)\n✅ Most individuals who paid a fare around $8 did not survive."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-7-survival-by-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 7: Survival by Passenger Class",
    "text": "Question 7: Survival by Passenger Class\nInstructions: The Pclass variable corresponds to the passenger class. Make three barplots. For the first, make a basic barplot of passenger class filled by survival. For the second, make the same barplot but use the argument position = position_fill() to show relative proportions in each group instead of counts. For the third, make a barplot of survival filled by passenger class using position = position_fill()\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Pclass)) + \n  geom_bar(width = 0.7) +\n  geom_text(aes(label = ..count..), stat = \"count\", vjust = -1) +\n  expand_limits(y = 530) +\n  ylab(\"count\")\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) + \n  geom_bar(width = 0.8, position = position_dodge(0.85)) +\n  scale_fill_manual(values = colors_survived)\n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Survived, fill = Pclass)) + \n  geom_bar(width = 0.7, position = position_fill(), color = \"white\") \n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ There were more third class passengers than passengers in the first two classes combined.\n❌ There were the fewest passengers in first class, second-most passengers in second class, and most passengers in third class.\n✅ Survival proportion was highest for first class passengers, followed by second class. Third-class had the lowest survival proportion.\n✅ Most passengers in first class survived. Most passengers in other classes did not survive.\n❌ The majority of survivors were from first class.\n✅ The majority of those who did not survive were from third class."
  },
  {
    "objectID": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "href": "posts/titanic-survival-exercises/titanic-survival-exercises.html#question-8-survival-by-age-sex-and-passenger-class",
    "title": "Titanic Survival Exercises",
    "section": "Question 8: Survival by Age, Sex and Passenger Class",
    "text": "Question 8: Survival by Age, Sex and Passenger Class\nInstructions: Create a grid of density plots for age, filled by survival status, with count on the y-axis, faceted by sex and passenger class.\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Pclass)) +\n  geom_density(alpha=0.5) \n\n\n\n\n\ntitanic %>% \n  ggplot(aes(Age, ..count.., fill = Survived)) +\n  geom_density(alpha=0.5) +\n  facet_grid((Sex ~ Pclass)) +\n  scale_fill_manual(values = colors_survived) + \n  theme(panel.border = element_rect(colour = \"black\", fill = NA)) +\n  theme(legend.position = \"top\")\n\n\n\n\nWhich of the following are true?\nSelect all correct answers.\n\n✅ The largest group of passengers was third-class males.\n❌ The age distribution is the same across passenger classes.\n❌ The gender distribution is the same across passenger classes.\n✅ Most first-class and second-class females survived.\n✅ Almost all second-class males did not survive, with the exception of children.\n\n\nThat’s all. Thanks for reading!"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "",
    "text": "The goal of this project is to scrape data on the 200 best movies of the last decade from the Rotten Tomatoes website with the R rvest package, and finally create a dashboard in Tableau. The idea is to show all the movies in one place. Hovering over the movie should reveal relevant data in the tooltip for quick overview. Clicking on the movie should open the movie’s website for more information.\nI’ve learned so much while working on this project (like web scraping, writing functions, iteration, …). The purrr package for functional programming is super-cool. It allows iteration with just one line of code (a very handy replacement for for loops).\nI hope you’ll enjoy the process as much as I did. At times it was quite challenging, but that’s how we learn!\n\n\n\n# loading packages\nlibrary(tidyverse)\nlibrary(rvest)\n\nAre we allowed to scrape data from the Rotten Tomatoes website?\n\nrobotstxt::paths_allowed(\"https://www.rottentomatoes.com/\")\n\n[1] TRUE"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#plan",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "Plan",
    "text": "Plan\nThe data will be scraped from this page. Since it doesn’t contain all the data I am interested in, I have to visit every movie’s web page on the list and scrape data from there. Here is the plan:\n\nScrape data from the main page: the urls of movies, and the urls of images.\nScrape title, year_genre_runtime, critics_score, audiaece_score, and synopsis from the first movie to develop the code.\nWrite a function that scrapes data based on movie’s URL.\nIteration - use this function to scrape data from each individual movie and create a data frame with the columns title, year_genre_runtime, critics_score, audiaece_score, synopsis, and url.\nDownload images\nPrepare data for Tableau\nCreate a dashboard in Tableau"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-from-the-main-page",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "1. Scraping data from the main page",
    "text": "1. Scraping data from the main page\nReading the main page with read_html().\n\nmain_url <- \"https://editorial.rottentomatoes.com/guide/the-200-best-movies-of-the-2010s/\"\nmain_page <- read_html(main_url)\n\n\n\n\n\n\n\nFigure 1: The main page\n\n\n\n\nI make use of the SelectorGadget to identify the tags for the relevant nodes. Here is the link for Chrome (recommended).\n\nExtracting urls of movies\nThe same nodes that contain the text for the titles also contain information on the links to individual movie pages for each title. We can extract this information using the html_attr() function, which extracts attributes.\n\nmovie_urls <- main_page %>% \n  html_nodes(\".article_movie_title a\") %>% \n  html_attr(\"href\")\n\nmovie_urls %>% head()\n\n[1] \"https://www.rottentomatoes.com/m/12_years_a_slave\"    \n[2] \"https://www.rottentomatoes.com/m/20_feet_from_stardom\"\n[3] \"https://www.rottentomatoes.com/m/45_years\"            \n[4] \"https://www.rottentomatoes.com/m/all_is_lost_2013\"    \n[5] \"https://www.rottentomatoes.com/m/amazing_grace_2018\"  \n[6] \"https://www.rottentomatoes.com/m/american_hustle\"     \n\n\n\n\nExtracting urls of images\n\nimage_urls <- main_page %>% \n  html_nodes(\".article_poster\") %>% \n  html_attr(\"src\")\n\nLet’s check the image for the 6th title.\n\nknitr::include_graphics(image_urls[6])"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#scraping-data-for-the-first-movie-on-the-list",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "2. Scraping data for the first movie on the list",
    "text": "2. Scraping data for the first movie on the list\nNow I’m going to scrape data for the movie 12 Years a Slave in order to develop the code.\nReading page for the first movie.\n\nurl <- \"https://www.rottentomatoes.com/m/12_years_a_slave\"\nmovie_page <- read_html(url)\n\n\n\n\n\n\n\nFigure 2: Title, year, genre, runtime, critics and audience score\n\n\n\n\nScroll down the page and you’ll find the movie synopsis.\n\n\n\n\n\nFigure 3: Synopsis\n\n\n\n\n\nExtracting title\n\ntitle <- movie_page %>% \n  html_node(\".scoreboard__title\") %>% \n  html_text()\n\ntitle\n\n[1] \"12 Years a Slave\"\n\n\n\n\nExtracting year, genre, and runtime\n\nyear_genre_runtime <- movie_page %>% \n  html_node(\".scoreboard__info\") %>% \n  html_text()\n\nyear_genre_runtime\n\n[1] \"2013, History/Drama, 2h 14m\"\n\n\n\n\nExtracting critics score\nThe next two are tricky. I had to look at the page source and find them manually.\n\ncritics_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"tomatometerscore\") %>% \n  str_c(.,\"%\")\n\ncritics_score\n\n[1] \"95%\"\n\n\n\n\nExtracting audience score\n\naudience_score <- movie_page %>% \n  html_element(\"score-board\") %>% \n  html_attr(\"audiencescore\") %>% \n  str_c(.,\"%\")\n\naudience_score\n\n[1] \"90%\"\n\n\n\n\nExtracting movie synopsis\n\nsynopsis <- movie_page %>% \n  html_node(\"#movieSynopsis\") %>% \n  html_text2()\n\nsynopsis\n\n[1] \"In the years before the Civil War, Solomon Northup (Chiwetel Ejiofor), a free black man from upstate New York, is kidnapped and sold into slavery in the South. Subjected to the cruelty of one malevolent owner (Michael Fassbender), he also finds unexpected kindness from another, as he struggles continually to survive and maintain some of his dignity. Then in the 12th year of the disheartening ordeal, a chance meeting with an abolitionist from Canada changes Solomon's life forever.\"\n\n\n\n\nMakinging a data frame of extracted elements\n\nmovie  <- tibble(title = title, \n                 year_genre_runtime = year_genre_runtime,\n                 critics_score = critics_score,\n                 audience_score = audience_score,\n                 synopsis = synopsis,  \n                 url = url)\n\nmovie %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"12 Years a Slave\"\n$ year_genre_runtime <chr> \"2013, History/Drama, 2h 14m\"\n$ critics_score      <chr> \"95%\"\n$ audience_score     <chr> \"90%\"\n$ synopsis           <chr> \"In the years before the Civil War, Solomon Northup…\n$ url                <chr> \"https://www.rottentomatoes.com/m/12_years_a_slave\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#writing-a-function",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "3. Writing a function",
    "text": "3. Writing a function\nInstead of manually scraping individual movies, I’ll write a function to do the same.\n\nscrape_movie <- function(x, ...){\n  \n  movie_page <- read_html(x)\n  \n  title <- movie_page %>% \n    html_node(\".scoreboard__title\") %>% \n    html_text()\n  \n  year_genre_runtime <- movie_page %>% \n    html_node(\".scoreboard__info\") %>% \n    html_text()\n  \n  critics_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"tomatometerscore\") %>% \n    str_c(.,\"%\")\n  \n  audience_score <- movie_page %>% \n    html_element(\"score-board\") %>% \n    html_attr(\"audiencescore\") %>% \n    str_c(.,\"%\")\n  \n  synopsis <- movie_page %>% \n    html_node(\"#movieSynopsis\") %>% \n    html_text2()\n  \n  movie_df <- tibble(title = title, \n                     year_genre_runtime = year_genre_runtime,\n                     critics_score = critics_score,\n                     audience_score = audience_score,\n                     synopsis = synopsis,\n                     url = x)\n  \n  return(movie_df)\n  \n}\n\n\nFunction in action\nNow that we have the scrape_movie() function, let’s scrape data for the movie “American Hustle”.\n\nscrape_movie(movie_urls[6]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"American Hustle\"\n$ year_genre_runtime <chr> \"2013, Crime/Drama, 2h 18m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"74%\"\n$ synopsis           <chr> \"Irving Rosenfeld (Christian Bale) dabbles in forge…\n$ url                <chr> \"https://www.rottentomatoes.com/m/american_hustle\"\n\n\nOr “Ex Machina” (an interesting SF movie).\n\n scrape_movie(movie_urls[53]) %>% glimpse()\n\nRows: 1\nColumns: 6\n$ title              <chr> \"Ex Machina\"\n$ year_genre_runtime <chr> \"2014, Sci-fi/Mystery & thriller, 1h 47m\"\n$ critics_score      <chr> \"92%\"\n$ audience_score     <chr> \"86%\"\n$ synopsis           <chr> \"Caleb Smith (Domhnall Gleeson) a programmer at a h…\n$ url                <chr> \"https://www.rottentomatoes.com/m/ex_machina\""
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#iteration",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "4. Iteration",
    "text": "4. Iteration\nTo make my workflow a little more efficient, I make use of the map_dfr() function from the purrr package to iterate over all movie pages. map_dfr() will apply the scrape_movie()function to each element in the vector of links, and return a data frame created by row-binding. It’s as simple as that.\n\nmovies <- map_dfr(movie_urls, scrape_movie)\n\nmovies \n\n# A tibble: 200 × 6\n   title                year_genre_runtime         criti…¹ audie…² synop…³ url  \n   <chr>                <chr>                      <chr>   <chr>   <chr>   <chr>\n 1 12 Years a Slave     2013, History/Drama, 2h 1… 95%     90%     In the… http…\n 2 20 Feet From Stardom 2013, Documentary, 1h 30m  99%     82%     Filmma… http…\n 3 45 Years             2015, Drama, 1h 33m        97%     67%     As the… http…\n 4 All Is Lost          2013, Adventure/Mystery &… 94%     64%     During… http…\n 5 Amazing Grace        2018, Documentary/Music, … 99%     80%     Singer… http…\n 6 American Hustle      2013, Crime/Drama, 2h 18m  92%     74%     Irving… http…\n 7 Amy                  2015, Documentary/Biograp… 95%     87%     Archiv… http…\n 8 Anomalisa            2015, Comedy/Drama, 1h 30m 91%     71%     An ins… http…\n 9 Ant-Man and The Wasp 2018, Action/Adventure, 1… 87%     80%     Scott … http…\n10 Apollo 11            2019, Documentary/History… 99%     90%     Never-… http…\n# … with 190 more rows, and abbreviated variable names ¹​critics_score,\n#   ²​audience_score, ³​synopsis"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#downloading-images",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "5. Downloading images",
    "text": "5. Downloading images\nI’ve already extracted urls of images in the first step and saved them to image_urls. Now I’m going to create a directory and directory paths for the images.\n\nfs::dir_create(\"images/top_200_images/\")\n\npaths <- c(str_c(\"images/top_200_images/\", sprintf(\"%0.3d\", 1:200), \".jpg\"))\n\npaths %>% head()\n\n[1] \"images/top_200_images/001.jpg\" \"images/top_200_images/002.jpg\"\n[3] \"images/top_200_images/003.jpg\" \"images/top_200_images/004.jpg\"\n[5] \"images/top_200_images/005.jpg\" \"images/top_200_images/006.jpg\"\n\n\nSince Tableau sorts images alphabetically (1, 11, 111, 2, 22, …) by default, these leading zeros will help Tableau to correctly match the images with the data so I don’t have to do it manually.\n\nDownloading images\nThis time I’ll use map2() function from the purrr package, It will apply the download.file() function to pairs of elements from two vectors, image_urls and paths.\n\nmap2(image_urls, paths, function(.x, .y) download.file(.x, .y, mode=\"wb\")) \n\nAre the images properly saved? Let’s read in the image for the first movie.\n\nknitr::include_graphics(\"images/top_200_images/001.jpg\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#data-wrangling",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "6. Data wrangling",
    "text": "6. Data wrangling\nPreparing the final dataset for Tableau.\n\nmovies <- movies %>% \n  \n  # separate year_genre_runtime column into year, genre, and runtime\n  separate(year_genre_runtime, sep = \", \", into = c(\"year\", \"genre\", \"runtime\")) %>% \n  mutate(year = as.factor(year)) %>% \n  \n  # separate genre column into primary and secondary genre\n  separate(genre, sep = \"/\", into = c(\"genre_1\", \"genre_2\"), remove = FALSE) %>% \n  \n  # create id column with leading zeroes so Tableau can automatically match the images\n  mutate(id = sprintf(\"%0.3d\", 1:200)) %>% \n  select(id, everything())\n\nmovies %>% head()\n\n# A tibble: 6 × 11\n  id    title  year  genre genre_1 genre_2 runtime criti…¹ audie…² synop…³ url  \n  <chr> <chr>  <fct> <chr> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>\n1 001   12 Ye… 2013  Hist… History Drama   2h 14m  95%     90%     In the… http…\n2 002   20 Fe… 2013  Docu… Docume… <NA>    1h 30m  99%     82%     Filmma… http…\n3 003   45 Ye… 2015  Drama Drama   <NA>    1h 33m  97%     67%     As the… http…\n4 004   All I… 2013  Adve… Advent… Myster… 1h 45m  94%     64%     During… http…\n5 005   Amazi… 2018  Docu… Docume… Music   1h 27m  99%     80%     Singer… http…\n6 006   Ameri… 2013  Crim… Crime   Drama   2h 18m  92%     74%     Irving… http…\n# … with abbreviated variable names ¹​critics_score, ²​audience_score, ³​synopsis\n\n\n\n# number of unique values in genre column\nmovies$genre %>% unique() %>% length()\n\n[1] 59\n\n\n\n# unique values in genre_1\nmovies$genre_1 %>% unique()\n\n [1] \"History\"            \"Documentary\"        \"Drama\"             \n [4] \"Adventure\"          \"Crime\"              \"Comedy\"            \n [7] \"Action\"             \"Sci-fi\"             \"Romance\"           \n[10] \"Horror\"             \"Biography\"          \"Mystery & thriller\"\n[13] \"Kids & family\"      \"War\"                \"Fantasy\"           \n[16] \"Musical\"            \"Western\"           \n\n\n\n# unique values in genre_2\nmovies$genre_2 %>% unique()\n\n [1] \"Drama\"              NA                   \"Mystery & thriller\"\n [4] \"Music\"              \"Biography\"          \"Adventure\"         \n [7] \"History\"            \"Romance\"            \"Comedy\"            \n[10] \"Lgbtq+\"             \"Action\"             \"War\"               \n[13] \"Fantasy\"            \"Sci-fi\"             \"Crime\"             \n[16] \"Musical\"            \"Western\"            \"Anime\"             \n[19] \"Horror\"            \n\n\nFinding values in genre_2, that are not in genre_1. This will help when creating a list parameter for filtering by primary or secondary genre.\n\nsetdiff(movies$genre_2, movies$genre_1)\n\n[1] NA       \"Music\"  \"Lgbtq+\" \"Anime\" \n\n\n\nDT table\nIf you prefer to search a table for data, then this one is for you!\n\nmovies %>% \n  select(1:9) %>% \n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\n\n\nWriting file\nI choose to save the data in an excel file only because the csv will remove the leading zeros in the id column.\n\nmovies %>% writexl::write_xlsx(\"datasets/top_200_movies_2010s_rotten_tomatoes.xlsx\")"
  },
  {
    "objectID": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "href": "posts/web-scraping-rotten-tomatoes/rotten-tomatoes.html#tableau-dashboard",
    "title": "Scraping 200 Best Movies of 2010s from Rotten Tomatoes",
    "section": "7. Tableau dashboard",
    "text": "7. Tableau dashboard\nThe final dashboard is created in Tableau. It’s actually a jitter plot, which separates overlapping movies with the same critics’ score.\nTo avoid two filters, one for primary and one for secondary genre, a list parameter is created that filters movies by primary or secondary genre, or “All” values.\nFor the best viewing experience, please click on the full screen in the bottom right corner.\nYou can nteract with the embedded dashboard below or go to Tableau Public. Enjoy!"
  }
]